<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Jacek's Blog</title>
    <link href="https://blog.galowicz.de/atom.xml" rel="self" />
    <link href="https://blog.galowicz.de" />
    <id>https://blog.galowicz.de/atom.xml</id>
    <author>
        <name>Jacek Galowicz</name>
        <email>jacek@galowicz.de</email>
    </author>
    <updated>2023-01-30T00:00:00Z</updated>
    <entry>
    <title>Interpolation of Records of Values in Purescript</title>
    <link href="https://blog.galowicz.de/2023/01/30/purescript-interpolation-of-records" />
    <id>https://blog.galowicz.de/2023/01/30/purescript-interpolation-of-records</id>
    <published>2023-01-30T00:00:00Z</published>
    <updated>2023-01-30T00:00:00Z</updated>
    <summary type="html"><![CDATA[<!-- cSpell:words lerp lerping lerped XKCD metaprogramming PureScript -->
<!-- cSpell:words lerpable lerps -->
<!-- cSpell:ignore inear olation -->
<p>This article is about a little interesting detour that I made in one of my
personal projects:
How to use strong type systems to generically apply a binary mathematical
function on all items of a possibly nested record.
The code provides interesting insights into other languages with similar type
systems.</p>
<!--more-->
<p>In this personal project, I work with numeric data that is displayed in diagrams
like these in the user’s browser:</p>
<figure>
<img src="/images/lapbench-diagram-amg-arena.png" alt="Example Diagrams of Numeric Data Input" />
<figcaption aria-hidden="true">Example Diagrams of Numeric Data Input</figcaption>
</figure>
<p>The amount of input data can be huge at times and needs post-processing for the
following reasons:</p>
<ul>
<li>Displaying too much data gives longer render times and stuttering movement
when scrolling or hovering over the data points, especially on smartphones or
older computers</li>
<li>More detailed data does not always lead to visibly more detailed diagrams
anyway</li>
<li>To compare two diagrams at the same data grid positions, the dataset
of one diagram needs to be interpolated into the other</li>
</ul>
<p>For these reasons I wrote a few functions that post-process the data in the
following steps:</p>
<ol type="1">
<li>Create the mathematical derivative of the data</li>
<li>Thin out the data of the first diagram by skipping samples that have the
lowest derivative values. This way the most detailed areas in the data stay
detailed enough, but the data gets much lighter.</li>
<li>Interpolate the data of the second graph onto the resulting data grid
positions of the first diagram.</li>
</ol>
<p>The data is represented as arrays of
<a href="https://github.com/purescript/documentation/blob/master/language/Records.md">records</a> in
<a href="https://www.purescript.org">PureScript</a>:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">type</span> <span class="dt">TelemetrySample</span> <span class="ot">=</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>  {<span class="ot"> position           ::</span> <span class="dt">Number</span>       <span class="co">-- Relative position on the track </span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>                                       <span class="co">-- between 0.0 and 1.0</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>  ,<span class="ot"> speed              ::</span> <span class="dt">Number</span>       <span class="co">-- km/h</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>  ,<span class="ot"> gasPedal           ::</span> <span class="dt">Number</span>       <span class="co">-- Value between 0.0 and 1.0</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>  ,<span class="ot"> brakePedal         ::</span> <span class="dt">Number</span>       <span class="co">-- Value between 0.0 and 1.0</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>  ,<span class="ot"> steeringWheelAngle ::</span> <span class="dt">Number</span>       <span class="co">-- Degrees</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>  ,<span class="ot"> tirePressures      ::</span> <span class="dt">Array</span> <span class="dt">Number</span> <span class="co">-- List of four PSI pressures</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>  ,<span class="ot"> tireTemperatures   ::</span> <span class="dt">Array</span> <span class="dt">Number</span> <span class="co">-- List of four °C temperatures</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>  , <span class="op">...</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>  }</span></code></pre></div>
<p>Having so much data on many different things like speed, pedal positions,
steering wheel, etc. means that steps 1-3 would need to be performed on all
these attributes for each diagram.
Interestingly, applying some knowledge of the actual domain the data comes from
helps out:
The interesting data is in the corners when the car gets slower and faster
quickly.
The long straights where the driver just drives full-throttle and does only
slight corrections with the steering wheel can be thinned out.
This means that the derivative of step 1 can be the derivative of the <code>position</code>
data, which shows low values when the car is <em>slow</em>, so we would <em>keep</em> those
values on the diagram.
An alternative is the derivative of the speed, which shows low values when the
car’s speed isn’t changing a lot (like on the straights), so we would <em>drop</em>
these values.</p>
<p>Step 2 can be performed repeatedly:
One would first sweep data carefully with a low threshold, and if the number of
samples is not low enough, then just repeat with a higher threshold, until the
size of the data is good enough for fast rendering.</p>
<p>Step 3, interpolating data, can be done by simple
<a href="https://en.wikipedia.org/wiki/Linear_interpolation">linear interpolation</a> between
two neighbor data points, or by including even more data points using
<a href="https://en.wikipedia.org/wiki/Polynomial_interpolation">polynomial interpolation</a>
or <a href="https://en.wikipedia.org/wiki/Fourier_analysis">Fourier synthesis</a>.
I chose to try the cheapest method - linear interpolation - first.
It turned out to be good enough.</p>
<h2 id="linear-interpolation">Linear Interpolation</h2>
<div class="floating-image-right">
<figure>
<img src="/images/linear-interpolation-wikipedia.svg" width="300" alt="Linear Interpolation between two points (Wikipedia)" />
<figcaption aria-hidden="true">Linear Interpolation between two points (<a href="https://en.wikipedia.org/wiki/Linear_interpolation">Wikipedia</a>)</figcaption>
</figure>
</div>
<p>Linear interpolation is adding two values after adapting their weight in the
final result:
If we need the middle between two data points, we would divide both of them
by 2 and add them together.
If we want the data at a 3/4 position between two points, we would take 1/4 of
the first point and 3/4 of the second point, and so on.</p>
<p>In PureScript, Haskell, ML, or any language that has
<a href="https://en.wikipedia.org/wiki/Type_class">type classes</a>, we can declare a
class <code>Lerp</code> (for <strong>L</strong>inear Int<strong>erp</strong>olation) and implement it for the
<code>Number</code> type like this:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> <span class="dt">Lerp</span> a <span class="kw">where</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="ot">  lerp ::</span> <span class="dt">Number</span> <span class="ot">-&gt;</span> a <span class="ot">-&gt;</span> a <span class="ot">-&gt;</span> a</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="kw">instance</span><span class="ot"> lerpNumber ::</span> <span class="dt">Lerp</span> <span class="dt">Number</span> <span class="kw">where</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>  lerp n a b <span class="ot">=</span> a <span class="op">+</span> (b <span class="op">-</span> a) <span class="op">*</span> n</span></code></pre></div>
<p>The <code>lerp</code> function accepts a parameter <code>n</code> which describes the relative
position to interpolate to between the two points that are the other two
parameters:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;</span> <span class="kw">import</span> <span class="dt">Lerp</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;</span> lerp <span class="fl">0.1</span> <span class="fl">0.0</span> <span class="fl">10.0</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="fl">1.0</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;</span> lerp <span class="fl">0.5</span> <span class="fl">0.0</span> <span class="fl">10.0</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="fl">5.0</span></span></code></pre></div>
<p>My input data is also represented by <code>Int</code> and <code>Boolean</code> points.
Integers can be cast to floating point <code>Number</code>s, <code>lerp</code>ed on their domain, and
then be brought back to <code>Int</code> by rounding.
The same can be done for <code>Boolean</code> values, which is a bit of a tongue-in-cheek
method, but on the given data domain this is fine for my purposes:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">instance</span><span class="ot"> lerpInt ::</span> <span class="dt">Lerp</span> <span class="dt">Int</span> <span class="kw">where</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>  lerp n a b <span class="ot">=</span> <span class="fu">round</span> <span class="op">$</span> lerp n (toNumber a) (toNumber b)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="kw">instance</span><span class="ot"> lerpBoolean ::</span> <span class="dt">Lerp</span> <span class="dt">Boolean</span> <span class="kw">where</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>  lerp n a b <span class="ot">=</span> from <span class="op">$</span> lerp n (to a) (to b)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">where</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    to <span class="ot">=</span> <span class="kw">if</span> _ <span class="kw">then</span> <span class="fl">1.0</span> <span class="kw">else</span> <span class="fl">0.0</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    from x <span class="ot">=</span> <span class="fu">round</span> x <span class="op">&gt;</span> <span class="dv">0</span></span></code></pre></div>
<p>For integers, this works as expected.
For booleans, we accept that due to <a href="https://en.wikipedia.org/wiki/Quantization_(signal_processing)">quantization</a>, <code>0.1 * true</code> equals <code>false</code>:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;</span> lerp <span class="fl">0.5</span> <span class="dv">0</span> <span class="dv">10</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="dv">5</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;</span> lerp <span class="fl">0.55</span> <span class="dv">0</span> <span class="dv">10</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="dv">6</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;</span> lerp <span class="fl">0.1</span> true false</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>true</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;</span> lerp <span class="fl">0.9</span> true false</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>false</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;</span> lerp <span class="fl">0.5</span> true false</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>true</span></code></pre></div>
<p>Lerping arrays of values that are in the type class already is not much work:
One type class instance can be implemented in terms of the other, which is where
nice type systems create a lot of joy:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">instance</span><span class="ot"> lerpArray ::</span> <span class="dt">Lerp</span> a <span class="ot">=&gt;</span> <span class="dt">Lerp</span> (<span class="dt">Array</span> a) <span class="kw">where</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>  lerp n a b <span class="ot">=</span> <span class="fu">zipWith</span> (lerp n) a b</span></code></pre></div>
<p>Now everything is ready to <code>lerp</code> whole records of data, but wait:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="ot">lerpRecord ::</span> <span class="dt">Number</span> <span class="ot">-&gt;</span> <span class="dt">TelemetrySample</span> <span class="ot">-&gt;</span> <span class="dt">TelemetrySample</span> <span class="ot">-&gt;</span> <span class="dt">TelemetrySample</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>lerpRecord n l r <span class="ot">=</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>  { speed <span class="ot">=</span> lerp n l<span class="op">.</span>speed r<span class="op">.</span>speed</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>  , gasPedal <span class="ot">=</span> lerp n l<span class="op">.</span>gasPedal r<span class="op">.</span>gasPedal</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>  , brakePedal <span class="ot">=</span> lerp n l<span class="op">.</span>brakePedal r<span class="op">.</span>brakePedal</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>  , steeringWheelAngle <span class="ot">=</span> lerp n l<span class="op">.</span>steeringWheelAngle r<span class="op">.</span>steeringWheelAngle</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>  , <span class="op">...</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>  }</span></code></pre></div>
<p>With a rising number of attributes in the record type, this gets very
repetitive, error-prone, and ugly very fast!
As soon as the record is nested, we would need another function for each of
them.
I wanted to be able to call the function <code>lerp</code> on records of data, assuming
that all record member types have instances of the <code>Lerp</code> type class.
At this point this would bring me:</p>
<ul>
<li>Eliminated repetition</li>
<li>Eliminated potential for typos</li>
<li>Learn how to implement such a mapping for generic records</li>
</ul>
<p>On the one hand, according to the famous
<a href="https://xkcd.com/1205/">XKCD comic “Is it Worth the Time?”</a>,
I really should not do it, because the repetition is still not getting out of
hand.
On the other hand, I like metaprogramming and learning the programming
languages that I use regularly <em>thoroughly</em>, so I decided to invest an afternoon
into learning how to do it.</p>
<figure>
<img src="/images/xkcd-is-it-worth-the-time.png" alt="Famous XKCD comic about time investments in the automation of tasks" />
<figcaption aria-hidden="true">Famous <a href="https://xkcd.com/1205/">XKCD comic</a> about time investments in the automation of tasks</figcaption>
</figure>
<h2 id="mapping-functions-over-records-of-data">Mapping Functions over Records of Data</h2>
<p>What we are aiming at is running the <code>lerp</code> function over records of values:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;</span> r1 <span class="ot">=</span> { a<span class="op">:</span> <span class="dv">1</span>, b<span class="op">:</span> <span class="fl">1.0</span>, c<span class="op">:</span> { d<span class="op">:</span> <span class="fl">10.0</span> } }          </span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;</span> r2 <span class="ot">=</span> { a<span class="op">:</span> <span class="dv">10</span>, b<span class="op">:</span> <span class="fl">2.0</span>, c<span class="op">:</span> { d<span class="op">:</span> <span class="fl">20.0</span> } }</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;</span> lerp <span class="fl">0.5</span> r1 r2</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>{ a<span class="op">:</span> <span class="dv">6</span>, b<span class="op">:</span> <span class="fl">1.5</span>, c<span class="op">:</span> { d<span class="op">:</span> <span class="fl">15.0</span> } }</span></code></pre></div>
<p>The journey of getting there had striking similarities to the other little
adventures I blogged about, namely <a href="/2022/03/20/unique-heterogeneous-typelists/">mapping over heterogeneous type lists in
Haskell</a>, or <a href="https://blog.galowicz.de/2016/06/16/cpp_template_compile_time_brainfuck_interpreter/">implementing a
compile-time Brainfuck interpreter in C++ template language</a>.
In many ways, generic data handling is similar in many programming languages.</p>
<p>In PureScript, <a href="https://github.com/purescript/documentation/blob/master/language/Records.md">records</a>
are a data type that can be transformed into a <a href="https://pursuit.purescript.org/builtins/docs/Prim.RowList"><code>RowList</code></a> at compile time.
A <code>RowList</code> is a <a href="https://en.wikipedia.org/wiki/Cons">cons-style</a> list
of items that point recursively to the next item until the last item points to
a special <code>Nil</code> terminator item.
In the <code>RowList</code> case, every item practically conveys the key and the value of
each record member.</p>
<p>This means that we need to implement a type class that decomposes a record into
a heterogeneous list of items that then recursively get mapped over our <code>lerp</code>
function:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">LerpRecord</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    (<span class="ot">rl ::</span> <span class="dt">RL.RowList</span> <span class="dt">Type</span>)              <span class="co">-- 1.) Class Parameters</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    (<span class="ot">r ::</span> <span class="dt">Row</span> <span class="dt">Type</span>)                      <span class="co">-- </span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    (<span class="ot">from ::</span> <span class="dt">Row</span> <span class="dt">Type</span>)                   <span class="co">--</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    (<span class="ot">to ::</span> <span class="dt">Row</span> <span class="dt">Type</span>)                     <span class="co">--</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>  <span class="op">|</span> rl <span class="ot">-&gt;</span> r from to                      <span class="co">-- 2.) Functional dependency</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>  <span class="kw">where</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>  lerpRecordImpl                         <span class="co">-- 3.) Interface </span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="ot">    ::</span> <span class="dt">Number</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    <span class="ot">-&gt;</span> <span class="dt">Proxy</span> rl</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    <span class="ot">-&gt;</span> <span class="dt">Record</span> r</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    <span class="ot">-&gt;</span> <span class="dt">Record</span> r</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    <span class="ot">-&gt;</span> <span class="dt">Builder</span> { <span class="op">|</span> from } { <span class="op">|</span> to }</span></code></pre></div>
<p>This looks very complicated, so let’s go through it step by step:
Beginning with line 3.) we see the signature of the function that we create.
This function accepts a proxy of a <code>RowList</code> and the two same-typed records that
we are going to lerp.
It returns a function that constructs a record with lerped fields and will be
later consumed by the <a href="https://pursuit.purescript.org/packages/purescript-record/3.0.0/docs/Record.Builder#v:build"><code>build</code></a> function of the <code>purescript-record</code> library.
We get to the builder pattern after the next code excerpt.</p>
<p>The first four lines that are commented with 2.) are the type class parameters,
and thanks to the
<a href="https://github.com/purescript/documentation/blob/master/language/Types.md#kind-system">kind system</a>
in PureScript, they are constrained to what we intend this class for.
The <a href="https://github.com/purescript/documentation/blob/master/language/Type-Classes.md#functional-dependencies">functional dependency</a>
in 2.) says that all types can be deduced from the
<code>rl</code> type, so the compiler can catch us filling in the wrong types.</p>
<p>Using this class, we can now create a function <code>lerpRecord</code> that has the same
interface as our basic <code>lerp</code> function, but for records:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>lerpRecord</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="ot">  ::</span> <span class="kw">forall</span> t r</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>   <span class="op">.</span> <span class="dt">RL.RowToList</span> r t</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>  <span class="ot">=&gt;</span> <span class="dt">LerpRecord</span> t r () r</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>  <span class="ot">=&gt;</span> <span class="dt">Number</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>  <span class="ot">-&gt;</span> <span class="dt">Record</span> r</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>  <span class="ot">-&gt;</span> <span class="dt">Record</span> r</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>  <span class="ot">-&gt;</span> <span class="dt">Record</span> r</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>lerpRecord n a b <span class="ot">=</span> Builder.build builder {}</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>  <span class="kw">where</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>  rowList <span class="ot">=</span> <span class="dt">Proxy</span><span class="ot"> ::</span> _ t</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>  builder <span class="ot">=</span> lerpRecordImpl n rowList a b</span></code></pre></div>
<p>Here we can see that the <code>lerpRecordImpl</code> function makes us a <code>builder</code> function
from both input records <code>a</code> and <code>b</code>, which we then use to return the lerped
record to the end user.
The <code>Proxy</code> constructor helps us putting the type-level information of <code>t</code> into
the variable <code>rowList</code>, so we can feed it into the function <code>lerpRecordImpl</code>.
In the <code>LerpRecord t r () r</code> line, we say that we need a builder function that
gets us from an empty record “<code>()</code>” to the final record type <code>r</code>.
While <code>lerpRecordImpl</code> could create many other build functions, we select <em>this
one</em>.</p>
<p>Now, we can use the type class <code>LerpRecord</code> and the function <code>lerpRecord</code> to
make all lerpable records an instance of the earlier type class <code>Lerp</code>:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">instance</span><span class="ot"> lerpRecordInstance ::</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>  ( <span class="dt">RL.RowToList</span> a t</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>  , <span class="dt">LerpRecord</span> t a () a</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>  ) <span class="ot">=&gt;</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>  <span class="dt">Lerp</span> (<span class="dt">Record</span> a) <span class="kw">where</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>  lerp <span class="ot">=</span> lerpRecord</span></code></pre></div>
<p>The type constraints are the minimal guidance that the compiler needs to select
the right type class instance for us when we call <code>lerp</code> on a record.</p>
<p>We “only” need to implement <code>lerpRecordImpl</code> now.
The abort case on the end of the <code>RowList</code> is simple:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">instance</span><span class="ot"> lerpRecordNil ::</span> <span class="dt">LerpRecord</span> <span class="dt">RL.Nil</span> trashA () () <span class="kw">where</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>  lerpRecordImpl _ _ _ _ <span class="ot">=</span> identity</span></code></pre></div>
<p>When the recursion is called on the list tail, we return the <code>identity</code>
function.
Every build step is meant to transform from a smaller RowList to a bigger one,
by appending another item.
The <code>Nil</code> step just does nothing.
The other recursion steps are performed in one other implementation of the
<code>lerpRecordImpl</code> function, which is the most complicated but last piece of
today’s puzzle:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">instance</span><span class="ot"> lerpRecordCons ::</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>  ( <span class="dt">LerpRecord</span> t r from to&#39;           <span class="co">-- Constraints, 1</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>  , <span class="dt">IsSymbol</span> k                        <span class="co">-- 2</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>  , <span class="dt">Row.Cons</span> k a trashA r             <span class="co">-- 3</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>  , <span class="dt">Row.Cons</span> k a to&#39; to               <span class="co">-- 4</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>  , <span class="dt">Row.Lacks</span> k to&#39;                   <span class="co">-- 5</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>  , <span class="dt">Lerp</span> a</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>  ) <span class="ot">=&gt;</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>  <span class="dt">LerpRecord</span> (<span class="dt">RL.Cons</span> k a t) r from to <span class="kw">where</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>  lerpRecordImpl n _ a b <span class="ot">=</span> current <span class="op">&lt;&lt;&lt;</span> next</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">where</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    current <span class="ot">=</span> Builder.insert key lerpedValue</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    next <span class="ot">=</span> lerpRecordImpl n <span class="fu">tail</span> a b</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>    lerpedValue <span class="ot">=</span> lerp n (R.get key a) (R.get key b)</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>    key  <span class="ot">=</span> <span class="dt">Proxy</span><span class="ot"> ::</span> _ k</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    <span class="fu">tail</span> <span class="ot">=</span> <span class="dt">Proxy</span><span class="ot"> ::</span> _ t</span></code></pre></div>
<div class="floating-image-right">
<figure>
<img src="/images/recursion-koch-flake.webp" width="300" alt="Recursion is Beautiful" />
<figcaption aria-hidden="true">Recursion is Beautiful</figcaption>
</figure>
</div>
<p>Whenever <code>lerpRecordImpl</code> is called with some parameters, the compiler
“searches” for the right implementation.
The section commented with “Constraints” gives the compiler the following
constraints when asking “is this the right implementation for this call?”:</p>
<ol type="1">
<li>Given parameters match for the next recursion step, where <code>to'</code> is the <code>to</code>
version of the next deeper recursive call.</li>
<li>The key element of a <code>RowList</code> item is a “symbol”. The <code>R.get</code> function that
we use later needs that.</li>
<li>A <code>Cons</code> part of a <code>RowList</code> can be constructed from our key type <code>k</code>, the
value type <code>a</code> of one item and the record <code>r</code>.
<code>trashA</code> means that we don’t care what the tail is, as this constraint
is only for verifying that our <code>Cons</code> head contains what we need.</li>
<li>Prepending the key-value pair to the <code>to'</code> type leads to the current <code>to</code>
type in the builder chain that describes the recursion level we are currently
looking at.</li>
<li>The <code>to'</code> type in the builder chain does not contain the key <code>k</code> already.</li>
<li>The <code>a</code> type in the current <code>Cons</code> head has an instance of type class <code>Lerp</code>.</li>
</ol>
<p>The compiler will only select this function if all the constraints match.
It essentially creates two functions and returns their concatenation:</p>
<ul>
<li><code>current</code> is a builder function that inserts the lerped values for the current
record key at the current recursion level.</li>
<li><code>next</code> is the function that contains the same concatenation of the next
recursion level.</li>
</ul>
<p>What remains magic is the handling with the <code>Proxy</code> types:
We need them to get at the key value and the tail of the row list which gets
shorter with every recursive call.
The <code>Proxy</code> type constructor accepts type-level information and gives us nice
variables from that.
This way we can feed the <code>key</code> variable to the <code>R.get</code> function from the
<code>RowList</code> library to obtain the current record item for lerping, and use <code>tail</code>
as a parameter to <code>lerpRecordImpl</code> for the next recursion call.</p>
<p>The unit test for the final lerp of a nested record structure proves that it
works:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>it <span class="st">&quot;correctly lerps nested records&quot;</span> <span class="kw">do</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> a <span class="ot">=</span> { a<span class="op">:</span> <span class="fl">0.0</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>          , b<span class="op">:</span> <span class="dv">0</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>          , c<span class="op">:</span> { d<span class="op">:</span> <span class="fl">0.0</span> , e<span class="op">:</span> false }</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>          }</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>      b <span class="ot">=</span> { a<span class="op">:</span> <span class="fl">10.0</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>          , b<span class="op">:</span> <span class="dv">10</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>          , c<span class="op">:</span> { d<span class="op">:</span> <span class="fl">10.0</span> , e<span class="op">:</span> true }</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>          }</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>      c <span class="ot">=</span> { a<span class="op">:</span> <span class="fl">7.0</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>          , b<span class="op">:</span> <span class="dv">7</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>          , c<span class="op">:</span> { d<span class="op">:</span> <span class="fl">7.0</span> , e<span class="op">:</span> true }</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>          }</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>  lerp <span class="fl">0.7</span> a b <span class="ot">`shouldEqual`</span> c</span></code></pre></div>
<h2 id="summary">Summary</h2>
<p>We ended up with more code than the “manual” record lerp function, but this
little quest was great for learning how to do it.
Two type classes later we got a function that works even on nested records.</p>
<p>Interestingly, such type-level programs look similar in all the languages that
provide sufficiently mighty type systems, so this is nothing that one learns
for one language and then never needs again.</p>
<p>While I was learning how to do this, other PureScript users from the
<a href="https://purescript.org/chat">PureScript Discord Server</a> pointed me to the
<a href="https://github.com/natefaubion/purescript-heterogeneous/"><code>purescript-heterogeneous</code></a>
library:
This library provides functionality for mapping over heterogeneous lists.
Essentially, a <code>RowList</code> is a heterogeneous list, so by using this library it
should be possible to provide the same functionality with less code (by
transforming a record to a <code>RowList</code>, then use such libraries on that, and
convert it back to a record).
But this would be material for another blog article.</p>
<p>The full working code together with build instructions is on GitHub:
<a href="https://github.com/tfc/purescript-ziprecord" class="uri">https://github.com/tfc/purescript-ziprecord</a></p>]]></summary>
</entry>
<entry>
    <title>Mixed C++ Monorepo Project Structure Development and Build Workflow</title>
    <link href="https://blog.galowicz.de/2023/01/23/mixed-cpp-monorepo-project" />
    <id>https://blog.galowicz.de/2023/01/23/mixed-cpp-monorepo-project</id>
    <published>2023-01-23T00:00:00Z</published>
    <updated>2023-01-23T00:00:00Z</updated>
    <summary type="html"><![CDATA[<!-- cSpell:words Pfeifer monorepo composability buildable ccache -->
<!-- cSpell:ignore iostream cout liba libb libc libd subproject endmacro -->
<!-- cSpell:ignore myapp AARCH vdso libstdc libm aarch SYSV stdenv -->
<!-- cSpell:ignore nixified frontends libgcc -->
<p>Most big C++ projects lack a clear structure:
They consist of multiple modules, but it is not as easy to create individually
buildable, portable, testable, and reusable libraries from them, as it is with
projects written in Rust, Go, Haskell, etc.
In this article, I propose a C++ project structure using CMake that makes it
easy to have incremental monorepo builds and a nice modular structure at the
same time.</p>
<!--more-->
<p>Let me show you how to structure a concrete example C++ project in a way that
works well for different perspectives while putting no burden on any
participating party.
First, we have a look at the different perspectives and then how to realize a
project structure that facilitates them.
In the end, we will build individual modules with nix - it’s possible to do it
without nix, but it would also be much more work.</p>
<h2 id="goal-maximum-utility-for-everyone">Goal: Maximum Utility for Everyone</h2>
<p>There are different perspectives from which a repository can provide value or
friction.
Most of the time, the people who write the code have their dominating share in
deciding how the structure of a project looks like: Developers.</p>
<h3 id="incremental-builds-for-developers">Incremental Builds for Developers</h3>
<p>No matter what changed where in the monorepo, there should be one command that
just builds and tests “it all” incrementally.
From this perspective, it seems like the best way is to have one build system
that sets up dependencies and builds everything in a highly parallelized
fashion.</p>
<h3 id="fast-cicd-pipelines">Fast CI/CD Pipelines</h3>
<div class="floating-image-right">
<p><img src="/images/traffic-jam.webp" width="300" /></p>
</div>
<p>What’s best for developers performing incremental changes, is not automatically
good for CI/CD:
When changes go into a project by pushing onto a branch and/or opening a merge
request, CI/CD pipelines are triggered.
In monolithic builds, this means that for every changed code line or comment,
the whole repo is rebuilt.
Caching compiler frontends like <a href="https://ccache.dev/"><code>ccache</code></a> might be of help
in such situations, but caching at this level brings its own pitfalls and is not
applicable in all situations.</p>
<p>Structuring a project towards modularity and providing a way to build modules
individually is a good way to accelerate big pipelines:
Small module-internal changes only trigger the builds of the modules and their
dependents.
In some sense, this way we can facilitate incremental builds on a higher level.</p>
<h3 id="modular-architecture">Modular Architecture</h3>
<div class="floating-image-right">
<p><img src="/images/modularity-puzzle.gif" width="300" /></p>
</div>
<p>Generally, developers should try to not mix different domains in the same code.
Mixing domains hinders reusability and makes it harder to inspect for
correctness.
Having code for different domains separated over different modules also
facilitates testing:
Each module can have its unit test suite.
Often enough, I observed how easy it was to reduce the overall number of test
cases and their complexity by decomposing library code.
The best proof of the independence of a module is building it in isolation
without the surrounding code.</p>
<p>In multiple C++ projects that I have seen, “reusing library code” looked like
this:</p>
<ol type="1">
<li>A developer would like to reuse an existing library for some new application.</li>
<li>The library is part of a big monorepo and is not easy to extract.
Maybe it could be extracted by duplicating it, but duplication is considered
bad in all circumstances.</li>
<li>Deadlines need to be met, so the compromise is to put the new application
into the same monorepo.</li>
<li>Over the years, the coupling and amount of specialization of the libraries
for the resident applications increases the general complexity.</li>
<li>At some point, a developer will repeat step 1.
This time, the slope towards the same procedure is even steeper.</li>
</ol>
<p>Pointing this out in projects often leads to discussions where the problem is
not fully realized by the majority and hence downplayed.
In fact, convincing people of architectural improvements is often hard, because
this is a strategic consideration with long-term impact and not a tactical one.
The <a href="https://blog.galowicz.de/2022/12/05/book-review-a-philosophy-of-software-design/">book “A Philosophy of Software Design” by John Ousterhout</a>
provides some very interesting insights on strategic vs. tactical programmers.</p>
<p>A good way to convince colleagues is generating an architecture graph and
showing them that it contains many circular subgraphs.
No one is proud of projects with architectural graphs that look like a dish of
spaghetti.
However, generating architecture graphs is harder, the less modularized the
project is - it’s a catch-22 situation.</p>
<h2 id="monolithic-repo-monolithic-build-system">Monolithic Repo == Monolithic Build System?</h2>
<div class="floating-image-right">
<figure>
<img src="/images/monolith.webp" alt="Monoliths can help humanity but also be dangerous" />
<figcaption aria-hidden="true">Monoliths can help humanity but also be dangerous</figcaption>
</figure>
</div>
<p>So we want to achieve that our C++ project structure is good for developers
<em>and</em> pipelines, integrators, (re)users of library code, etc.
How do we get there?
It should be incrementally buildable for developers when they work at it, but
also be modular with all the good things that we mentioned before.</p>
<p>I created a minimal example app that has a somewhat complicated but clean
dependency structure.
The code is already available on GitHub as a whole:
<a href="https://github.com/tfc/cmake_cpp_example" class="uri">https://github.com/tfc/cmake_cpp_example</a></p>
<p>Starting the final app produces the following output:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> ./app/MyApp</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="ex">abcd</span></span></code></pre></div>
<p>For every letter, it calls one of four library calls:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="pp">#include </span><span class="im">&lt;a.hpp&gt;</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="pp">#include </span><span class="im">&lt;b.hpp&gt;</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="pp">#include </span><span class="im">&lt;d.hpp&gt;</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="pp">#include </span><span class="im">&lt;iostream&gt;</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="dt">int</span> main<span class="op">()</span> <span class="op">{</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>  <span class="bu">std::</span>cout<span class="op"> &lt;&lt;</span> liba<span class="op">::</span>function_a<span class="op">()</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>            <span class="op">&lt;&lt;</span> libb<span class="op">::</span>function_b<span class="op">()</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>            <span class="op">&lt;&lt;</span> liba<span class="op">::</span>function_c<span class="op">()</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>            <span class="op">&lt;&lt;</span> libd<span class="op">::</span>function_d<span class="op">()</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>            <span class="op">&lt;&lt;</span> <span class="ch">&#39;</span><span class="sc">\n</span><span class="ch">&#39;</span><span class="op">;</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<div class="floating-image-right">
<figure>
<img src="https://raw.githubusercontent.com/tfc/cmake_cpp_example/master/deptree.png" alt="The abcd project dependency structure. Dashed arrows are private deps." />
<figcaption aria-hidden="true">The abcd project dependency structure. Dashed arrows are private deps.</figcaption>
</figure>
</div>
<p>The little architecture diagram on the right shows that libraries <code>A</code> and <code>B</code>
are direct dependencies, while <code>C</code>’s symbols are private to <code>A</code>, but
re-exported by its header.
<code>D</code> can be transitively reached because it is a public dependency of <code>A</code>.</p>
<p>The project structure looks like this, where all libraries and the app are
located next to each other at the same level of the repository structure:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="bu">.</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="ex">├──</span> a</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="ex">│  </span> ├── CMakeLists.txt</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="ex">│  </span> ├── include</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="ex">│  </span> │   └── a.hpp</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="ex">│  </span> └── main.cpp</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="ex">├──</span> b</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="ex">│  </span> └── <span class="co"># similar to `a`</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="ex">├──</span> c</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="ex">│  </span> └── <span class="co"># similar to `a`</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="ex">├──</span> d</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="ex">│  </span> └── <span class="co"># similar to `a`</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="ex">├──</span> app</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="ex">│  </span> ├── CMakeLists.txt</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="ex">│  </span> └── main.cpp</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="ex">├──</span> CMakeLists.txt</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="ex">└──</span> README.md</span></code></pre></div>
<p>Every subproject has its <code>CMakeLists.txt</code> that stands for itself as a
standalone project and does not know the location of the others:
<a href="https://cmake.org/">CMake</a> allows us to describe libraries in an
object-oriented way where they get symbol names that can be exported to be
imported by others.
Let’s look at the <code>CMakeLists.txt</code> file of library <code>A</code>:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode cmake"><code class="sourceCode cmake"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">cmake_minimum_required</span>(<span class="ot">VERSION</span> <span class="dt">3.13</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Note that this is a *standalone project* although it&#39;s in a subfolder of a</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># monorepo</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="kw">project</span>(A <span class="ot">CXX</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Other modules can reference me as A::A</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="kw">add_library</span>(<span class="bn">A</span> main.cpp)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="kw">add_library</span>(<span class="bn">A::A</span> <span class="ot">ALIAS</span> <span class="bn">A</span>) <span class="co"># this alias is explained later</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="kw">target_include_directories</span>(</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>  <span class="bn">A</span> <span class="ot">PUBLIC</span> <span class="ot">$&lt;</span><span class="kw">BUILD_INTERFACE</span><span class="ot">:</span><span class="dv">${CMAKE_CURRENT_SOURCE_DIR}</span><span class="ot">/include&gt;</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>           <span class="ot">$&lt;</span><span class="kw">INSTALL_INTERFACE</span><span class="ot">:include&gt;</span>)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co"># We need library C and D. But what we don&#39;t need is knowing from where.</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="kw">find_package</span>(C <span class="ot">REQUIRED</span>)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="kw">find_package</span>(D <span class="ot">REQUIRED</span>)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="kw">target_link_libraries</span>(</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>  <span class="bn">A</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>  <span class="ot">PRIVATE</span> <span class="bn">C::C</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>  <span class="ot">PUBLIC</span> <span class="bn">D::D</span>)</span></code></pre></div>
<p>With this structure, we have a library object that describes what it needs, but
not where it comes from.
CMake is able to discover dependencies via <code>pkg-config</code> or CMake-native files
when they are located in the standard system folders or listed in the
environment variable <a href="https://cmake.org/cmake/help/latest/command/find_package.html"><code>CMAKE_MODULE_PATH</code></a>.
If we use this mechanism instead of hardcoding paths, we buy ourselves a lot of
freedom.
I only omitted some lines that would be needed for running <code>make install</code>, but
you can look <a href="https://github.com/tfc/cmake_cpp_example/blob/master/a/CMakeLists.txt">them up in the GitHub repo</a>.</p>
<p>All libraries and the app have similar simple <code>CMakeLists.txt</code> files which
describe what they need, but not where they come from.
We could now build them one after the other in the right order to get the final
application executable and provide the binaries (App + libraries as shared
object files, or just the app if it is statically linked) as packages.
This would be more of an end-user scenario.
For other developers, we can also publish them as <a href="https://conan.io/">Conan</a>
packages.</p>
<p>The workflow that we want to provide for developers is building everything in
one invocation of <code>cmake</code> and <code>make</code>:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> mkdir build <span class="kw">&amp;&amp;</span> <span class="bu">cd</span> build</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> cmake ..</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> make</span></code></pre></div>
<p>To enable this with our loosely coupled collection of libraries we can create a
<code>CMakeLists.txt</code> file in the top-level directory of the repository:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode cmake"><code class="sourceCode cmake"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">cmake_minimum_required</span>(<span class="ot">VERSION</span> <span class="dt">3.13</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="kw">project</span>(abc_example <span class="ot">CXX</span>)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Define which library symbols are local sub-projects</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="kw">set</span>(as_subproject A B C D)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Make find_package a dummy function for these sub-projects.</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="kw">macro</span>(find_package)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>  <span class="kw">if</span>(<span class="ot">NOT</span> <span class="st">&quot;</span><span class="dv">${ARGV0}</span><span class="st">&quot;</span> <span class="ot">IN_LIST</span> as_subproject)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># This is a forward call to the *original* find_package function</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    <span class="fu">_find_package</span>(<span class="dv">${ARGV}</span>)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>  <span class="kw">endif</span>()</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="kw">endmacro</span>()</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="kw">add_subdirectory</span>(a)</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="kw">add_subdirectory</span>(b)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a><span class="kw">add_subdirectory</span>(c)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a><span class="kw">add_subdirectory</span>(d)</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a><span class="kw">add_subdirectory</span>(app)</span></code></pre></div>
<p>This way, we support the developer’s incremental workflow.
But at the same time, developers of other projects can reuse these libraries
without having to touch everything else.
If a library grows bigger and gets reused by multiple projects, it can easily be
moved to its own repository.</p>
<p>The idea to rewrite CMake’s
<a href="https://cmake.org/cmake/help/latest/command/find_package.html"><code>find_package</code></a>
function originates from
<a href="https://www.youtube.com/watch?v=bsXLMQ6WgIk">Daniel Pfeifer’s talk at C++Now 2017</a>.
The talk contains much advice on how to use CMake the correct way and is
generally a must-watch for every C and C++ developer.</p>
<p>The one thing that each library needs to do to prepare for being consumed this
way, is to provide an alias like this:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode cmake"><code class="sourceCode cmake"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">add_library</span>(<span class="bn">A::A</span> <span class="ot">ALIAS</span> <span class="bn">A</span>)</span></code></pre></div>
<p>Libraries that are imported using <code>find_package</code> get a <code>MyLib::MyLib</code> scope,
but libraries that are imported using <code>add_subdirectory</code> don’t, so we have to
add it like this.</p>
<h2 id="result">Result</h2>
<p>Incremental developer-oriented builds do now work with the typical <code>cmake</code> and
<code>make</code> commands.
At the same time, this does not hinder anyone to create Debian/RPM/whatever
packages for the individual libraries.</p>
<p>In order to show composability and packaging the nix way, I created nix
derivations for each module, which look quite similar to the respective
<code>CMakeLists.txt</code> files but from a higher level:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode nix"><code class="sourceCode nix"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># project file: a/default.nix</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="op">{</span> <span class="va">stdenv</span><span class="op">,</span> <span class="va">cmake</span><span class="op">,</span> <span class="va">libc</span><span class="op">,</span> <span class="va">libd</span> <span class="op">}</span>:</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>stdenv.mkDerivation <span class="op">{</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>  <span class="va">name</span> <span class="op">=</span> <span class="st">&quot;liba&quot;</span><span class="op">;</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>  <span class="va">buildInputs</span> <span class="op">=</span> <span class="op">[</span> libc libd <span class="op">];</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>  <span class="va">nativeBuildInputs</span> <span class="op">=</span> <span class="op">[</span> cmake <span class="op">];</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>  <span class="va">src</span> <span class="op">=</span> <span class="ss">./.</span><span class="op">;</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>The nixified library packages depend just on CMake and respectively on each
other as depicted in the architecture diagram earlier.
An overlay file puts it all together:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode nix"><code class="sourceCode nix"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># file: overlay.nix</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="va">final</span><span class="op">:</span> <span class="va">prev</span><span class="op">:</span> <span class="op">{</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>  <span class="va">liba</span> <span class="op">=</span> final.callPackage <span class="ss">./a</span> <span class="op">{</span> <span class="op">};</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>  <span class="va">libb</span> <span class="op">=</span> final.callPackage <span class="ss">./b</span> <span class="op">{</span> <span class="op">};</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>  <span class="va">libc</span> <span class="op">=</span> final.callPackage <span class="ss">./c</span> <span class="op">{</span> <span class="op">};</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>  <span class="va">libd</span> <span class="op">=</span> final.callPackage <span class="ss">./d</span> <span class="op">{</span> <span class="op">};</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>  <span class="va">myapp</span> <span class="op">=</span> final.callPackage <span class="ss">./app</span> <span class="op">{</span> <span class="op">};</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>The <code>callPackage</code> function fills out all the parameters from the derivations’
parameters, which you see in the first line of the example derivation.
the <code>liba</code> derivation finds <code>libc</code> and <code>libd</code> in the global scope of packages,
because the overlay defines them, too.
This way we can import a version of the nixpkgs package list with the overlay:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode nix"><code class="sourceCode nix"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>pkgs = <span class="bu">import</span> &lt;nixpkgs&gt; <span class="op">{</span> <span class="va">overlays</span> <span class="op">=</span> <span class="op">[</span> <span class="op">(</span><span class="bu">import</span> <span class="ss">./overlay.nix</span><span class="op">)</span> <span class="op">];</span> <span class="op">}</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co"># we can now access pkgs.myapp, pkgs.liba, etc. ...</span></span></code></pre></div>
<p>The <a href="https://github.com/tfc/cmake_cpp_example/blob/master/release.nix"><code>release.nix</code> file</a>
imports a pinned version of nixpkgs and exposes all these build targets.
We can now build them individually:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> nix-build release.nix <span class="at">-A</span> liba</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> nix-build release.nix <span class="at">-A</span> libb</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> nix-build release.nix <span class="at">-A</span> libc</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> nix-build release.nix <span class="at">-A</span> libd</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> nix-build release.nix <span class="at">-A</span> myapp</span></code></pre></div>
<p>…which results in individual packages (of which the libraries could be
consumed by other projects, external to our repo).
Of course, building the <code>myapp</code> target attribute transitively builds all the
others that it depends on.</p>
<p>The most interesting thing that we can do at this point might be
cross-compilation:
I added a few targets to also link the app statically and cross-compile it for
AARCH64 and Windows, to demonstrate that the build system does not really need
to be prepared for this other than just being as portable as possible (mostly
by not holding CMake wrong):</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># default build</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> ldd <span class="va">$(</span><span class="ex">nix-build</span> release.nix <span class="at">-A</span> myapp<span class="va">)</span>/bin/MyApp</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>        <span class="ex">linux-vdso.so.1</span> <span class="er">(</span><span class="ex">0x00007fffced69000</span><span class="kw">)</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>        <span class="ex">libstdc++.so.6</span> =<span class="op">&gt;</span> .../libstdc++.so.6 <span class="er">(</span><span class="ex">0x00007f03ea419000</span><span class="kw">)</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>        <span class="ex">libm.so.6</span>      =<span class="op">&gt;</span> .../libm.so.6 <span class="er">(</span><span class="ex">0x00007f03ea339000</span><span class="kw">)</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>        <span class="ex">libgcc_s.so.1</span>  =<span class="op">&gt;</span> .../libgcc_s.so.1 <span class="er">(</span><span class="ex">0x00007f03ea31f000</span><span class="kw">)</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>        <span class="ex">libc.so.6</span>      =<span class="op">&gt;</span> .../libc.so.6 <span class="er">(</span><span class="ex">0x00007f03ea116000</span><span class="kw">)</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>        <span class="ex">.../ld-linux-x86-64.so.2</span> =<span class="op">&gt;</span> .../ld-linux-x86-64.so.2 <span class="er">(</span><span class="ex">0x00007f03ea631000</span><span class="kw">)</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="co"># static build</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> ldd <span class="va">$(</span><span class="ex">nix-build</span> release.nix <span class="at">-A</span> myapp-static<span class="va">)</span>/bin/MyApp</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>        <span class="ex">not</span> a dynamic executable</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Aarch64 build</span></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> file <span class="va">$(</span><span class="ex">nix-build</span> release.nix <span class="at">-A</span> myapp-aarch64<span class="va">)</span>/bin/MyApp</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a><span class="ex">...</span> ELF 64-bit LSB executable, ARM aarch64, version 1 <span class="er">(</span><span class="ex">SYSV</span><span class="kw">)</span><span class="ex">,</span> ⏎</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>    <span class="ex">dynamically</span> linked, interpreter .../ld-linux-aarch64.so.1, ⏎</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> GNU<span class="ex">/Linux</span> 3.10.0, not stripped</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Windows build</span></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> file <span class="va">$(</span><span class="ex">nix-build</span> release.nix <span class="at">-A</span> myapp-win64<span class="va">)</span>/bin/MyApp.exe</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a><span class="ex">...</span> PE32+ executable <span class="er">(</span><span class="ex">console</span><span class="kw">)</span> <span class="ex">x86-64</span> <span class="er">(</span><span class="ex">stripped</span> to external PDB<span class="kw">)</span><span class="ex">,</span> for MS Windows</span></code></pre></div>
<h2 id="summary">Summary</h2>
<p>What this project does on a high level is separate dependency management and
build management:
The CMake files resemble pure recipes which describe who depends on what.</p>
<p>Depending on the use case, the consumer decides how it all comes together by
either using the top-level CMake file or the nix dependency management.</p>
<p>Outside users can build the individual modules as packages and install them
in their systems globally, produce Conan packages, add them to docker images,
or VMs, or consume them via the nix overlay.</p>
<p>If a library grows bigger and gets imported by more outside users, it is much
easier now to move it into its own repository (and maybe open-source it).</p>]]></summary>
</entry>
<entry>
    <title>(Qt)Quick C++ Project Setup with Nix</title>
    <link href="https://blog.galowicz.de/2023/01/16/cpp-qt-qml-nix-setup" />
    <id>https://blog.galowicz.de/2023/01/16/cpp-qt-qml-nix-setup</id>
    <published>2023-01-16T00:00:00Z</published>
    <updated>2023-01-16T00:00:00Z</updated>
    <summary type="html"><![CDATA[<!-- cSpell:words devenv Dockerfiles cachix Flatpak -->
<!-- cSpell:ignore qtcreator qtbase shellhook bashdir mktemp stdenv pname -->
<!-- cSpell:ignore qtwebkit -->
<p>I never install toolchains globally on my systems.
Instead, every project comes with its own nix file that describes the complete
development toolchain versions and dependencies.
This way, fresh checkouts always build the same way on every machine.
This week I would like to show you how I set up a C++ project with the Qt Quick
framework, and how to package the app and make it runnable for other nix users.</p>
<!--more-->
<p>Nix typically allows you to run</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="ex">nix-shell</span> <span class="at">-p</span> someTool someOtherTool</span></code></pre></div>
<p>… to download everything and put you in a shell where everything is as if you
had these tools installed.
This time, I wanted to run
<a href="https://www.qt.io/product/development-tools">Qt Creator</a> to fiddle around with
some <a href="https://doc.qt.io/qt-6/qtquick-index.html">Qt Quick</a> ideas.</p>
<h2 id="getting-qt-creator-up-and-running">Getting Qt Creator Up and Running</h2>
<p>For that purpose, running</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="ex">nix-shell</span> <span class="at">-p</span> cmake qt6.full qtcreator</span></code></pre></div>
<p>… got me Qt Creator, the Qt6 libraries, and CMake as a build system.
This is already enough to get Qt Creator up and running, create a new QtQuick
UI project in the project wizard with CMake as its build system, and actually
<em>build</em> it using the UI menu buttons in Qt Creator, which was already a nice
and uncomplicated user experience.</p>
<figure>
<img src="/images/qt-quick-wizard.png" alt="Qt Creator’s project wizard" />
<figcaption aria-hidden="true">Qt Creator’s project wizard</figcaption>
</figure>
<p>At first, Qt tried to use the wrong toolchain: I found out that my home folder
still had some older configuration state from an earlier Qt Creator version
installed.
Quickly running <code>rm -rf ~/.config/QtProject*</code> helped out:
Qt Creator chose the right Qt libraries the next time I ran the wizard.
<em>Now</em> it was able to build the app.</p>
<p>Unfortunately, <em>running</em> the application still produced the following output:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="ex">QQmlApplicationEngine</span> failed to load component</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="ex">qrc:/main.qml:2:1:</span> module <span class="st">&quot;QtQuick.Window&quot;</span> is not installed</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="ex">qrc:/main.qml:</span> module <span class="st">&quot;QtQml.WorkerScript&quot;</span> is not installed</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="ex">qrc:/main.qml:2:1:</span> module <span class="st">&quot;QtQuick.Window&quot;</span> is not installed</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="ex">qrc:/main.qml:</span> module <span class="st">&quot;QtQml.WorkerScript&quot;</span> is not installed</span></code></pre></div>
<p>After briefly researching this, it turned out that QtQuick apps require the
environment variable <code>QML2_IMPORT_PATH</code> to be set.
On computers where Qt is installed the <em>normal</em> way, this variable is already
set globally.
As there are plenty of Qt5 and Qt6 packages in NixOS already, we can just reuse
the scripts to get the same effect for our developer shell.</p>
<p>To get packages into the shell environment <em>and</em> add environment variables, the
function <code>mkShell</code> from nixpkgs can help us.
To manipulate more than the list of packages that are available in a nix-shell,
we need to set up a nix expression that describes the whole shell environment,
like this <code>flake.nix</code> file:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode nix"><code class="sourceCode nix"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>  <span class="va">inputs</span>.<span class="va">nixpkgs</span>.<span class="va">url</span> <span class="op">=</span> <span class="st">&quot;github:nixos/nixpkgs&quot;</span><span class="op">;</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>  <span class="va">outputs</span> <span class="op">=</span> <span class="op">{</span> <span class="va">self</span><span class="op">,</span> <span class="va">nixpkgs</span> <span class="op">}</span>:</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="va">pkgs</span> <span class="op">=</span> nixpkgs.legacyPackages.x86_64<span class="op">-</span>linux<span class="op">;</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>  <span class="kw">in</span> <span class="op">{</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    <span class="va">devShells</span>.<span class="va">x86_64-linux</span>.<span class="va">default</span> <span class="op">=</span> pkgs.mkShell <span class="op">{</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>      <span class="va">buildInputs</span> <span class="op">=</span> <span class="kw">with</span> pkgs<span class="op">;</span> <span class="op">[</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        cmake</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>        gdb</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>        qt6.full</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>        qt6.qtbase</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>        qtcreator</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># this is for the shellhook portion</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>        qt6.wrapQtAppsHook</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>        makeWrapper</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>        bashInteractive</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>      <span class="op">];</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>      <span class="co"># set the environment variables that Qt apps expect</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>      <span class="va">shellHook</span> <span class="op">=</span> <span class="st">&#39;&#39;</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="st">        bashdir=$(mktemp -d)</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a><span class="st">        makeWrapper &quot;$(type -p bash)&quot; &quot;$bashdir/bash&quot; &quot;</span><span class="sc">&#39;&#39;$</span><span class="st">{qtWrapperArgs[@]}&quot;</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="st">        exec &quot;$bashdir/bash&quot;</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a><span class="st">      &#39;&#39;</span><span class="op">;</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>    <span class="op">};</span></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>  <span class="op">};</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<blockquote>
<p>I am not explaining everything in this article.
Flakes in general are described
<a href="https://nixos.org/manual/nix/stable/command-ref/new-cli/nix3-flake.html">in the nix documentation</a>,
and the <code>mkShell</code> function is described
<a href="https://nixos.org/manual/nixpkgs/stable/#sec-pkgs-mkShell">in the nixpkgs documentation</a>.</p>
</blockquote>
<p>Running the command</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="ex">nix</span> develop</span></code></pre></div>
<p>… in the same folder as this flake file adds CMake, the full Qt6 library
package set, Qt Creator, the GDB debugger, and also runs the facilities from
nixpkgs that are used to equip Qt applications with needed environment variables
to the local shell.
We are not packaging anything yet, but running the <code>nix develop</code> shell with this
flake gives us a Qt Creator that can <em>build</em>, <em>run</em>, and <em>debug</em> our QtQuick
app.
(I found the <code>shellHook</code> part in the
<a href="https://discourse.nixos.org/t/python-qt-woes/11808">NixOS Discourse</a>)</p>
<blockquote>
<p>If the <code>nix develop</code> command does not work for you, it might be the case that
the nix “flakes” feature is not yet enabled by default on your system.
To fix that temporarily, add <code>--experimental-features 'nix-command flakes'</code> to
your command line, or have a look
<a href="https://nixos.wiki/wiki/Flakes#Enable_flakes">here</a> to see how to enable
flakes permanently.</p>
</blockquote>
<p>Running <code>nix develop</code> for the first time will download everything that is needed
and can take some time depending on the internet connection.
Running it another time is instant.
Every collaborator on this project will have a much easier time developing this
package than with downloading, installing, and configuring Qt manually.</p>
<p>During the first run, the file <code>flake.lock</code> was created:
This is similar to lock files in other development environments.
When we give this project folder to a colleague and they run <code>nix develop</code>, they
will get <em>exactly</em> the same version as ours.
They will also be able to add more packages without having to rebuild the
environment as a whole (which is often the case with Dockerfiles).</p>
<p>The environment can now be updated using <code>nix flake update</code>, which is useful in
git repositories where you can first check out if the update works, and <em>then</em>
commit and push the change (updates and fixes in the same atomic commit).</p>
<h2 id="packaging-the-app">Packaging the App</h2>
<p>The example app that I prepared for this blog resides in the GitHub repo
<a href="https://github.com/tfc/qt-example" class="uri">https://github.com/tfc/qt-example</a>
and looks like this:</p>
<figure>
<img src="https://raw.githubusercontent.com/tfc/qt-example/main/app-screenshot.png" alt="The QtQuick example app" />
<figcaption aria-hidden="true">The QtQuick example app</figcaption>
</figure>
<p>Let’s assume this repository structure for now:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="bu">.</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="ex">├──</span> build.nix</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="ex">├──</span> flake.lock</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="ex">├──</span> flake.nix</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="ex">├──</span> qt-example <span class="co"># subfolder as created by the Qt Creator wizard</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="ex">│  </span> ├── CMakeLists.txt</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="ex">│  </span> ├── images</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="ex">│  </span> │   ├── nix.svg</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="ex">│  </span> │   └── qt.svg</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="ex">│  </span> ├── main.cpp</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="ex">│  </span> ├── main.qml <span class="co"># I only tweaked the window, see screenshot</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="ex">│  </span> └── qml.qrc</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="ex">└──</span> README.md</span></code></pre></div>
<p>To package this app the nix way I wrote a <code>build.nix</code> file that looks like this:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode nix"><code class="sourceCode nix"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="op">{</span> <span class="va">stdenv</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="op">,</span> <span class="va">qtbase</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="op">,</span> <span class="va">full</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="op">,</span> <span class="va">cmake</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="op">,</span> <span class="va">wrapQtAppsHook</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="op">}</span>:</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>stdenv.mkDerivation <span class="op">{</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>  <span class="va">pname</span> <span class="op">=</span> <span class="st">&quot;qt-example&quot;</span><span class="op">;</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>  <span class="va">version</span> <span class="op">=</span> <span class="st">&quot;1.0&quot;</span><span class="op">;</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>  <span class="co"># The QtQuick project we created with Qt Creator&#39;s project wizard is here</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>  <span class="va">src</span> <span class="op">=</span> <span class="ss">./qt-example</span><span class="op">;</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>  <span class="va">buildInputs</span> <span class="op">=</span> <span class="op">[</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    qtbase</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    full</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>  <span class="op">];</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>  <span class="va">nativeBuildInputs</span> <span class="op">=</span> <span class="op">[</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    cmake</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>    wrapQtAppsHook</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>  <span class="op">];</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>  <span class="co"># If the CMakeLists.txt has an install step, this installPhase is not needed.</span></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>  <span class="co"># The Qt default project however does not have one.</span></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>  <span class="va">installPhase</span> <span class="op">=</span> <span class="st">&#39;&#39;</span></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a><span class="st">    mkdir -p $out/bin</span></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a><span class="st">    cp qt-example $out/bin/</span></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a><span class="st">  &#39;&#39;</span><span class="op">;</span></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>Side note: The <code>wrapQtAppsHook</code> automatically wraps the application after the
build phase into a script that sets all needed Qt-related environment variables.
This way users don’t need to fiddle with env vars just to run our app.</p>
<p>This build recipe is separate from the <code>flake.nix</code> file because it only decides
what to do with given inputs to create a package but not where the packages come
from and how they are selected.
Our flake now calls this recipe like this:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode nix"><code class="sourceCode nix"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>  <span class="va">inputs</span> <span class="op">=</span> <span class="op">{</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    <span class="va">nixpkgs</span>.<span class="va">url</span> <span class="op">=</span> <span class="st">&quot;github:nixos/nixpkgs&quot;</span><span class="op">;</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>  <span class="op">};</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>  <span class="va">outputs</span> <span class="op">=</span> <span class="op">{</span> <span class="va">self</span><span class="op">,</span> <span class="va">nixpkgs</span> <span class="op">}</span>:</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    <span class="va">pkgs</span> <span class="op">=</span> nixpkgs.legacyPackages.x86_64<span class="op">-</span>linux<span class="op">;</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>  <span class="kw">in</span> <span class="op">{</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># This is our new package for end-users</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    <span class="va">packages</span>.<span class="va">x86_64-linux</span>.<span class="va">default</span> <span class="op">=</span> pkgs.qt6Packages.callPackage <span class="ss">./build.nix</span> <span class="op">{};</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    <span class="va">devShells</span>.<span class="va">x86_64-linux</span>.<span class="va">default</span> <span class="op">=</span> pkgs.mkShell <span class="op">{</span> </span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>      <span class="co"># The shell references this package to provide its dependencies</span></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>      <span class="va">inputsFrom</span> <span class="op">=</span> <span class="op">[</span> self.packages.x86_64-linux.default <span class="op">];</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>      <span class="va">buildInputs</span> <span class="op">=</span> <span class="kw">with</span> pkgs<span class="op">;</span> <span class="op">[</span></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>        gdb</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>        qtcreator</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># this is for the shellhook portion</span></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>        qt6.wrapQtAppsHook</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>        makeWrapper</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>        bashInteractive</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>      <span class="op">];</span></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>      <span class="co"># set the environment variables that unpatched Qt apps expect</span></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>      <span class="va">shellHook</span> <span class="op">=</span> <span class="st">&#39;&#39;</span></span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a><span class="st">        bashdir=$(mktemp -d)</span></span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a><span class="st">        makeWrapper &quot;$(type -p bash)&quot; &quot;$bashdir/bash&quot; &quot;</span><span class="sc">&#39;&#39;$</span><span class="st">{qtWrapperArgs[@]}&quot;</span></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a><span class="st">        exec &quot;$bashdir/bash&quot;</span></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a><span class="st">      &#39;&#39;</span><span class="op">;</span></span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>    <span class="op">};</span></span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>  <span class="op">};</span></span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>You might have noticed that I removed all the <code>buildInputs</code> from the development
shell definition that are also already part of the package definition in
<code>build.nix</code>.
Instead, we get these inputs via the <code>inputsFrom</code> function that simply reuses
what’s defined in the list of packages that we assign.</p>
<p>We assign the <code>pkgs.qt6Packages.callPackage ./build.nix {}</code> call to be the
default package’s result.
(What’s also cool: Instead of using <code>qt6Packages</code>, we could also build a qt5
version of this recipe without changing it)
This way we can run</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="ex">nix</span> build</span></code></pre></div>
<p>… and get the pre-packaged application in the <code>results</code> symlink that nix
produced for us.
The best about this is that we do <em>not</em> need to run a nix-shell to run the app
(we needed to do this before because of the <code>QML2_IMPORT_PATH</code> environment
variable that is only set on systems where Qt is installed).</p>
<p>Even without cloning the repo at all, we can run get the app running on our
desktop without fiddling with environment variables, installing Qt, or even
<em>knowing</em> that the app runs with Qt or what Qt is:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="ex">nix</span> run github:tfc/qt-example</span></code></pre></div>
<p>The only prerequisite is Nix and an internet connection for the first attempt.</p>
<p>Limiting the target group to “nix users” and throwing them a
<code>nix run github:...</code> over the fence works, but still could take time to download
all the toolchain packages and build the application, which is a burden for
users who want it to just work immediately.
To reduce downloads and build times of toolchains and dependencies for building
the package, it is possible to <em>cache</em> the final build results.
Nix caches can be set up on any web server, but also nice tooling and services
like <a href="https://www.cachix.org/">cachix.org</a> exist which are even free to some
extent for open-source projects.
(Flakes can hint at caches so users don’t have to configure them manually)</p>
<h3 id="non-nix-packaging">Non-Nix Packaging</h3>
<div class="floating-image-right">
<p><img src="/images/where-is-my-package.webp" width="200" /></p>
</div>
<p>Of course, one could argue that just providing a nix flake for something, and
telling them to configure some cache to be used, does
not mean it is “packaged” now, and limiting the user base to nix users might be
considered unacceptable even if
<a href="https://twitter.com/jgalowicz/status/1591345129817784324">nixpkgs is in the top 10 of GitHub projects</a>
and the
<a href="https://repology.org/repositories/graphs">biggest and most up-to-date package distribution in the world</a>.</p>
<p>If we want or need to support non-nix users, we still can build Docker images,
(cross-)compile our apps, and link them statically, etc.
There is also <a href="https://github.com/matthewbauer/nix-bundle">nix-bundle</a>, a tool
that wraps the executable into another executable that unpacks itself with all
dependencies at runtime.
There is also <a href="https://flatpak.org/">Flatpak</a> and
<a href="https://appimage.org">AppImage</a>.
This is all possible (did these things in the past successfully for customers
and friends) but out of the scope of this blog article.
However, if you control the target platform (which most *aaS and embedded
systems companies do), nix is arguably a great choice for your deployment
cycles.
No part of the project is nix-specific - we can ignore the nix files that we put
into the repository.
This way we could still use other tooling for packaging parallel to nix, but at
least have simplified developer setup, workflow, building, testing,
toolchain, and dependency updates, quite a lot.</p>
<h2 id="summary">Summary</h2>
<p>Being a seasoned nix user, I came up with the initial nix flake in a few
minutes.
The final build recipe is some ~50 lines of nix code (following pretty much
standard patterns of the nix world), which might also be longer than your
average <code>Dockerfile</code>.
This first looks like a bigger and much more time-consuming step for nix
newbies, which is going to get better over time more and more nix-related
tooling is emerging in the last few years.
(One example is <a href="https://devenv.sh/">devenv</a>, a new development environment
tool that aims to simplify project setup and more. I did not try it, yet.)
However, this got us not only a development environment but also a way to
package our app, and some more advantages:</p>
<p>As soon as we exit the nix-shell, we don’t have any Qt apps or environment
variables polluting our global system scope.
This means that we can have multiple Qt projects and (cross-)compilers
(e.g. very new and very old ones) on the same system in different nix-shells.
They will never interfere with each other and an update of one cannot influence
the other.</p>
<p>The same <code>flake.nix</code> file works for different systems, regardless if it’s an
x86 or an ARM system (together with the small changes on the <code>flake.nix</code> file
that I pushed into the repository on GitHub but which I don’t explain in this
article to keep the scope crisp).
As long as you have nix installed, it will build and run the same way
everywhere.</p>
<div class="floating-image-right">
<p><img src="/images/works-for-me.gif" width="300" /></p>
</div>
<p>With the <code>flake.lock</code> file, <em>everything</em> is <strong>pinned</strong>:
The project will still build in years on different machines and create the same
app.
Not only the same app is created, but it will be bundled with the same
<em>dependencies</em> (the whole stack from Qt down to the C library).
This means that it will not only build but also <em>run</em> the same way in many
years.
It will not happen any longer that something “works for me” on some coworker’s
computer, but not on a different machine, because they essentially have the same
packages (Although things might run differently for bugs that happen to be in
the macOS version of Qt and not in the Linux version or the other way around).</p>
<p><strong>Updates</strong> are painless:
Run <code>nix flake update</code> to update all the nix package inputs, and commit the
new lock file (which happens automatically if you add the command line argument
<code>--commit-lock-file</code>).
If it doesn’t work because the update came with breaking changes, then fix it
and commit the changes <em>together</em> with the new lock file to have individual
atomic working commits on your main branch.</p>]]></summary>
</entry>
<entry>
    <title>Automatic Testing of Display Resolution and Frame Drop Detection</title>
    <link href="https://blog.galowicz.de/2023/01/09/automatic-testing-of-display-resolution-and-frame-drop-detection" />
    <id>https://blog.galowicz.de/2023/01/09/automatic-testing-of-display-resolution-and-frame-drop-detection</id>
    <published>2023-01-09T00:00:00Z</published>
    <updated>2023-01-09T00:00:00Z</updated>
    <summary type="html"><![CDATA[<!-- cSpell:words ArUco GPIO Raspberry raspi actionism Vasa -->
<!-- cSpell:words Bjarne Stroustrup -->
<!-- cSpell:ignore autoplay -->
<p>This week, I like to share a project with you that started as a very interesting
challenge and developed into an interdisciplinary, productive, and fun
experience:
A fully automatic multi-display end-to-end customer test prototype.
In the end, it was surprising to see what parts of the code the most effort
went into.</p>
<!--more-->
<h2 id="initial-situation">Initial Situation</h2>
<div class="floating-image-right">
<figure>
<img src="/images/display-testing/developer-qa.webp" width="300" alt="The impact of bad news gets worse the later it arrives" />
<figcaption aria-hidden="true">The impact of bad news gets worse the later it arrives</figcaption>
</figure>
</div>
<p>The project partner’s business sells laptops with customized operating system
support.
Before every release, custom operating system images have to be tested against
the software and hardware combinations that are certified and shipped with and
for the supported laptop models.
These tests have so far been executed manually in a dedicated QA department.
Manual testing introduces unwanted delays in the product cycles, so the project
partner’s challenge was:
How to automate the most time-consuming part of it?
Concentrating on the manual, most time-consuming, and costly phases of the
software release process is strategically the best way to improve the throughput
of the organization, as we might already have learned from the
<a href="/2022/12/19/book-review-the-phoenix-project">three ways in the book “The Phoenix Project”</a>.</p>
<p>After each successful test phase, professionals of the QA department sign the
test result documentation <em>personally</em> (autograph signature, not (yet)
cryptographic signature).
For this reason, automated testing not only had implications for the development
department but also legal ones for the QA:
We could improve the processes by automating phases that have been performed
manually before and then have developers use those earlier and frequently to
reduce the number of bugs in release phases in the final test runs.
But this had to happen step by step, so the test method had to be designed in a
way that the non-programmers, who still have to <em>sign</em> the test results before
every release, trust the automated test approach to be equivalent to manual
testing in its validation of tested functionality.
If we failed this constraint, we would just produce another stage somewhere in
the existing processes.
It would slowly drift away from what happens in actual pre-release QA over time
and all the investment would miss its target.</p>
<p>This constraint made the overall problem much harder.
I see this as a side effect of change being more difficult in big corporations
compared to smaller startups:
You typically face multiple stakeholders with sometimes contradicting
incentives, often combined with a big portion of risk avoidance.
In such committee-like environments, no one can take full ownership and “just
decide” requirements - instead, solutions are discussed until everyone agrees.
This often leads to extremely complex solutions with the risk of missing the
point.</p>
<blockquote>
<p>This part of the story reminded me of the <a href="https://www.stroustrup.com/P0977-remember-the-vasa.pdf">“Remember the Vasa!” paper by
Bjarne Stroustrup in the C++ committee</a>.
The <a href="https://en.wikipedia.org/wiki/Vasa_(ship)">Wikipedia article about the Vasa story</a>
is also worth reading.</p>
</blockquote>
<h2 id="technical-problem-description">Technical Problem Description</h2>
<div class="floating-image-right">
<figure>
<img src="/images/display-testing/im-not-a-robot.webp" width="300" alt="Many systems are exclusively designed for use by humans" />
<figcaption aria-hidden="true">Many systems are exclusively designed for use by humans</figcaption>
</figure>
</div>
<p>The manual process that should be automated, starts with installing a release
candidate image on a laptop, then booting into the installed OS.
From there, all the basic things like network/internet access, display support,
suspend-resume support, video playback performance, etc., etc. had to be tested.
The whole test process resembled multiple phases of a typical day in the life
of the product:</p>
<ul>
<li>The image is installed on the laptop under test (unattended USB installer)</li>
<li>The laptop gets power cycled and boots into the OS</li>
<li>LAN and Wi-Fi connectivity are verified</li>
<li>external screens are plugged, in different variations:
<ul>
<li>different order of plugging</li>
<li>unplugging all screens must result in an automatic system suspend, if and
only if the laptop lid is closed</li>
<li>the system has to properly wake up with/without external screens when the
power button is actuated or respectively the laptop lid was opened</li>
</ul></li>
<li>VMs are launched, and their network connectivity verified</li>
<li>Multiple display resolutions have to be tried and verified that each tested
display model shows the right picture (multi-display context: orientation and
ordering) with the right resolution</li>
<li>HD Videos are launched inside/outside VMs and verified for playback with good
frame rate</li>
</ul>
<p>After the iterative process of thinking of and proposing solutions, the
following technical requirements for the automatic test process were defined:</p>
<ul>
<li>Network access via both cable and Wi-Fi has to be verified</li>
<li>Plugging in and out displays must happen physically, i.e. the electric cable
contact shall be cut instead of doing it in software or switching off the
displays</li>
<li>Similar to the displays, laptop docking and undocking must look from a
hardware perspective as if happened physically</li>
<li>Laptop lid closing and opening must happen physically</li>
<li>Automated installation of the test image may be performed via network boot</li>
<li>The product including software and hardware must not differ from the shipped
production version for the test as this would impede the validity of the test
results.
The only exceptions are:
<ul>
<li>Remote control of operating system and applications can be done via a
specific service (developed and used by the project partner’s development
department) that is reachable via serial line</li>
<li>We can solder an electrical cable to the power buttons of the laptop models
to simulate users powering the device on/off and suspending to RAM, as
<a href="https://en.wikipedia.org/wiki/Intel_Active_Management_Technology">Intel AMT</a>
was disabled for security reasons</li>
</ul></li>
<li>The <em>real physical</em> display output has to be captured for different display
models
<ul>
<li>For this reason, video capture devices as HDMI/DP grabbers were ruled out
by the project partner</li>
</ul></li>
</ul>
<p>The result must provide the users (Developer, QA specialist) a REST interface
to upload an installer image and trigger a full test run.
After the test has finished, the test results must be available for download.
In the beginning, the result document must be human-readable, printable HTML
(and in that sense, signable).
All screenshots and videos must be attached to it as proof.
In the future, this would happen without human interaction, and instead being
triggered by CI pipelines as part of a quality gate that each product change
must go through.</p>
<h2 id="solution">Solution</h2>
<p>The prototype design we came up with, roughly followed this schema:</p>
<figure>
<img src="/images/display-testing/multi-display-setup.png" width="600" alt="Schematic System and Display Test Setup" />
<figcaption aria-hidden="true">Schematic System and Display Test Setup</figcaption>
</figure>
<p>The <strong>Test Controller</strong> was a normal Linux computer with enough USB plugs and
bandwidth to withstand multiple USB webcam HD streams and a serial cable
adapter.
It also had to have two LAN connections: One to the system under test for DHCP,
PXE boot, and general network gateway, as well as one to the outside that
provides the REST interface.
Also, it had to provide Wi-Fi to be configured as an access point.</p>
<p>The <strong>I/O Control</strong> module was a Raspberry Pi due to its easily programmable
general-purpose I/O ports and USB OTG functionality.
The USB OTG functionality was not yet used in the prototype.
In later versions, it should be used for booting official installer images via
USB.</p>
<p>The first physical setup was just enough to verify the feasibility of the
whole project as quickly as possible:</p>
<figure>
<img src="/images/display-testing/setup.jpg" width="800" alt="Physical Prototype System and Display Test Setup: The odd part of this project are these hand-crafted display frames that hold the webcams in the right place." />
<figcaption aria-hidden="true">Physical Prototype System and Display Test Setup: The odd part of this project are these hand-crafted display frames that hold the webcams in the right place.</figcaption>
</figure>
<p>The most important hardware setup details are:</p>
<ul>
<li>The display port switch was a hardware Y-switch with a button that was
connected to the Raspi’s <a href="https://en.wikipedia.org/wiki/General-purpose_input/output">GPIO ports</a></li>
<li>For HDMI, which was more complicated because
<a href="https://en.wikipedia.org/wiki/HDMI#Content_protection_(HDCP)">HDMI is encrypted</a>,
we did not find such devices, so we used a remote-controllable HDMI switch.
It turned out that this device was <em>not</em> matching the requirements.
The project partner accepted that we design a new board that fits the
requirements as a follow-up after this prototype.</li>
<li>The docking station’s power plug state was controlled via a
remote-controllable power socket</li>
<li>The webcams were simply mounted on quick-to-build aluminum frames.
These were the most unusual part of the whole setup and regularly drew
intrigued looks.
The bigger the screen, the bigger the minimal distance of the webcam to the
screen had to be to fully capture the full display.
Being made of aluminum and very light wood, the displays would not tilt under
the webcam’s weight.</li>
</ul>
<p>The last detail is how we automated the laptop lid opening and closing.
Most laptops have a
<a href="https://en.wikipedia.org/wiki/Hall_effect_sensor">hall sensor</a>,
so we installed an electromagnet that could be triggered by another one of the
raspi’s GPIO pins:</p>
<figure>
<img src="/images/display-testing/laptop-lid-magnet.png" width="600" alt="Most laptop lids have Hall Sensors that can be triggered with magnets" />
<figcaption aria-hidden="true">Most laptop lids have Hall Sensors that can be triggered with magnets</figcaption>
</figure>
<p>After all physical setup and connections were put in place, the remaining work
was “only” software.
Setting up the raspberry so that it would accept I/O commands over network, and
the test controller so that it would be a DHCP and PXE boot server, control all
the webcams, the Raspberry Pi, the remote power socket, etc. was a lot of system
configuration.
Using <a href="https://nixos.org/">NixOS</a>, the configuration of both systems was simple
and could be performed in a short time with only a few hundred lines of NixOS
configuration modules.
The best thing about this approach was that we could commit the configuration
with the code, and it was trivially reproducible on other machines.</p>
<p>The bulk of the work was now in the software development of a service that runs
a REST interface to accept uploads of new installer images, creates and
schedules test runs from those, and then executes them.
The professionals in the project partner’s department who would later work with
this were already familiar with the Python language and ecosystem.
This made Python a natural choice.</p>
<h3 id="display-resolution-verification">Display Resolution Verification</h3>
<p>How to determine if a screen shows the right display solution?
Typically, one would query the X/Wayland server or video driver for this
information.
Previously, it was common to check this manually during the manual test phase:
If the resolution was wrong, the experienced human tester would be able to
recognize this at the first glance.
In the end, the prototype had to verify that with the selected display model,
the resolution that is visible to the end user is <em>really</em> the one that the
driver claims.</p>
<p>To accomplish this task, the following problems had to be solved in that order:</p>
<ol type="1">
<li>Take a photo of the physical display using the mounted webcam</li>
<li>Identify the display’s corners and normalize the photo, so it contains only
the display’s “screenshot” portion</li>
<li>Evaluate the screenshot’s content: Is it the right picture and resolution?</li>
</ol>
<p>Steps 1 and 2 were straightforward as the test controller could dictate
what the laptop under test should display on which screen.
This way the test script would ask the OS under test to set up the screen(s)
with specific ordering and resolutions and then display some test images for
further verification.</p>
<p>We chose to use the Python version of the <a href="https://opencv.org/">OpenCV</a> library,
which provides
<a href="https://docs.opencv.org/4.x/d5/dae/tutorial_aruco_detection.html">ArUco codes</a>
and image recognition functions to detect them:</p>
<figure>
<img src="/images/display-testing/aruco-markers-example.jpg" alt="Example ArUco Markers from docs.opencv.org" />
<figcaption aria-hidden="true">Example ArUco Markers from <a href="https://docs.opencv.org/4.x/d5/dae/tutorial_aruco_detection.html">docs.opencv.org</a></figcaption>
</figure>
<p>Different information can be extracted from detected ArUco codes on photos:</p>
<dl>
<dt>The code’s ID</dt>
<dd>
<p>This can be freely chosen depending on the number of “blocks” the code
consists of. To choose the number of blocks, library users would select
so-called ArUco dictionaries.
Dictionaries with bigger blocks are easier to detect even on bad photos or
from a far distance but transport smaller ranges of ID numbers.</p>
</dd>
<dt>The pixel position of all the corners of each code</dt>
<dd>
<p>The size or distance can be calculated from those</p>
</dd>
<dt>The code’s geometric orientation as a euclidean XYZ-vector</dt>
<dd>
<p>In our case, this was useful for normalizing slightly crooked photos, as the
mounted webcam is never perfectly aligned</p>
</dd>
</dl>
<figure>
<img src="/images/display-testing/aruco-markers-orientation-example.jpg" alt="Detected ArUco markers with added orientation docs.opencv.org" />
<figcaption aria-hidden="true">Detected ArUco markers with added orientation <a href="https://docs.opencv.org/4.x/d5/dae/tutorial_aruco_detection.html">docs.opencv.org</a></figcaption>
</figure>
<p>So, to quickly evaluate how well OpenCV would work for us, I printed an ArUco
code and glued it onto my vacuum bot at home.
Then, I wrote an OpenCV script that “normalizes” the image in a way that the
ArUco code gets translated to the middle of the output image:</p>
<figure>
<iframe width="1000" height="400" src="https://www.youtube.com/embed/d6SpXS115fg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
<figcaption>
Left side: The original video.
Right side: The “corrected” version of the same video with the intention to
always have the ArUco code upright and straight in the video.
</figcaption>
</figure>
<p>The vacuum bot proof-of-concept was more a fun project for me than a serious
demonstrator for display-corner detection.
To present the project partner with how solid this approach would be, I created
the next demo, which was more relevant for actual screen recording.
The script turned out to be robust in such extreme examples.
Our use case would be “boring” in contrast.</p>
<figure>
<iframe width="1000" height="280" src="https://www.youtube.com/embed/pCcIABQVBUU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
<figcaption>
Left side: The original video.
Right side: The “corrected” version of the same video with the intention to
always have a straight view of the display’s content.
</figcaption>
</figure>
<p>So that was steps 1 and 2.
How to verify the display resolution?
We did this:</p>
<figure>
<img src="/images/display-testing/pilot-image.png" alt="Test Pilot Image with Display Resolution Encoding: The green numbers show how we encoded the information that the display ought to have a resolution of 1920x1080 pixels. The “primary” bit was for differentiating between different displays." />
<figcaption aria-hidden="true">Test Pilot Image with Display Resolution Encoding: The green numbers show how we encoded the information that the display ought to have a resolution of 1920x1080 pixels. The “primary” bit was for differentiating between different displays.</figcaption>
</figure>
<p>We decomposed the image resolution to transport it encoded in the IDs of
multiple ArUco codes, which then could easily be composed back from the webcam
photos.</p>
<blockquote>
<p>At this point, this information was redundant because the test system already
knew at any point in time which display resolution it is testing.
However, adding some redundancy allowed us to greatly decompose the system’s
parts into more independent and reusable modules.</p>
</blockquote>
<p>From knowing the nominal display resolution, we can calculate how many percent
of the picture the black ArUco code square size may occupy.
This is even possible without knowing the display’s physical size.
Fortunately, algorithms like this are trivially unit-testable with synthesized
and real-life input samples.</p>
<p>Another remaining problem was that the webcams tried to automatically refocus
and readjust contrast whenever the display content changed (especially from dark
to bright or the other way around), and often would not find the focus for a
long time.
Even on really bad photos, the ArUco codes with big blocks were easy to
identify, so it was technically very easy to distinguish if the display is
already displaying the right content, <em>but</em> the webcam has still not finished
re-focusing.
The firmware does officially allow disabling autofocus via the
<a href="https://www.kernel.org/doc/html/v4.8/media/v4l-drivers/index.html">V4L</a>
interface, but it did not work for us - it was unclear to us if this was due to
the camera model, a firmware bug, or whatever.
In the limited project time, we made it work with robust error handling and wait
phases.
These could simply be dropped in future efforts of fixing the autofocus.</p>
<h3 id="automatic-frame-drop-detection">Automatic Frame Drop Detection</h3>
<p>To check if video playback is fluid, we had to select an approach where the
video itself can be chosen by the project partner’s customers:
If end-users experience some video to be stuttering, the partner desired to be
able to embed it in their testing to give developers a way to verify their fix.</p>
<p>We wrote code that takes the provided video and slightly adapts it by adding
multiple ArUco codes.
By defining how fast (i.e. how many pixels per frame) the ArUco codes move, we
were able to determine if and how many frames were dropped during playback.</p>
<figure>
<iframe width="1000" height="575" src="https://www.youtube.com/embed/y2YUuynf9AE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
<figcaption>
The ArUco codes move at a predefined speed.
This video has artificial stutters added to test the accuracy of the
algorithm.
</figcaption>
</figure>
<p>As the display’s and the webcam’s image buildup are not synchronized, there’s
some probability for one or the other frame to contain a distorted ArUco code.
So we added four codes of which the majority stays undistorted, which made the
frame drop detection very solid on every frame.</p>
<p>During manual testing, testers were able to say if a video stutters or not,
and maybe quantify it by “stutters a lot” or “only a bit”.
The output of the algorithm allowed us to count the <em>frequency</em> of frame drops
as well as the <em>quantity</em> of dropped frames.
Objective quality metrics using histograms could be considered from this point,
which enhances the whole QA phase through rich automatic quantitative analysis.
We did not go too crazy on the analysis here because it was yet unclear which
exact metrics developers would later ask for (if at all) when analyzing low
frame rates.</p>
<h2 id="learnings">Learnings</h2>
<p>The whole project took roughly three months.
The parts that were initially thought to be the most complex - the image &amp; video
processing - were done in a few weeks, including all trial &amp; error.</p>
<p>There was no <em>specific</em> part that contributed the most complexity - it was the
mix of all disciplines:
System setup, network (LAN and Wi-Fi) configuration, dynamic network boot,
control of primitive I/O, photo and video stream capture, remote control, VM
configuration, etc., etc.
Setting up a system with so many moving parts is simply complex.</p>
<p>Maybe the most important consideration that went into all spots of all library
code that was written for this project, was <strong>error handling</strong>.
In this project, errors could come from <em>everywhere</em>:
Some cable might get loose, some network connection might not work, some webcam
might need extraordinarily long to focus, some VM might start up with abnormal
delay, the Wi-Fi connection takes longer than normal despite lab conditions,
etc. - all the cases needed to be handled with care.</p>
<p>We rigorously worked on the clean discrimination between two general kinds of
errors:</p>
<ul>
<li><u>Infrastructure failures</u>: The test setup failed, not the product under test.</li>
<li><u>Test failures</u>: The product under test behaves incorrectly, hence a bug is
discovered.</li>
</ul>
<div class="floating-image-right">
<p><img src="/images/display-testing/prove-improve.webp" /></p>
</div>
<p>The whole prototype had to be rock solid in all cases every single time.
Flaky test infrastructure eventually results in more cost than value.
The project was part of an effort to optimize the overall throughput of the
development and QA departments.
Running such an automatism for years and gaining experience with it would expose
further optimization potential.
To facilitate that this proof-of-concept is just the start of an ongoing journey
to faster and more reliable testing, it had to be designed in a way that it is
obvious at all times where things fall apart, and how often.
With this information, the operators of such automatic infrastructure would be
able to fix outages as fast as possible, and also improve its mechanisms to
continuously reduce failure potential.</p>
<p>Every line of code that makes the product more testable saves you hundreds and
thousands of lines of testing code.
So far, <em>testability</em> was mostly considered on the level of unit tests, but
looking at higher levels of the integration chain, testability as a concept
vanished more and more.
This is understandable, considering that programmers have less and less
influence the farther away the integration of their components is away from
them - but in the end, it’s only the programmers who can make a software product
testable as this has to happen at the beginning, in the code.
If not only builds and unit tests but the integration of components is within
reach for software developers, it starts to make sense to put the design of
higher-level integration tests into their definition of done.
This is, of course, the idea of putting <em>Dev</em> and <em>Ops</em> together to <em>DevOps</em>.
In many companies, this is more said than done, because management is not
made aware or does not understand the commitment and restructuring of
departments that this requires.
<em>Restructuring</em> as a word is also mostly frowned upon by all participating
parties.</p>
<p>Talking about integration and ops of products:
This new test automatism was in some sense a little product itself that needs
continuous improvement, integration, and deployment, too.
A big stack of Python code (we also reused some existing C++ and Haskell code)
is not yet a running deployment.
But this turned out to be rather simple due to our technology choice:
Interestingly, using <a href="https://nixos.org/">NixOS</a> helped us a <strong>lot</strong> in defining
what the running configuration looks like.
Finishing this product in this time frame with something else than the
dependency and package management of nix, as well as the compositional way to
define system configurations of NixOS, is hardly imaginable.
While developing complex scripts that operate on many different technical
domains, nix also helped us with its
<a href="https://repology.org/repositories/graphs">huge and up-to-date package collection</a>.
Duplicating the lab setup with slightly different hardware in the project
partner’s lab was a smooth process due to the nature of NixOS configurations.</p>
<h2 id="summary">Summary</h2>
<p>The project acceptance meeting and its big demo were delightful and fun for both
sides.
The project partner was happy that it all worked out as planned and
accepted the follow-ups that resulted from building the prototype.
We were happy with how well it went and the amount of complexity we got
under control with a relatively slick system design.</p>
<div class="floating-image-right">
<p><img src="/images/display-testing/production.webp" /></p>
</div>
<p>Apart from the follow-ups, it is now in their hands to run and integrate this
automated test infrastructure deeper into their processes, and scale it up over
time in the ongoing quest to increase the overall throughput of their software
pipeline.
Whenever the next improvement or reorganization comes along, we stand by as a
helping partner.</p>
<p>I think it is very important to understand that
<a href="https://en.wikipedia.org/wiki/Shift-left_testing">shift-left testing</a>,
<a href="https://en.wikipedia.org/wiki/Scrum_(software_development)">SCRUM</a>,
<a href="https://en.wikipedia.org/wiki/Extreme_programming">Extreme Programming</a>,
<a href="https://en.wikipedia.org/wiki/Agile_software_development">Agile</a> and
<a href="https://en.wikipedia.org/wiki/Lean_software_development">Lean</a>
software development, etc. are not approaches that work well with short-term
actionism.
If you encounter problematic releases, quality problems, or eternal lead times
for new features and bug fixes in your company, then this is most certainly not
solvable with short projects that are thrown on the problem.
The solution always lies in the change of existing structures:
Such problems are often just a symptom of the fact that the existing structures
no longer scale on the current level.
Changing structures is very problematic, especially in big companies, and can
not result in perfection after the first step.
External helpers with new perspectives can help with the question of <em>what</em>
scalable structures for your situation look like and <em>how</em> to get there step by
step.</p>]]></summary>
</entry>
<entry>
    <title>Book Review: Basic Forms of Anxiety</title>
    <link href="https://blog.galowicz.de/2023/01/02/book-review-basic-forms-of-anxiety" />
    <id>https://blog.galowicz.de/2023/01/02/book-review-basic-forms-of-anxiety</id>
    <published>2023-01-02T00:00:00Z</published>
    <updated>2023-01-02T00:00:00Z</updated>
    <summary type="html"><![CDATA[<!-- cSpell:words Riemann Thomann Nico Fleisch -->
<!-- cSpell:ignore -->
<p>Software engineering, or generally IT jobs, are highly social because all big
software projects or products are built by teams of professionals.
When humans work with humans, all kinds of conflict can emerge, and they are
often not as easy to fix as IT problems, especially in distributed teams with
less offline human interaction.
This article is about <a href="https://amzn.to/3Gl86yK">Fritz Riemann’s book “Anxiety”</a>,
which answers the question “What do people fear and how do they cope with it?”.</p>
<!--more-->
<div class="book-cover">
<figure>
<img src="/images/books/grundformen-der-angst.jpg" alt="Book Cover of the original German language version of “Anxiety”" />
<figcaption aria-hidden="true">Book Cover of the original German language version of “Anxiety”</figcaption>
</figure>
</div>
<h2 id="book-authors">Book &amp; Authors</h2>
<p><a href="https://amzn.to/3Gl86yK">Link to the Amazon Store Page of the English version</a></p>
<p><a href="https://amzn.to/3C87PfQ">Link to the Amazon Store Page of the original German version</a></p>
<p>The book was initially published in German in 1961 and since 2022 its 47th
edition is sold, while its English translation was published in 2009.
Both versions are a bit more than 200 pages.
I have read the German version of the book.
The English version is advertised as a 1:1 translation.</p>
<p><a href="https://en.wikipedia.org/wiki/Fritz_Riemann_(psychologist)">Fritz Riemann</a> was
a German psychologist, psychoanalyst, and book author.
In 1946, he co-founded the Institute for psychological research and psychotherapy
in Munich.
Riemann became an honorary member of the Academy of Psychoanalysis New York.</p>
<h2 id="content-and-structure">Content and Structure</h2>
<p>This book is from a psychologist for non-psychologists.
People go to places, found companies, quit their job to apply for new ones,
build houses, give talks, etc., for reasons.
According to Riemann, people sometimes do these things as a <em>reaction</em> to fear.
Especially when people work or live together, all kinds of conflicts emerge
due to differences in their characters.
What appears normal to one might trigger fears in another.
This can especially happen in international teams due to cultural differences
but for that topic have a look at my
<a href="https://blog.galowicz.de/2022/12/12/book-review-the-culture-map/">book review of “The Culture Map”</a>.</p>
<p>Fritz Riemann explains his model that I call the “Riemann Model” over the rest
of this article.</p>
<h3 id="the-model">The Model</h3>
<p>No model is a silver bullet but knowing in which situations to apply what model
or at least take it as a rough guide often proves useful.
Riemann postulates that there are different
<a href="https://en.wikipedia.org/wiki/Personality_type">Personality Types</a>
based on different atomic fears and needs that each individual has.
People’s lives are <em>driven by their fears</em>, some more than others.
These fears and needs are:</p>
<table style="width:88%;">
<colgroup>
<col style="width: 26%" />
<col style="width: 25%" />
<col style="width: 36%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Fear</th>
<th style="text-align: left;">Need</th>
<th style="text-align: left;">Experienced as</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Fear of
<strong>self-surrender</strong></td>
<td style="text-align: left;">Need to be an
<strong>individual</strong></td>
<td style="text-align: left;">loss of self and
dependence</td>
</tr>
<tr class="even">
<td style="text-align: left;">Fear of
<strong>self-realization</strong></td>
<td style="text-align: left;">Need to be part
of a <strong>group</strong></td>
<td style="text-align: left;">isolation and lack of
emotional security</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Fear or
<strong>necessity</strong></td>
<td style="text-align: left;">Need for
<strong>change</strong></td>
<td style="text-align: left;">finality and lack of
freedom</td>
</tr>
<tr class="even">
<td style="text-align: left;">Fear of
<strong>change</strong></td>
<td style="text-align: left;">Need for
<strong>continuity</strong></td>
<td style="text-align: left;">transience and
insecurity</td>
</tr>
</tbody>
</table>
<p>Regarded as pairs, they form two conflicting poles that can be visualized as
a diagram:</p>
<ul>
<li>Emotional distance vs. proximity from/to others</li>
<li>Change vs. continuity</li>
</ul>
<figure>
<img src="/images/books/grundformen-der-angst-schulz-von-thun-diagram.jpg" width="500" alt="The four types of need-driven character according to Riemann (schulz-von-thun.de)" />
<figcaption aria-hidden="true">The four types of need-driven character according to Riemann (<a href="https://www.schulz-von-thun.de/die-modelle/das-riemann-thomann-modell">schulz-von-thun.de</a>)</figcaption>
</figure>
<p>The book puts some emphasis on explaining the <em>extremes</em> of these characters
first.
These are explained in 40-50 pages each, so if the summarized presentation in
this blog gives the impression that the topic is dealt with like in a magazine
checklist, then this is due to the abridged presentation.
These explanations seem very pathologic at first glance because the book is not
about mentally sick but healthy people.
The point of this pathological view is that understanding the extremes improves
comprehension of such traits when observing them in real life, although
of course not in the depicted extremes.
For every character trait, Riemann gives a very rich and detailed overview of
the trait but also delves into how they deal with love and aggression and gives
examples of how they experience typical situations in life.
These chapters are explaining it all so well that it hurts to provide only a
short and simplified explanation here, but that’s why one reads whole books
instead of summaries anyway.
I am pretty sure that everyone will recognize themselves in all chapters to
some degree, but in different severity.
At last, he also explains what situations and events in early life cause the
development towards each pole.
Let’s summarize every trait into caricatures to have an overview:</p>
<div class="floating-image-right">
<figure>
<img src="/images/books/grundformen-der-angst-data.webp" width="300" alt="Distance is a shelter" />
<figcaption aria-hidden="true">Distance is a shelter</figcaption>
</figure>
</div>
<dl>
<dt>Schizoid Personality (Distance)</dt>
<dd>
<p>Schizoids want and need to be seen as individuals (have you ever been
irritated when someone got your name or gender slightly wrong?).
They fight for their freedom and independence.
For that reason, they feel the need to avoid compromise and commitment.
Keeping enough emotional distance from everyone is on the one hand like a
defense mechanism against everything that reduces their freedom and
independence.
Talking about emotions: They don’t like them much and see pure cold
rationality as a cure - which is a strength and weakness at the same time.
However, proximity to others is unavoidable, so they end up developing some
protective stance that might often look impersonal and cold to others.
Schizoids often drift into isolation, as this is what they rescue themselves
into after conflicts with others.
While behaving how schizoids behave, they can look aggressive from the
perspective of others, which might also be intimidating.<br />
<br />
Such shields are a result of their past: They most likely experienced too much
pain in dependent relationships in their young years and grew independent to
free themselves from it.<br />
<br />
Typical traits or attributions by others:
Aggressive, arrogant, cold, isolated, objective, strong self-esteem,
task-oriented, criminal, anti-social</p>
</dd>
</dl>
<div class="floating-image-right">
<figure>
<img src="/images/books/grundformen-der-angst-clingy.webp" width="300" alt="Let’s be together always, forever" />
<figcaption aria-hidden="true">Let’s be together always, forever</figcaption>
</figure>
</div>
<dl>
<dt>Depressed Personality (Proximity)</dt>
<dd>
<p>“Depressed” people have a deeply embedded wish for affection and emotional
proximity to others, literally all the time.
“I love you because I need you” and “I need you because I love you” are two
equally true statements for them.
All distance that Schizoids try to build up, Depressed try to tear down as it
feels like loneliness and being forgotten to them.
To counter their separation anxiety, they build up all kinds of dependency on
others, towards which they also form expectations - which they won’t explain
because they see that as an indicator of not being understood.
Depressed often end up blaming others for their situation - the obvious way
to work towards a solution independently triggers their separation anxiety so
they “can’t”.
They hope that when they lament their problems, someone will help them.
Most of the time, help is not what they ask for when they keep lamenting about
their problems - it’s affection in the form of listening and empathy.
To others, this often looks like
<a href="https://en.wikipedia.org/wiki/Learned_helplessness">learned helplessness</a>,
self-pity, a lack of direction, and clumsiness.<br />
<br />
Deep inside, depressive parents don’t want their children to grow up and
become independent because this bears the potential for the distance they
fear.
So they sometimes end up raising spoiled children with a lack of skills to
master life on their own, which produces the next generation of depressive
humans.<br />
<br />
Typical traits or attributions by others:
Indecision, Blaming others, insecure, warm, needy, clingy, caring, dependent,
subordinate, people-oriented, empathic, envy, resigning, self-hate,
depression, suicide</p>
</dd>
</dl>
<div class="floating-image-right">
<figure>
<img src="/images/books/grundformen-der-angst-sheldocracy.webp" width="300" alt="The perfect world is free of surprises" />
<figcaption aria-hidden="true">The perfect world is free of surprises</figcaption>
</figure>
</div>
<dl>
<dt>Compulsive Personality (Continuity)</dt>
<dd>
<p>Compulsive characters fear changes and unplanned events.
They are on a quest for permanence and security.
For that reason, they enjoy planning ahead everything in meticulous depth,
to be prepared even for events that may probably never occur.
Changing plans even in light of changed circumstances is difficult for the
compulsive.
Enduring the costly trade-offs of a local optimum is much easier for them than
trying a promising new solution because it might introduce new unforeseen
problems.
New ideas are always evaluated in terms of how they help support the Old
World.
The lack of control over life and the future is a burden.
When compulsive people catch themselves in the act of giving in to temptation
or losing control of their emotions in an argument, guilt plagues them for a
long time.<br />
<br />
Strict parents instill in their children the need for planning, control, and
security by constantly admonishing them about order, punctuality, and
compliance from an early age.
In later life, the ever-admonishing feeling remains in the back of their mind.<br />
<br />
Typical traits or attributions by others:
Reliable, dutiful, precise, rigid, jealous, fanatic, despotic, pedantic,
prejudice, perfectionism, responsible, diligent, tidiness, parsimony</p>
</dd>
</dl>
<div class="floating-image-right">
<figure>
<img src="/images/books/grundformen-der-angst-princess.webp" width="300" alt="I want everything, now!" />
<figcaption aria-hidden="true">I want everything, now!</figcaption>
</figure>
</div>
<dl>
<dt>Histrionic Personality (Change)</dt>
<dd>
<p>Histrionics fear stagnation.
It does not matter if everything is fine where they are: They want to
experience new things, travel to new places, and try out new ideas.
That the unknown does not always bear good things is irrelevant:
It’s change what they are after, change for its own sake.
Making no plans helps histrionics to concentrate on the moment, free from the
obligation to express what goal would be achieved by what they are doing.
They don’t find security at all convincing because they are busy trying to
free themselves from rules and compromises.
In between their chaotic life they are nonetheless very charismatic and
charming.
But don’t feel too flattered if they tell you confidential gossip as you’re
one of many listeners providing them attention.<br />
<br />
Milieus that are chaotic, contradictory, incomprehensible, and/or without
guidance and healthy models give the child too little orientation and support.
In such environments, children learn:
“If they don’t love you enough when you’re easy and uncomplicated, then their
concern for you will get you there when you’re being problematic.
The more often sick, the more loved.”<br />
<br />
Typical traits or attributions by others:
Adventurous, impulsive, spirited, attention-seeking, narcissistic, unreliable,
airs and graces, superficial, manipulable, charming, impatient</p>
</dd>
</dl>
<h3 id="application">Application</h3>
<p>Similar to the question about what <em>good</em> criticism or <em>good</em> communication
is, as covered in my
<a href="/2022/12/12/book-review-the-culture-map">review of the book The Culture Map</a>,
the model highlights what the limits of empathy can look like and where they
originate:
Characters of one side feel like they have the cure for the other.
That is because their fears and feelings are not only very different - they
never felt each other’s emotions the same way in comparable situations like
this.</p>
<p>From the perspective of the diagram, Riemann’s model has a striking similarity
to the <a href="https://en.wikipedia.org/wiki/DISC_assessment">DISC model</a> which is
more internationally known.
But Riemann’s model is not designed to match people with lists of traits to
classify them in isolation (although you can always analyze your own fears,
use the model to help yourself understand the underlying causes and improve your
daily interaction with the world at work or in relationships).
Every human has traits from all the presented poles at the same time.
It’s the personal weighting of each trait that makes the character.
The Swiss psychologist
<a href="https://de.wikipedia.org/wiki/Christoph_Thomann">Christoph Thomann</a>
extended this model to the Riemann-Thomann model:
The most obvious changes are that he substituted the pathological terms
<em>schizoid</em>, <em>depressed</em>, <em>compulsive</em>, and <em>histrionic</em> with the softer terms
<em>distance</em>, <em>proximity</em>, <em>continuity</em>, and <em>change</em> (as already reflected in
this article).
He also suggests that people might have different positions in the model at work
than in private life.
This way the model has less of a judging character when used to assess people.
As a couple therapist and later conflict moderator in companies, he
demonstrated how to use it to arrange people <em>relative</em> to each other to
understand their conflict potential.</p>
<div class="floating-image-right">
<figure>
<img src="/images/books/grundformen-der-angst-schulz-von-thun-proximity-distance.png" width="400" alt="The tension potential between a proximity and a distance person (source: Schulz-von-Thun book “Let’s Talk”)" />
<figcaption aria-hidden="true">The tension potential between a proximity and a distance person (source: <a href="https://amzn.to/3C9hT8z">Schulz-von-Thun book “Let’s Talk”</a>)</figcaption>
</figure>
</div>
<p>If you identify someone as more of a proximity-continuity type in comparison
to someone else who is more of a change-distance type, it becomes relatively
simple to foresee the kinds of conflicts they will have with each other at work
or in their private life.
In a conflict between such a pair, the conflict resolution must begin with
strengthening the bond for the proximity person, while the distance person first
needs distance to recover.
This example models a
<a href="https://en.wikipedia.org/wiki/Deadlock">deadlock</a>
that is often easier to solve at work than in romantic relationships.</p>
<p>Another conflict example would be:
What happens if you put one or more distance-change types of persons together
with an existing established team of proximity-continuity types and give them
the mission to introduce some complex change?
It is not unlikely that the productivity of the group will be reduced while
the team gets stuck in discussions between the people who want a plan for
everything and the ones who think that you need more of an open experimental
approach because trying out new things will produce new unforeseeable problems
where planning doesn’t help you much.
Most likely, no one will say “I feel uncomfortable like this, can you please at
least give me something to hold on” but instead non-productively fantasize bad
intentions or cram out some prejudices.</p>
<div class="floating-image-right">
<figure>
<img src="/images/books/grundformen-der-angst-change-plan.webp" alt="How a change type explaining his “plan” may look like to a continuity type" />
<figcaption aria-hidden="true">How a change type explaining his “plan” may look like to a continuity type</figcaption>
</figure>
</div>
<p>In such cases, the model helps find out why people don’t want things and how to
present each other’s positions in a way that doesn’t trigger the other side’s
fears.
The more people in the team educated themselves with such models, the less need
would be for a manager or moderator to solve such crises.</p>
<p>Also, if someone works together with a happy team of relatively homogeneous
character traits and realizes that he/she is in the opposite corner of the
Riemann coordinate system than most of the others, then the Riemann model
provides new ways to explain why such an environment feels exhausting or even
toxic to them without even anyone having about bad intentions.
In such cases, the model provides some guidance in describing why some team
constellations simply are less efficient or effective than others, and how
to compose teams with what combination of personality types for which kind of
task.
If all advice fails, the book gives a good decision basis on whether to leave a
constellation.</p>
<p>If you speak in front of a group or sell products or ideas, having some
knowledge about the character types of the key people in the room will also help
present your subject in the right way by carefully circumnavigating what might
trigger their discomfort.</p>
<p>The Riemann Model is also a useful complement to the
<a href="https://en.wikipedia.org/wiki/Situational_leadership_theory">Situational Leadership</a>
model for managers:
This model prescribes that leaders should adapt their leadership style to the
maturity of the people or group they manage.
At the same time, the Riemann Model suggests that the style of interaction with
people and groups should be chosen by their personality traits.
From that perspective, the situational leadership model provides a <em>temporal</em>
view that develops over time, and the Riemann model a <em>spatial</em> view that
considers the needs of the underlying character.</p>
<h2 id="summary">Summary</h2>
<p>Over the years, it kind of became my thing at work to introduce new technologies
or ways to solve challenges - having a tendency towards the distance and change
types from the book - this always happened with the best intentions to improve
my customer’s/team’s/company’s agility, ability to innovate, etc.
Reading this book helped me understand better <em>why</em> and how other people
sometimes see this as a curse or threat instead of a blessing, and how to
present ideas to reach more people in the intended ways with fewer
misunderstandings.
Also, while founding my own company together with colleagues, I faced new
challenges in human interaction e.g. by becoming manager of a team, etc.</p>
<p>Most popular science books that originate from psychology or social sciences
don’t contain completely new things:
As an “experienced human” we have seen most situations already.
What such books provide are views on the situations/challenges that are already
known, with a new structure that helps describe challenges and find better
solutions.
Not only in that regard I found this book to be a fascinating and inspirational
read.
To me, reading this book equals never again having no explanation for the
strange behavior of others that have a completely different character.
Such insights don’t automatically provide solutions but at least extend limits
of empathy.</p>
<p>The model provides useful insights but less actionable advice.
For more detailed advisory I suggest reading
<a href="https://amzn.to/3Q6njH4">Nico Fleisch’s book about the Riemann-Thomann Model</a>
for which I, unfortunately, did not find any English version.</p>
<p>Humans who work with other humans should read this.</p>]]></summary>
</entry>
<entry>
    <title>Book Review: Algorithms to Live By - The Computer Science of Human Decisions</title>
    <link href="https://blog.galowicz.de/2022/12/28/book-review-algorithms-to-live-by" />
    <id>https://blog.galowicz.de/2022/12/28/book-review-algorithms-to-live-by</id>
    <published>2022-12-28T00:00:00Z</published>
    <updated>2022-12-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[<!-- cSpell:words Griffiths Gittins Belew Hosken diseconomy Overfitting -->
<!-- cSpell:words incent overfit AIMD protokollon diseconomies -->
<!-- cSpell:words crêpe crêpes Tolins Backoff Primality Karlton Beladys -->
<!-- cSpell:words counterincentive Cinco -->
<!-- cSpell:ignore huhs hmms -->
<p>What does the math tell us about how many job applicants we should look at
before hiring one?
While onboarding our new employees, how can ideas from the
<a href="https://en.wikipedia.org/wiki/Transmission_Control_Protocol">TCP networking protocol</a>
help us to identify the optimal workload for them?
Why would giving employees unlimited vacation days most likely lead to less
vacation being taken?
<a href="https://amzn.to/3HAmDaM">Algorithms to live By - The Computer Science of Human Decisions</a>
gives some fascinating insights into such questions.</p>
<!--more-->
<div class="book-cover">
<figure>
<img src="/images/books/algorithms-to-live-by.jpg" alt="Book Cover of “Algorithms to Live By - The Computer Science of Human Decisions”" />
<figcaption aria-hidden="true">Book Cover of “Algorithms to Live By - The Computer Science of Human Decisions”</figcaption>
</figure>
</div>
<h2 id="book-authors">Book &amp; Authors</h2>
<p><a href="https://amzn.to/3HAmDaM">Link to the Amazon Store Page</a></p>
<p>The first edition of this book is from 2016 and the store page on Amazon says
that since 2017 it’s already the 12th edition.
It is ~370 pages in total but the actual content without notes etc. is just
about 260 pages.</p>
<p><a href="https://en.wikipedia.org/wiki/Brian_Christian">Brian Christian</a> is a
non-fiction author, speaker, poet, programmer (e.g. Ruby on Rails contributor),
and researcher.
This book and his other books, <a href="https://amzn.to/3G4nkbn">The Most Human Human</a>
and <a href="https://amzn.to/3vbQqPP">The Alignment Problem</a> won several awards.</p>
<p><a href="https://en.wikipedia.org/wiki/Tom_Griffiths_(cognitive_scientist)">Tom Griffiths</a>
is a professor of computer science and psychology and the director of the
<a href="https://cocosci.princeton.edu/">Computational Cognitive Science Lab</a>
at Princeton University.</p>
<h2 id="content-and-structure">Content and Structure</h2>
<p>The authors selected 11 topics from mathematics and computer science.
In each chapter, they describe practical and relevant challenges from real life
that can be solved with formulas and algorithms.
If you ever asked yourself “What’s the point? I will never use this in real
life!” at school or university, these are for you!</p>
<p>The authors spend some time explaining probabilities, trade-offs,
complexities, etc. with scientific methods, but they don’t go too deep.
This makes the content comprehensible for nearly everyone, especially interested
readers who never studied anything mathematical/technical.
I might not be the best person to judge that because I did study at university,
but I am certain that as long as seeing variable names or diagrams does not
straightly trigger fear, you should be able to understand it and experience fun
reading the stories.</p>
<p>Let’s get to the different chapters and their content.
This review is much longer than <a href="/tags/book.html">the others in my blog</a>:
The different chapters provide valuable insights, but on mostly disconnected
topics.
I did not want to drop any.</p>
<h3 id="optimal-stopping---when-to-stop-looking">1. Optimal Stopping - When to Stop Looking</h3>
<p>This chapter is about the <em>stopping problem</em>, which is a classic decision-making
problem that involves choosing the optimal time to stop a process or activity.
When you want to find the best specimen out of an unknown set (in the sense that
you don’t know what’s the lower and upper bounds of “good” are),
the <strong>Look-Then-Leap rule</strong> states that you should set an amount of time you
want to spend looking.
This amount of time should then be split into a looking phase and a leaping
phase.
In the looking phase, you categorically don’t choose anyone.
After that point, you enter the leaping phase and commit to the next candidate
who outshines the best applicants from the looking phase.
The looking phase shall occupy 37% of the time or the number of candidates
you can afford to look at.
This leads to a 37% chance of selecting the best (That it’s both 37% is a
coincidence that results from the maths).
37% does not appear great, but is better or worse than the gut feeling?</p>
<div class="floating-image-right">
<figure>
<img src="/images/books/algorithms-to-live-by-the-office.webp" width="300" alt="Finding the best applicant is hard" />
<figcaption aria-hidden="true">Finding the best applicant is hard</figcaption>
</figure>
</div>
<p>This strategy can be used in a variety of interesting situations:</p>
<ul>
<li>The Secretary Problem: Choosing the best candidate from a pool of applicants</li>
<li>Selling a house: Determining the best offer</li>
<li>Finding a partner: Deciding when to commit to a relationship after a series of
different dates</li>
<li>Selecting a parking lot: Choosing the best parking space available while
trading off between how far it is from your flat and how often you will have
to do another round around the block</li>
<li>Quitting an activity: Deciding when to stop doing something</li>
</ul>
<h3 id="exploreexploit---the-latest-vs.-the-greatest">2. Explore/Exploit - The Latest vs. the Greatest</h3>
<p>This chapter starts with the very concrete question of when you should stick to
the restaurants you know to be good, and when to try out new ones.
It turns out that this is the same problem that casino visitors face in a saloon
full of one-armed bandits:
Should they try the same bandit again, or should they switch to another one?
It’s also the same problem that marketing specialists face when they do
<a href="https://en.wikipedia.org/wiki/A/B_testing">A/B testing</a>
of different wordings or styles of advertisements (the book mentions that Google
tested 41 shades of blue for a toolbar in 2009).</p>
<div class="floating-image-right">
<figure>
<img src="/images/books/algorithms-to-live-by-casino.webp" width="300" alt="Choosing the best option is crucial" />
<figcaption aria-hidden="true">Choosing the best option is crucial</figcaption>
</figure>
</div>
<p>The <a href="https://en.wikipedia.org/wiki/Gittins_index">Gittins Index</a>
models a strategy for deciding when to switch from one
solution to another, based on the <em>history</em> of success rates.
The way it works is that each alternative gets a score.
For every decision, the alternative with the highest score is selected.
Based on success or failure, the score is updated following a specific scheme.
This strategy results in the following behavior:</p>
<ul>
<li><em>Untested options</em> are tried if the tested ones go below a success rate of 70%</li>
<li>The strategy is more merciful on failing alternatives in the beginning
than on long-known alternatives</li>
</ul>
<p>Switching or not switching between alternatives raises questions about other
use cases:
In clinical studies, the set of voluntary patients (or not so voluntary if the
only cure is still experimental) is split into a group that
gets the new experimental medicine, and another group that gets placebos.
When the study is only half over but the new treatment already proved very
effective, how ethical is it to stick to giving the placebo group placebos
instead of switching over all patients to the seemingly effective treatment?
Switching might however jeopardize the needed statistical backing that is needed
to approve the effectiveness of new treatments.</p>
<p>While the Gittins Index looks at the past, the chapter also introduces
the reader to other strategies as the <strong>regret minimization strategy</strong> based on
so-called
<a href="https://en.wikipedia.org/wiki/Thompson_sampling#Upper-Confidence-Bound_%28UCB%29_algorithms">Upper Confidence Bound algorithms</a>,
which take assumptions on the future into account.
These give the benefit of the doubt a mathematical backing:</p>
<blockquote>
<p>Following the advice of these algorithms, you should be excited to meet new
people and try new things — to assume the best about them, in the absence of
evidence to the contrary.
In the long run, optimism is the best prevention for regret.</p>
</blockquote>
<p>When looking at the future, it also becomes relevant how much time is left:</p>
<blockquote>
<p>Explore when you will have time to use the resulting knowledge and exploit
when you’re ready to cash in.
The interval makes the strategy.</p>
</blockquote>
<h3 id="sorting---making-order">3. Sorting - Making Order</h3>
<p>This chapter handles sorting theory and discusses the cost of sorting.
Sorting costs <em>time</em> (and comparisons - sometimes these are not free), which is
something that computer science students learn to quantify with complexity
theory up and down, typically by analyzing different sorting algorithms and
estimating their costs as a function of the input size.</p>
<div class="floating-image-right">
<figure>
<img src="/images/books/algorithms-to-live-by-sorting-mail.gif" width="300" alt="Depending on the input size, sorting can become an unwieldy task" />
<figcaption aria-hidden="true">Depending on the input size, sorting can become an unwieldy task</figcaption>
</figure>
</div>
<p>Ever thought about how long it would take to sort a deck of 5 cards? Or 10?
Or 20? The cost of sorting them goes up much steeper than the deck of cards
grows.
The book demonstrates the science behind scale nicely to non-computer-scientists
using more examples that show how much scale hurts when sorting inputs that are
just too big.
Social hierarchies and pecking orders have been established with physical
fighting as sorting methods, which puts the “cost” of comparison/sorting in a
completely different perspective.
Civilization has brought softer and more efficient ways to sort with
sports and markets, which resemble crowd algorithms, so to say.</p>
<p>The example of sports is explained more in-depth:
Championships and leagues are a way to sort teams by performance.
For sports where one person or team competes version one other at a time,
complex tournament strategies like
<a href="https://en.wikipedia.org/wiki/Single-elimination_tournament">Single Elimination</a>,
<a href="https://en.wikipedia.org/wiki/Round-robin_tournament">Round Robin</a>,
<a href="https://en.wikipedia.org/wiki/Ladder_tournament">Ladder</a>, and
<a href="https://en.wikipedia.org/wiki/Bracket_(tournament)">Bracket Tournament</a>
strategies are used.
Races with many competitors are simpler, but even these have qualifying events,
which sort them before the race which is supposed to sort them in the first
place.
Each of these strategies resembles different sort algorithms for different
problem sizes, long before sorting algorithms have been formalized.</p>
<p>Even <em>search</em> machines offer some kind of sorting, although this is surprising
at first:
You enter some search words into Google, and Google presents you not one but
many websites - sorted by relevance.</p>
<h3 id="caching---forget-about-it">4. Caching - Forget About It</h3>
<p>This chapter motivates the concept of “caching” by explaining
<a href="https://en.wikipedia.org/wiki/Memory_hierarchy">memory hierarchies</a>:
Computers have smaller portions of very fast but expensive memory and bigger
portions of memory that is cheap but slow.
Users like their computers fast, so engineers have to deal with the challenge
to provide the right data at the right time from limited faster memory.</p>
<div class="floating-image-right">
<figure>
<img src="/images/books/algorithms-to-live-by-fortune-teller.webp" alt="A cache tries to predict what portions of memory will be asked for to reduce wait times" />
<figcaption aria-hidden="true">A cache tries to predict what portions of memory will be asked for to reduce wait times</figcaption>
</figure>
</div>
<p>Phil Karlton, at that time an engineer at Carnegie Mellon, half-jokingly
originated
<a href="https://www.karlton.org/2017/12/naming-things-hard/">this quote around 1970</a>:</p>
<blockquote>
<p>There are only two hard things in Computer Science:
Cache invalidation and naming things.</p>
</blockquote>
<p>The chapter before was all about sorting data to make specific items
easier to find.
This chapter brings some (half-joking) news for the lazy.
With all the knowledge about caching, it turns out, that <em>not</em> sorting data
can shorten access times a lot:</p>
<blockquote>
<p>Tom’s otherwise extremely tolerant wife objects to a pile of clothes next to
the bed, despite his insistence that it’s in fact a highly efficient caching
scheme.
Fortunately, our conversations with computer scientists revealed a solution to
this problem too.
Rik Belew of UC San Diego, who studies search engines from a cognitive
perspective, recommended the use of a valet stand.</p>
</blockquote>
<p>The mentioned “pile of clothes” in some sense resembles the
<a href="https://en.wikipedia.org/wiki/Cache_replacement_policies#LRU">Least Recently Used</a>
caching scheme:</p>
<blockquote>
<p>LRU teaches us that the next thing we can expect to need is the last one we
needed, while the thing we’ll need after that is probably the
second-most-recent one.
And the last thing we can expect to need is the one we’ve already gone
the longest without.</p>
</blockquote>
<p>The chapter makes interesting detours between principles known from computer
science like
<a href="https://en.wikipedia.org/wiki/B%C3%A9l%C3%A1dy%27s_anomaly">Beladys Anomaly</a>,
<a href="https://en.wikipedia.org/wiki/FIFO_(computing_and_electronics)">First-In-First-Out (FIFO)</a>,
and
<a href="https://en.wikipedia.org/wiki/Cache_replacement_policies#Random_replacement_(RR)">Random Replacement</a>,
and shows how similar these principles are to processes that happen
in our brains, like the
<a href="https://en.wikipedia.org/wiki/Forgetting_curve">Human Forgetting Curve</a>, which
is a known phenomenon from neurosciences and psychology.</p>
<p>The science of caching can not only be applied to computers, but also the
physical layout of library rooms, the ordering of clothes in the bedroom,
management of post-its, and shelves, and why/when/how people remember or forget
things:</p>
<blockquote>
<p>Caching gives us the language to understand what’s happening.
We say “brain fart” when we should really say “cache miss”.</p>
</blockquote>
<h3 id="scheduling---first-things-first">5. Scheduling - First Things First</h3>
<p>Beginning with the question of how to schedule tasks in everyday life, the
book delves briefly into how computers schedule technical tasks, and what to
learn from them:</p>
<blockquote>
<p>The first lesson in single-machine scheduling is literally before we even
begin: make your goals explicit.</p>
</blockquote>
<p>As things are generally a bit more arranged in computer memory than in
real life, it is easier to summarize observations and extract guidelines:</p>
<blockquote>
<p>Minimizing the sum of completion times leads to a very simple optimal
algorithm called Shortest Processing Time:
Always do the quickest task you can.</p>
</blockquote>
<div class="floating-image-right">
<figure>
<img src="/images/books/algorithms-to-live-by-scheduling.webp" alt="Planning is hard (source)" />
<figcaption aria-hidden="true">Planning is hard <a href="https://giphy.com/gifs/funny-how-task-iCFlLMvzDHIk0">(source)</a></figcaption>
</figure>
</div>
<p>After a few motivating examples that relate to real-life tasks, the amount of
theory is ramped up a bit, for example when explaining
<a href="https://en.wikipedia.org/wiki/Single-machine_scheduling">Earliest Due Date (also called Moore’s Algorithm)</a>
vs
<a href="https://en.wikipedia.org/wiki/Shortest_job_next">Shortest Processing Time</a> and
discussing such strategies with real-life problems (e.g. something bad happens
when a deadline is crossed).</p>
<p>This example cracked me up:</p>
<blockquote>
<p>There’s an episode of The X-Files where the protagonist Mulder, bedridden and
about to be consumed by an obsessive-compulsive vampire, spills a bag of
sunflower seeds on the floor in self-defense.
The vampire, powerless against his compulsion, stoops to pick them up one by
one, and ultimately the sun rises before he can make a meal of Mulder.
Computer scientists would call this a “ping attack” or a “denial of service”
attack:
Give a system an overwhelming number of trivial things to do, and the
important things get lost in the chaos.</p>
</blockquote>
<p>The chapter also goes a meta-level up:
A perfect schedule or time plan must reflect two things:</p>
<blockquote>
<p>[…], preemption isn’t free.
Every time you switch tasks, you pay a price, known in computer science as a
context switch.</p>
</blockquote>
<p>The little pause between two tasks should be reflected, otherwise, we drown in
task switching:</p>
<blockquote>
<p>Anyone you interrupt more than a few times an hour is in danger of doing no
work at all.</p>
</blockquote>
<p>…for which computer science also has a name:
<a href="https://en.wikipedia.org/wiki/Thrashing_(computer_science)">Thrashing Phenomenon</a></p>
<p>But that is not all, the time in which we are rethinking to create the plan
must also be part of the plan, which is where it gets complicated.
In many cases, no perfect plan exists because whenever a human or a machine
waits for external events to happen, they have to deal with uncertainty and
in between do what’s possible, which in turn brings new problems:</p>
<blockquote>
<p>What makes real-time scheduling so complex and interesting is that it is
fundamentally a negotiation between two principles that aren’t fully
compatible.
These two principles are called responsiveness and throughput:
How quickly you can respond to things, and how much you can get done overall.</p>
</blockquote>
<p>Most computer scientists already know this example from real-time scheduling
lectures at university, but I think that this is one of the most interesting
examples in this chapter:</p>
<blockquote>
<p>For the first time ever, a rover was navigating the surface of Mars. The $150
million Mars Pathfinder spacecraft had accelerated to a speed of 16,000 miles
per hour, traveled across 309 million miles of empty space and landed with
space-grade airbags onto the rocky red Martian surface.
And now it was procrastinating.</p>
</blockquote>
<p>By <em>procrastinating</em> the author means that the $150 million Pathfinder was not
responding to commands from the earth due to a scheduling problem called
<a href="https://en.wikipedia.org/wiki/Priority_inversion">Priority Inversion</a>.
A good solution for priority inversion is
<a href="https://en.wikipedia.org/wiki/Priority_inheritance">Priority Inheritance</a>:
If you realize that employees come late to very important meetings all the time,
do some research to find the reason and find out that the coffee machine is so
slow that employees end up in endless queues:
Increase the importance of high-quality coffee machine maintenance to the same
level like the most important meetings.</p>
<p>The main message is:</p>
<blockquote>
<p>As business writer and coder Jason Fried says, “Feel like you can’t proceed
until you have a bulletproof plan in place? Replace ‘plan’ with ‘guess’ and
take it easy.” Scheduling theory bears this out.</p>
</blockquote>
<h3 id="bayess-rule---predicting-the-future">6. Bayes’s Rule - Predicting the Future</h3>
<p>This chapter begins with a problem that, if it was solved, would make scheduling
much easier: Predicting the future.
Statistics and stochastic theory are typically used to model the certainty of
the timing and quantity of events or things.
The whole chapter starts with historic developments on this matter and tries to
make it entertaining a bit, but statistics are still notoriously hard to make
look interesting.</p>
<p>The biggest part of the chapter educates (in easy-to-understand ways) about
<a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes’ Rule</a>,
<a href="https://en.wikipedia.org/wiki/Rule_of_succession">Laplace’s Law</a>,
the <a href="https://en.wikipedia.org/wiki/Copernican_principle">Copernican Principle</a>,
<a href="https://en.wikipedia.org/wiki/Normal_distribution">normal distribution</a>
vs.
<a href="https://en.wikipedia.org/wiki/Power_law">power-law distribution</a>, and many
others, which is relatively boring.
After we read through that part (or skipped it), we reach my favorite example:</p>
<div class="floating-image-right">
<figure>
<img src="/images/books/algorithms-to-live-by-marshmallow.webp" alt="Marshmallow Torture" />
<figcaption aria-hidden="true">Marshmallow Torture</figcaption>
</figure>
</div>
<p>The
<a href="https://www.simplypsychology.org/marshmallow-test.html">Marshmallow experiment</a>
is widely known:</p>
<blockquote>
<p>Each child would be shown a delicious treat, such as a marshmallow, and told
that the adult running the experiment was about to leave the room for a while.
If they wanted to, they could eat the treat right away.
But if they waited until the experimenter came back, they would get two
treats.
Unable to resist, some of the children ate the treat immediately.
And some of them stuck it out for the full fifteen minutes or so until the
experimenter returned, and got two treats as promised.</p>
</blockquote>
<p>Everyone who knows about this experiment also knows that long-time observation
of these kids suggested that the ones who are patient enough to wait to get
both marshmallows are also generally more successful in their later life (which
has to be questioned because the study has been repeated to find out that the
statistical significance is too weak).
The chapter however continues with an interesting twist, which is by far not as
widely known:</p>
<blockquote>
<p>[…] the most interesting group comprised the ones in between — the ones
who managed to wait a little while, but then surrendered and ate the treat.</p>
</blockquote>
<p>The researchers tried to find out why kids would behave so irrationally - is it
a sheer lack of discipline?
They found out that it is more about trust in authorities than discipline:</p>
<blockquote>
<p>[…], the kids in the experiment embarked on an art project.
The experimenter gave them some mediocre supplies and promised to be back
with better options soon.
But, unbeknownst to them, the children were divided into two groups.
In one group, the experimenter was reliable and came back with the better art
supplies as promised.
In the other, she was unreliable, coming back with nothing but apologies.</p>
</blockquote>
<p>When the marshmallow experiment was repeated with children who first went
through this experiment, the results showed that the children which were
disappointed by the experimenter would more likely give up in the middle of the
marshmallow experiment.
I thought about this a little longer - This is an interesting data point for
potential disadvantages that many (poor/less successful) people might suffer
from:
Some may have grown up in environments where they have been disappointed by
their parents/authorities too often and ended up trusting less in such
investments.</p>
<h3 id="overfitting---when-to-think-less">7. Overfitting - When to Think Less</h3>
<p>This chapter was very interesting and captivating read:
The phenomenon of <a href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a>
is a known problem in the domain of artificial intelligence/machine learning.
Most explanations are extremely abstract and hard to grasp for outsiders.
But actually, overfitting is very easy to understand when explained through
real-life situations:</p>
<blockquote>
<p>In the military and in law enforcement, for example, repetitive, rote training
is considered a key means for instilling line-of-fire skills.
The goal is to drill certain motions and tactics to the point that they become
totally automatic.
But when overfitting creeps in, it can prove disastrous.
There are stories of police officers who find themselves, for instance, taking
time out during a gunfight to put their spent casings in their pockets — good
etiquette on a firing range.</p>
</blockquote>
<div class="floating-image-right">
<figure>
<img src="/images/books/algorithms-to-live-by-training.gif" alt="You become what you train" />
<figcaption aria-hidden="true">You become what you train</figcaption>
</figure>
</div>
<p>The effect of this case of overfitting was an unnecessary increase in dead
officers:</p>
<blockquote>
<p>On several occasions, dead cops were found with brass in their hands, dying in
the middle of an administrative procedure that had been drilled into them.</p>
</blockquote>
<p>Another example from the same domain goes like this:</p>
<blockquote>
<p>[…] the FBI was forced to change its training after agents were found
reflexively firing two shots and then holstering their weapon—a standard
cadence in training — regardless of whether their shots had hit the target
and whether there was still a threat.
Mistakes like these are known in law enforcement and the military as
“training scars,” and they reflect the fact that it’s possible to overfit
one’s own preparation.</p>
</blockquote>
<p>Overfitting and the difficulty to set incentives in a way that they don’t end
up being counter-effective have a lot of overlap:</p>
<blockquote>
<p>[…] focusing on production metrics led supervisors to neglect maintenance
and repairs, setting up future catastrophe.
Such problems can’t simply be dismissed as a failure to achieve management
goals.
Rather, they are the opposite:
The ruthless and clever optimization of the wrong thing.</p>
</blockquote>
<p>The presented examples were my highlights, but the chapter has some more good
ones.
Out of all the theory and real-life examples, the authors extract one piece of
good advice against overthinking:</p>
<blockquote>
<p>The greater the uncertainty, the bigger the gap between what you can measure
and what matters, the more you should watch out for overfitting — that is, the
more you should prefer simplicity, and the earlier you should stop.</p>
</blockquote>
<h3 id="relaxation---let-it-slide">8. Relaxation - Let It Slide</h3>
<p>At university in group assignments, but also later at work while discussing with
colleagues, I often experienced that
<a href="https://en.wikipedia.org/wiki/Perfect_is_the_enemy_of_good">perfect is the enemy of the good</a>:
The group of students or colleagues would rather discuss forever while
ditching one 80-90% idea after the other, instead of simply deciding for one and
live with a “good” result.
(After having the last chapter also discussing the difficulty of setting good
incentives: Often company culture makes employees want to avoid making mistakes
to not damage their career, so it seems better to discuss perfect solutions
forever.)</p>
<div class="floating-image-right">
<figure>
<img src="/images/books/algorithms-to-live-by-relax.webp" alt="Relaxation helps solve hard problems" />
<figcaption aria-hidden="true">Relaxation helps solve hard problems</figcaption>
</figure>
</div>
<p>This is irrational, but what’s the rational way if a problem looks too hard to
solve?
In computer science, problems are considered “too hard” when the runtime and/or
memory complexity of finding the solution has exponential growth (instead of
polynomial, which is considered “easy”).
The chapter contains strategies and algorithms that do
<a href="https://en.wikipedia.org/wiki/Relaxation_(approximation)">Constraint Relaxation</a>:</p>
<blockquote>
<p>But [computer scientists] don’t relax themselves; they relax the problem.</p>
</blockquote>
<p>The first example that can be solved faster with this strategy is the
<a href="https://en.wikipedia.org/wiki/Travelling_salesman_problem">Traveling Salesman Problem</a>:
If you want to visit many places, what is the right order to visit them that at
the same time gives you the shortest overall travel distance?
To really find out, one would have to sum up the travel distances of
<em>all</em> possible paths and then take the shortest one out of the huge list.
Constraint relaxing algorithms make shortcuts and are hence faster, but don’t
guarantee you the correct result - instead, you get a “good” answer
that’s most probably better than following your gut feeling or just trying
randomly.
The message is clear:</p>
<blockquote>
<p>If we’re willing to accept solutions that
are close enough, then even some of the hairiest problems around can be tamed
with the right techniques.</p>
</blockquote>
<p>This chapter is really short and less technical than the following ones which
will pick up on relaxation again,
but it leaves another very interesting life-philosophic inspiration:</p>
<blockquote>
<p>One day as a child, Brian was complaining to his mother about all the things
he had to do: his homework, his chores…
“Technically, you don’t have to do anything,” his mother replied.
“You don’t have to do what your teachers tell you.
You don’t have to do what I tell you.
You don’t even have to obey the law.
There are consequences to everything, and you get to decide whether you want
to face those consequences.”
Brian’s kid-mind was blown.
It was a powerful message, an awakening of a sense of agency, responsibility,
moral judgment.</p>
</blockquote>
<p>It was probably not clear to both at that moment, but she taught him
<a href="https://en.wikipedia.org/wiki/Lagrangian_relaxation">Lagrangian Relaxation</a>:
Take rules (constraints) and transform them into costs which means taking the
<em>impossible</em> and downgrading it to <em>costly</em>.
Sometimes the consequences of breaking some rules are less bad than not solving
some bigger problem.</p>
<h3 id="randomness---when-to-leave-it-to-chance">9. Randomness - When to Leave It to Chance</h3>
<p>We usually only leave important things to chance (if at all) when we exhausted
all other strategies, and they did not work out.
This chapter illuminates some use cases where introducing randomness into
algorithms makes them perform better than deterministic algorithms would.
Again, let us skip over the rich details and background stories that are shared
about the
<a href="https://en.wikipedia.org/wiki/Monte_Carlo_method">Monte Carlo Method</a>,
<a href="https://en.wikipedia.org/wiki/Bloom_filter">Bloom Filters</a>,
<a href="https://en.wikipedia.org/wiki/Hill_climbing">Hill Climbing</a>,
<a href="https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm">Metropolis Algorithm</a>,
and
<a href="https://en.wikipedia.org/wiki/Simulated_annealing">Simulated Annealing</a>.
Instead, let’s have a look at the
<a href="https://en.wikipedia.org/wiki/Miller%E2%80%93Rabin_primality_test">Miller-Rabin Primality Test</a>
and its significance to the majority of private communication on this planet:</p>
<div class="floating-image-right">
<figure>
<img src="/images/books/algorithms-to-live-by-simulated-annealing.gif" alt="Animated solution of the Traveling Salesman Problem with Simulated Annealing source" />
<figcaption aria-hidden="true">Animated solution of the Traveling Salesman Problem with Simulated Annealing <a href="https://en.wikipedia.org/wiki/Simulated_annealing">source</a></figcaption>
</figure>
</div>
<p>When encrypting messages before sending and decrypting after receiving them,
we typically rely on
<a href="https://en.wikipedia.org/wiki/Public-key_cryptography">asymmetric cryptography</a>.
For the user, this means that encryption and decryption are done with <em>different
keys</em>.
Having different keys has the advantage that the key for encryption can be
publicly shared, enabling anyone to send us an encrypted message that only we
can read.
The decryption key is stored and protected in private.
Without going too deep into detail, such algorithms rely on the fact that there
are no “fast” (following the notion of fast from the earlier chapters)
mathematical algorithms that can reverse multiplication or exponentiation of
very large numbers if not all variables are known (i.e. the key that is part of
the calculation).
A simple example with small numbers is the question “What are the two prime
factors of the number <code>15</code>?”.
The answer is: <code>3 * 5</code> equals <code>15</code> as both <code>3</code> and <code>5</code> are prime and there
is no other combination of prime numbers for which this works, so this
is the correct answer.
For small numbers, this is very easy, but for huge numbers, computers would need
centuries - so the designers of algorithms like
<a href="https://en.wikipedia.org/wiki/Pretty_Good_Privacy">PGP</a> decided to rely on the
security of messages on this fact:
The user’s private key for decryption consists of two huge prime numbers and
must be kept secret.
The public key that the user can freely send around consists of … the
<em>product</em> of these numbers. (So if the combination of <code>3</code> and <code>5</code> would be the
user’s private key, the product <code>15</code> would be the public one!)
This means that everyone actually <em>has</em> access to the secret private key.
It is nonetheless safe from being misused because no one can extract the
two individual prime numbers from it.
This might sound like a wonky foundation for security for everyone who thought
that messages are only secure if third parties cannot decrypt
them, but such a perfect algorithm does not exist:
The best we can get is that it’s only <em>too hard</em> to decrypt a message within the
same century.</p>
<p>It does not stop there:
Let’s see how the user selects the prime numbers.
When running a mail program or secure messenger for the first time, the laptop
or cell phone would automatically generate a key for the user
(Often hidden from the user’s sight to improve the user experience).
To get two huge prime numbers, the program rolls the dice to get
two huge numbers and then tests if they are prime.
If they are not, it rolls the dice again and again, until it has two huge prime
numbers.
Testing if a number is a prime number is typically done with the
<a href="https://en.wikipedia.org/wiki/Miller%E2%80%93Rabin_primality_test">Miller-Rabin Primality Test</a>.
This test consists of a formula that is cheap to compute.
Its result tells if the number is a <em>strong probable prime</em> or not - it can’t
say if it <em>is</em> a prime, but only if it is likely one.
To get to <code>99.999999...%</code> (and many more nines) probability of being
correct, the algorithm is repeated a lot.
Now we (or our messenger application) know that we have two prime numbers that
are most probably prime, and unlikely to have been chosen by someone else for
their keys, so we can now happily encrypt our most secret and important
messages with them.
This might again sound like a wonky foundation, but it’s more probable to end up
with insecure keys due to
<a href="https://en.wikinews.org/wiki/Predictable_random_number_generator_discovered_in_the_Debian_version_of_OpenSSL">broken random number generators</a>
than due to trusting the probabilities from the encryption algorithms.
I learned about these things at university in multiple cryptography lectures,
but we were too busy learning how they work to have a look at the history
and real-life stories, so reading about it again from some fresh perspective in
this book was enlightening.</p>
<p>The main message of this chapter is:</p>
<blockquote>
<p>There is a deep message in the fact that on certain problems, randomized
approaches can outperform even the best deterministic ones.
Sometimes the best solution to a problem is to turn to chance rather than
trying to fully reason out an answer.</p>
</blockquote>
<h3 id="networking---how-we-connect">10. Networking - How We Connect</h3>
<p>After beginning with a historic tour of the content of the first telegraph,
first phone call, first <em>mobile</em> phone call, and first text message over the
internet, the first subsection finds an elegant conclusion:</p>
<blockquote>
<p>The foundation of human connection is protocol — a shared convention of
procedures and expectations, from handshakes and hellos to etiquette,
politesse, and the full gamut of social norms.
Machine connection is no different.
Protocol is how we get on the same page; in fact, the word is rooted in the
<a href="https://en.wikipedia.org/wiki/Protocol_(diplomacy)">Greek <em>protokollon</em></a>,
“first glue,” which referred to the outer page attached
to a book or manuscript.</p>
</blockquote>
<p>Most of the chapter goes into explaining how switched packet networking works
because all the digital communication on the planet relies on it.
The interesting bit of switched networking is that there is no such thing as
a <em>connection</em> like telephone calls used to have, although video meeting apps
tell you the opposite.
Ditching dedicated connections gives more flexibility because computers
don’t maintain only a few connections that are used all the time, but instead
maintain many connections which are only used in bursts.
The book shares the amount of technical detail about
<a href="https://en.wikipedia.org/wiki/Transmission_Control_Protocol">TCP</a>,
<a href="https://en.wikipedia.org/wiki/Network_congestion#Congestion_control">Congestion Control</a>,
the <a href="https://en.wikipedia.org/wiki/Two_Generals%27_Problem">Byzantine Generals Problem</a>,
and <a href="https://en.wikipedia.org/wiki/Exponential_backoff">Exponential Backoff</a>
that is needed to bridge the ideas behind them with real-life scenarios that
most readers know.</p>
<div class="floating-image-right">
<figure>
<img src="/images/books/algorithms-to-live-by-squirrel.gif" alt="Approach carefully, withdraw quickly (source)" />
<figcaption aria-hidden="true">Approach carefully, withdraw quickly (<a href="https://www.reddit.com/r/gifs/comments/2p38by/squirrel_fighting_with_a_bag/">source</a>)</figcaption>
</figure>
</div>
<p>I liked how the authors found something in nature that makes TCP’s flow
control strategy look very natural:</p>
<blockquote>
<p>Other animal behavior also evokes TCP flow control, with its characteristic
sawtooth.
Squirrels and pigeons going after human food scraps will creep forward a step
at a time, occasionally leap back, then steadily creep forward again.</p>
</blockquote>
<p>This strategy is also called
<a href="https://en.wikipedia.org/wiki/Additive_increase/multiplicative_decrease">Additive Increase/Multiplicative Decrease</a>
due to its nature to increment the send rate in small steps while the
transmission of packets is successful but drop the rate drastically in case of
transmission errors.
This way messages are often sent at a slower rate than possible, but
transmission errors are kept to a minimum.</p>
<p>The book suggests the application of the AIMD strategy during the onboarding of
new employees:
If it’s unclear how much they will perform in their new environment, increase
their workload in small steps, and as soon as they appear overloaded and make
too many mistakes due to stress, drastically reduce the number again.
Midterm, the employee would work slightly below their maximum and not be
over-stressed, which is good for all participants.</p>
<p>Application of this strategy would also have a positive long-term impact
if applied as a countermeasure against the bad effects of the
<a href="https://en.wikipedia.org/wiki/Peter_principle">Peter principle</a>:
Employees are promoted based on their success in previous jobs until they reach
a level at which they are no longer competent, as skills in one job do not
necessarily translate to another.
The AIMD strategy suggests that we should be able to demote people again if
their last promotion decreased the overall organization’s strength (it does not
appear useful in this case to demote the person <em>multiple</em> levels lower although
AIMD works like that, depending on the metrics).</p>
<p>Two other interesting messages can be extracted from this chapter:</p>
<blockquote>
<p>In 2014, for instance, UC Santa Cruz’s Jackson Tolins and Jean Fox Tree
demonstrated that those inconspicuous “uh-huhs” and “yeahs” and “hmms” and
“ohs” that pepper our speech perform distinct, precise roles in regulating the
flow of information from speaker to listener—both its rate and level of
detail.
Indeed, they are every bit as critical as ACKs are in TCP.
Says Tolins, “Really, while some people may be worse than others, ‘bad
storytellers’ can at least partly blame their audience.”</p>
</blockquote>
<p>I liked to read this because it backs something that I always felt in online
meetings:
If all or most participants are on mute with disabled webcams, it hurts
the effectiveness of online meetings.
Unfortunately, I have seen this online meeting culture a lot.
I typically also annoy the students of <a href="https://qasm.de/">my university lecture</a>
into enabling their webcams because it enables me to present them with a better
listening experience when I see the students’ faces: If they look annoyed, I
might have been talking about the same thing for too long (thinking that the
majority does not understand it yet) and if they pinch their eyes, I might have
skipped over something too quickly.
Giving some kind of talk in front of a screen without faces on it is a bad
experience and makes it harder to give the audience a good experience.</p>
<p>My last highlight of this chapter is the explanation of the technical phenomenon
in computer networks called
<a href="https://en.wikipedia.org/wiki/Bufferbloat">Buffer Bloat</a>:
When network devices that are under heavy load are configured with too large
buffers (This is typically not a configuration knob that is visible for
end-users) to queue up packets that can’t be processed yet, then this often
impacts latencies of TCP network connections negatively.
The real-life example that the authors came up with to explain this to
non-engineers is strikingly simple:</p>
<blockquote>
<p>When Tom took his daughter to a Cinco de Mayo festival in Berkeley, she set
her heart on a chocolate banana crêpe, so they got in line and waited.
Eventually — after twenty minutes — Tom got to the front of the line and
placed his order.
But after paying, they then had to wait <em>forty more minutes</em> to actually get
the crêpe.</p>
</blockquote>
<p>The solution for this problem is simple and works both on network devices and
in crêpe shops:</p>
<blockquote>
<p>Turning customers away would have made everyone better off—whether they ended
up in a shorter crêpe line or went elsewhere.
And wouldn’t have cost the crêpe stand a dime of lost sales, because either
way they can only sell as many crêpes as they can make in a day, regardless
of how long their customers are waiting.</p>
</blockquote>
<p>This might suggest finding peace with one or the other dropped packet, and
more generally in life learning to say “no”:
If you tend to accept too many requests from others to not disappoint
them, you will end up disappointing them with long wait times anyway.</p>
<h3 id="game-theory---the-minds-of-others">11. Game Theory - The Minds of Others</h3>
<p><a href="https://en.wikipedia.org/wiki/Game_theory">Game Theory</a> is all about studying
the rules of games and the strategies that players can follow to get the best
results for them.
This chapter explains the so-called
<a href="https://en.wikipedia.org/wiki/Nash_equilibrium">Nash Equilibrium</a>, which is
a state of any game where all players found the strategy that gives them the
best result from their perspective and everyone sticks to theirs.
Depending on the design of the game, this means anything between good and bad
results for all players.</p>
<div class="floating-image-right">
<figure>
<img src="/images/books/algorithms-to-live-by-prisoners-dilemma.png" width="600" alt="All possible outcomes of the Prisoner’s Dilemma Game for each player’s action" />
<figcaption aria-hidden="true">All possible outcomes of the Prisoner’s Dilemma Game for each player’s action</figcaption>
</figure>
</div>
<p>The best example of a nash equilibrium that brings bad results for the players
is the <a href="https://en.wikipedia.org/wiki/Prisoner%27s_dilemma">Prisoner’s Dilemma</a>:
Imagine two collaborators of a crime are caught but the police do not have
enough evidence to convict them on the principal charge.
During the interrogation, each collaborator has the choice to remain silent or
cooperate with the police, which means betraying their collaborator but getting
out of jail immediately.
If both collaborators betray each other they will however both end up in jail
for long.</p>
<p>The only good way out of this game for both participants is if they really can
trust each other, but the individual player will get the best “score” for them
if they do the betrayal (going out of jail is still better than sitting for just
a few years).
The message of the prisoner’s dilemma is that if you set up a game like this,
the mainstream of participants will converge on the bad behavior because this
is the rational choice resulting from understanding the game.
At this point, it’s easy to blame the players, but the book suggests blaming the
game author for setting up the rules like this in the first place.</p>
<p>While the prisoner’s dilemma will most likely be familiar to most readers
already, the book comes up with two other nice and intriguing examples:</p>
<blockquote>
<p>Imagine two shopkeepers in a small town.
Each of them can choose either to stay open seven days a week or to be open
only six days a week, taking Sunday off to relax with their friends and
family.
If both of them take a day off, they’ll retain their existing market share and
experience less stress.
However, if one shopkeeper decides to open his shop seven days a week, he’ll
draw extra customers</p>
</blockquote>
<p>The Nash equilibrium of this game is a market where all shops are under pressure
to have open all the time.
Depending on the question if this is a good thing for all participants, it might
be necessary to change the rules to relieve the players from pressure.</p>
<p>As intuitive as the market example is, the next example seems counter-intuitive
and surprising at first glance.
What would happen, if a company gave every one of their employees unlimited
vacation time?</p>
<blockquote>
<p>All employees want, in theory, to take as much vacation as possible.
But they also all want to take just slightly less vacation than each other,
to be perceived as more loyal, more committed, and more dedicated (hence more
promotion-worthy).
Everyone looks to the others for a baseline, and will take just slightly less
than that.
The Nash equilibrium of this game is zero.</p>
</blockquote>
<p>This is shocking because it shows how quickly bad scores can result from
initially well-intended rules.
The authors call this the
<a href="https://en.wikipedia.org/wiki/Tragedy_of_the_commons">Tragedy of the Commons</a>.
Stock markets close at defined times as otherwise traders would lose money if
they went to bed, leading to many burned-out traders.</p>
<p><a href="https://en.wikipedia.org/wiki/Mechanism_design">Mechanism Design</a> is then
presented as the solution:
Game theory asks what behavior will result from a given set of rules and
mechanism design asks what set of rules should be given for the desired
behavior.
Mechanism design can help make the game moves that would otherwise be
irrational, rational.
Revenge, for example, is a very natural behavior in animals and humans, but it
is irrational because it just increases the damage and does not bring
anything back to anyone.
Still, it seems to be helpful, because the sheer <em>likelihood</em> that someone
would take vengeance if someone else did them badly models a counterincentive.</p>
<div class="floating-image-right">
<figure>
<img src="/images/books/algorithms-to-live-by-doc-brown.webp" width="400" alt="The problem is not the other drivers - it’s the number of cars" />
<figcaption aria-hidden="true">The problem is not the other drivers - it’s the number of cars</figcaption>
</figure>
</div>
<p>The next interesting principle from game theory research is
<a href="https://en.wikipedia.org/wiki/Price_of_anarchy">the price of anarchy</a>, which
measures how much worse the average outcome of a game gets for everyone due to
selfish behavior.
Games with a high price of anarchy are hence worthy of being redesigned to
reduce the effect of the tragedy of the commons.
Calculating the price of anarchy can even show that some games don’t necessarily
need a redesign, although one would intuitively think so:
The price of anarchy shows that human traffic with egoistic drivers is only
33% worse than a perfect centrally planned traffic of computer drivers (not
including the reduced number of accidents with injuries/deaths).
There are arguments against individual car traffic, but they originate more from
the scaling perspective than from game theory.</p>
<h3 id="conclusion---computational-kindness">Conclusion - Computational Kindness</h3>
<p>My favorite main message of this concluding chapter is, that mathematics and
algorithms show us that we can stop stressing ourselves over always improving in
all areas, because even with optimal strategies, the results are not always
optimal, and accepting that is just rational.
This does not mean that one should not try if science says that the probability
of success is too low - but that one should try and simply adjust their
expectations.</p>
<div class="floating-image-right">
<figure>
<img src="/images/books/algorithms-to-live-by-math-child.webp" width="250" alt="Overly polite and modest answers can leave overwhelmingly many options to the enquirer" />
<figcaption aria-hidden="true">Overly polite and modest answers can leave overwhelmingly many options to the enquirer</figcaption>
</figure>
</div>
<p>This chapter also cultivates the principle of being “computationally kind” to
others:</p>
<blockquote>
<p>We can be “computationally kind” to others by framing issues in terms that
make the underlying computational problem easier.
This matters because many problems—especially social ones, as we’ve seen—are
intrinsically and inextricably hard.
[…]
Politely withholding your preferences puts the computational problem of
inferring them on the rest of the group.
In contrast, politely asserting your preferences (“Personally, I’m inclined
toward x. What do you think?”) helps shoulder the cognitive load of moving the
group toward resolution.</p>
</blockquote>
<p>Life is complicated and full of decisions with no upfront clear outcome, so
relax and follow the final advice:</p>
<blockquote>
<p>In the hard cases, the best algorithms are all about doing what makes the most
sense in the least amount of time, which by no means involves giving careful
consideration to every factor and pursuing every computation to the end.
Life is just too complicated for that.</p>
</blockquote>
<h2 id="summary">Summary</h2>
<p>I think that the people who profit most from reading this book are curious
non-technical people, people who work with (software) engineers (e.g. their
managers), and early students:
The examples and anecdotes are interesting and vivid, as they back otherwise
boring theory with relevant and partly entertaining real-life scenarios that
have strong potential to motivate further study.
Computer scientists will appreciate the examples and anecdotes because they are
entertaining, but also because they help to explain tricky technical situations
to non-technical colleagues with good comparisons when it matters.</p>
<p>While reading about algorithms and strategies and their application to social
situations, I remembered many situations at work where the whole team rendered
trapped in escaping local maximums because solutions worked “well enough” to
not change them, although there were problems that could have been solved by
shaking everything up a little (as suggested by e.g. the simulated annealing
algorithm).
These were situations where people would use all their engineering skills to
solve technical challenges, but would not use the same knowledge to challenge
their feelings and comfort zone - but that is the game changer that would help
many to be more innovative.
A good part of the messages in this book converges to “stop overthinking, even
science says that it’s more rational to try something new”.</p>
<p>This book is a bridge between the technical and the non-technical worlds.
It is not a must-read but a very good book for everyone who likes a mixture of
slight entertainment, story-telling, and a closer but not too technical look at
interdisciplinary connections of life with mathematics and computer science.
If you don’t read it, you might be missing out on some of the most interesting
details of the inner workings of the modern world.</p>]]></summary>
</entry>
<entry>
    <title>Book Review: The Phoenix Project</title>
    <link href="https://blog.galowicz.de/2022/12/19/book-review-the-phoenix-project" />
    <id>https://blog.galowicz.de/2022/12/19/book-review-the-phoenix-project</id>
    <published>2022-12-19T00:00:00Z</published>
    <updated>2022-12-19T00:00:00Z</updated>
    <summary type="html"><![CDATA[<!-- cSpell:words Behr ITPI Spafford Eliyahu Goldratt Miyagi Kanban Andon -->
<!-- cSpell:words Behr Jidōka Autonomation Kaizen culting rebranding -->
<!-- cSpell:words overcommitted incentivized Beyoncé CISO Pesche -->
<!-- cSpell:words Taiichi Ohno Zuckerberg rebranded -->
<!-- cSpell:ignore -->
<p><a href="https://amzn.to/3hcHGpf">The Phoenix Project</a> is a novel that has been declared
a must-read by many IT executives.
It provides smart solutions to the problems that most IT companies struggle with
and is at the same time educating to read.
What principles does it teach, and where does it fall short?</p>
<!--more-->
<div class="book-cover">
<figure>
<img src="/images/books/the-phoenix-project.jpg" alt="Book Cover of “The Phoenix Project”" />
<figcaption aria-hidden="true">Book Cover of “The Phoenix Project”</figcaption>
</figure>
</div>
<h2 id="book-authors">Book &amp; Authors</h2>
<p><a href="https://amzn.to/3hcHGpf">Link to the Amazon Store Page</a></p>
<p>The first edition of this book is from 2013, its second version is from 2014,
and the third version, which is called <em>the 5th-anniversary edition</em>, was
released in 2018.
This version of the book is ~350 pages for the main story but contains
additional ~70 pages for an excerpt of
<a href="https://amzn.to/3iXmzHW">The DevOps Handbook</a>.</p>
<p><a href="http://www.realgenekim.me/">Gene Kim</a> is a multi-award-winning CTO, researcher,
and author.
His books include <a href="https://amzn.to/3iXmzHW">The DevOps Handbook</a>,
<a href="https://amzn.to/3W5QGf3">The Visible Ops Handbook</a>, and
<a href="https://amzn.to/3uEkxPz">Visible Ops Security</a>.</p>
<p><a href="https://www.linkedin.com/in/kevinbehr/">Kevin Behr</a> is the founder of the
<a href="https://itpi.org/">Information Technology Process Institute (ITPI)</a> and the
general manager and chief science officer of
<a href="https://www.praxisflow.com/">Praxis Flow LLC</a>.
He is also the co-author of <a href="https://amzn.to/3W5QGf3">The Visible Ops Handbook</a>.</p>
<p><a href="https://www.linkedin.com/in/gspafford/">George Spafford</a> is a
<a href="https://www.gartner.com/analyst/38065">research director for Gartner</a>.
His publications include articles and books on IT service improvement, as well
as co-authorship of <a href="https://amzn.to/3W5QGf3">The Visible Ops Handbook</a>, and
<a href="https://amzn.to/3uEkxPz">Visible Ops Security</a>.</p>
<h2 id="content-and-structure">Content and Structure</h2>
<div class="floating-image-right black-border-image">
<figure>
<img src="/images/books/the-phoenix-project-bill.jpg" alt="Main Protagonist Bill Palmer" />
<figcaption aria-hidden="true">Main Protagonist Bill Palmer</figcaption>
</figure>
</div>
<p>In this novel, the three gentlemen tell a story about DevOps and organizational
change.
Practically everyone who works in IT will recognize the situations depicted in
the book.
In 1984, <a href="https://en.wikipedia.org/wiki/Eliyahu_M._Goldratt">Dr. Eliyahu M. Goldratt</a>
published the book <a href="https://en.wikipedia.org/wiki/The_Goal_(novel)">The Goal</a>,
which is a novel about bottlenecks in old-school manufacturing processes.
It seems like the Phoenix Project as a book is strongly inspired by this one:
<em>The Goal</em> was about optimizing production plants and <em>The Phoenix Project</em> is
about optimizing IT “plants”, so to say.</p>
<p>I will introduce only the from my point of view most important three figures
out of twelve from the book:</p>
<p>The story is told from the perspective of the protagonist
<u>Bill Palmer</u> and begins like this:</p>
<blockquote>
<p>Bill is an IT manager at Parts Unlimited.
It’s Tuesday morning, and on his drive into the office, Bill gets a call from
the CEO.
The company’s new IT initiative, code named <em>Phoenix Project</em> is critical to
the future of Parts Unlimited, but the project is massively over budget and
very late.
The CEO wants Bill to report directly to him and fix the mess in ninety days
or else Bill’s entire department will be outsourced.</p>
</blockquote>
<div class="floating-image-right black-border-image">
<figure>
<img src="/images/books/the-phoenix-project-erik.jpg" alt="Board Candidate Erik Reid" />
<figcaption aria-hidden="true">Board Candidate Erik Reid</figcaption>
</figure>
</div>
<p>In the middle of all the turmoil that Bill experiences, he is asked to take time
to meet <u>Erik Reid</u>, who is also described by other figures in the
book as a mysterious <em>tech hotshot</em>.
At first, Erik seems forced upon the protagonist, with no obvious specialty
other than forgetting everyone’s names all the time.
Bill does not get why Erik is invited to become a board member.
Their interaction begins with strange homework that Erik gives Bill, like
finding out what “the four types of work” are and calling him afterward.
Erik seemingly knows a lot about optimizing IT organizations,
but he lets Bill find out most of it by himself instead of just explaining it.
This way he initially comes across as some complacent and self-imposed version
of <a href="https://en.wikipedia.org/wiki/Mr._Miyagi">Mr. Miyagi</a>.
Over the rest of the book, he proves to be a helpful genius and a great mentor
to Bill.
With Erik’s help, he learns to use his new to get the chaos under control.</p>
<p><u>Brent Geller</u> is mentioned in the second chapter when some
huge infrastructure outage occupies a whole department.
Bill describes Brent to the reader:</p>
<div class="floating-image-right black-border-image">
<figure>
<img src="/images/books/the-phoenix-project-brent.jpg" alt="Super 10X Engineer Brent Geller" />
<figcaption aria-hidden="true">Super 10X Engineer Brent Geller</figcaption>
</figure>
</div>
<blockquote>
<p>He’s always in the middle of the important projects that IT is working on.
I’ve worked with him many times.
He’s definitely a smart guy but can be intimidating because of how much he
knows. What makes it worse is that he’s right most of the time.</p>
</blockquote>
<p>Throughout the story, Brent presents himself as the bottleneck of the entire
organization.
Due to his outstanding competence and knowledge of the company’s software and
systems, practically nothing gets done without him.
This did not happen intentionally - it just ended up like this.</p>
<p>On the one hand, you want to have employees like Brent, because they are
enormously productive.
On the other hand, you don’t want to depend too much on individual employees
like that, because this drives up your
<a href="https://en.wikipedia.org/wiki/Bus_factor">bus factor</a>.
As multiple bosses have direct “access” to Brent, he seems like a shared
resource.
The organizational problem with this is that he is seriously overcommitted
because there is no overarching plan/control of his time availability.
It even gets worse over time as he is pulled into every new emergency that
happens over time, which involuntarily makes him a full-time firefighter.
But he also never says <em>no</em>, because he is not the boss and may be too nice a
person.</p>
<p>The problem with Brents in companies has a striking similarity to the <em>priority
inversion problem</em> that brought down the
<a href="https://en.wikipedia.org/wiki/Mars_Pathfinder">Mars Pathfinder</a> project.
(Which is also mentioned in the
<a href="/2022/12/26/book-review-algorithms-to-live-by">next book review</a>)
Throughout the book’s story, <em>the Brent Problem</em> is solved by meticulously
controlling Brent’s schedule and protecting it and him from unauthorized
external influences.</p>
<p>While Bill and Erik form the team that conveys the book’s main principles to
the reader, Brent embodies all the firefighting in IT companies.
Their interplay with all the other characters (which also are very original
and recognizable from what you know from other IT organizations, especially the
CISO John Pesche) is exciting to read, but let’s continue with the principles
that the book teaches.</p>
<h3 id="the-four-types-of-work">The Four Types of Work</h3>
<p>The <em>four types of work</em> that Erik teaches Bill are inspired by Dr. Eliyahu M.
Goldratt’s
<a href="https://en.wikipedia.org/wiki/Theory_of_constraints">Theory of Constraints</a>.
This theory’s main point is that to increase the throughput of a
system, its constraints (bottlenecks) need to be identified, and then everything
needs to be subordinated to the task of removing them.
All other efforts that don’t concentrate on removing actual constraints are
futile.</p>
<p>The book slightly deviates from this school that stems from manufacturing to fit
its learnings into the world of modern IT organizations, which share some but
not all problems, or not the same way, with manufacturing organizations (storage
costs are for example not something that a software organization must optimize
away).
It takes nearly the first half of the book for Bill to identify the four types
of work that Erik asked him for.
The four types as presented in the story are:</p>
<dl>
<dt>Business Projects</dt>
<dd>
<p>Business initiatives, most of the development work.
The main reason for companies to even exist.</p>
</dd>
<dt>IT Operations Projects</dt>
<dd>
<p>IT Operations exist to support the Business.
They are not an end in itself, but end up being so complex that they make
practically every big company an IT company compared to how many employees
are hired to operate IT infrastructure and projects.</p>
</dd>
</dl>
<p>The exact amount of business and IT operations projects is often not properly
tracked.</p>
<dl>
<dt>Changes</dt>
<dd>
<p><em>Changing</em> and updating processes and (infra)structure is another type of work.
The tasks of maintaining the normal business of some big infrastructure and
the task of changing something in it are most of the time contradictory
because it is hard to find ways how to do both at the same time.
Changes are typically strategic and proactive.</p>
</dd>
<dt>Unplanned Work</dt>
<dd>
<p>Unplanned work is urgent firefighting that keeps organizations from reaching
their goals.
Technical debt is the main driver behind it.
Unplanned work hinders employees from keeping up with planned work, because of
the exceptional urgency that it has, e.g. before release deadlines.
Erik mentions <em>anti-work</em> as another synonym.
Unplanned work is tactical and reactive.</p>
</dd>
</dl>
<p>In chapter 19, Eric Reid explains the impact of unplanned work:</p>
<blockquote>
<p>“Unplanned work has another side effect.
When you spend all your time firefighting, there’s little time or energy
left for planning.
When all you do is react, there’s not enough time to do the hard mental work
of figuring out whether you can accept new work.
So, more projects are crammed onto the plate, with fewer cycles available to
each one, which means more bad multitasking, more escalations from poor code,
which mean more shortcuts.
As Bill said, ‘around and around we go.’ It’s the IT capacity death spiral.”</p>
</blockquote>
<p>This categorization is simple to follow and to adapt your organization to.
If we look at a snapshot of an organization in good times, we would just need
business projects because these earn the money, and the IT projects
enable/support the business.
When everything goes well, no changes are needed because there is no unplanned
work.
But of course, times change, markets change, etc., and after some scaling up and
down structures cease to work correctly which creates unplanned work and the
need for change.</p>
<h3 id="the-three-ways">The Three Ways</h3>
<p>The three ways reminded me a lot of the ideals and principles that I learned
earlier from reading the
<a href="https://amzn.to/3VSudBY">Toyota Production System (TPS) Book by Taiichi Ohno</a>.
This is not completely surprising as Toyota developed its ideas of optimized
manufacturing after world war 2.
In the 80s, they were so efficient that the world started copying their ideas.
I will add some insights from the TPS to the learnings from the book to
show the overlap and practicality of the ideas.</p>
<p>Erik’s adaption of how to adapt the constraint theory on IT in three ways
(/steps) looks like the following:</p>
<dl>
<dt>1.) Flow</dt>
<dd>
<p>By <em>flow</em>, the book means the flow of <em>value</em> produced by the software, which
originates in development and “moves” to operations.
Customers wish for new features and bug/security fixes all the time.
Maximizing this flow means increasing the <em>throughput</em> and decreasing the
<em>wait times</em> (latencies) for such.
This flow is increased by making work in progress (WIP) visible and then
reducing batch sizes and work intervals, and enhancing the quality and defect
prevention.
With this global goal in mind, every value-adding stage that passes
deliverables downstream to the next stage has to perform such optimizations.</p>
</dd>
</dl>
<p>At this point, this reads like a description of
<a href="https://en.wikipedia.org/wiki/Kanban">Kanban</a>:
Supervisors write work packages on cards and move them between columns that
indicate that something is planned, work in progress (WIP), or done.
At the same time, the amount of cards in the WIP stage is kept at a minimum.</p>
<figure>
<img src="/images/books/the-phoenix-project-kanban-board.jpg" alt="A Kanban Board to visualize WIP at a Toyota production plant (source)" />
<figcaption aria-hidden="true">A Kanban Board to visualize WIP at a Toyota production plant (<a href="https://www.toyota-global.com/company/history_of_toyota/75years/text/entering_the_automotive_business/chapter1/section4/item4.html">source</a>)</figcaption>
</figure>
<dl>
<dt>2.) Feedback</dt>
<dd>
<p>The second way is all about the <em>defect prevention</em> that was mentioned in the
first way, and the resolving of bottlenecks/constraints.
The earlier defects are detected, the faster and cheaper they can be fixed.
(Ever heard of <a href="https://en.wikipedia.org/wiki/Shift-left_testing">Shift-Left</a>?)
For this reason, the organization must be built in a way that enables early
defect detection.
As soon as a defect is detected, instead of finding the one to blame, the
organization must concentrate on the opportunity to learn from the defect and
optimize the flow to prevent it from happening again.</p>
</dd>
</dl>
<p>In Toyota production facilities, this is called
<a href="https://en.wikipedia.org/wiki/Andon_(manufacturing)">Andon</a>:
Whenever a defect is found (or an accident happens), workers would pull the
<em>Andon Cord</em> which immediately stops the assembly line and signals one of many
numbered light indicators on an <em>Andon board</em>.
A supervisor would then rush for assistance to find out how to resolve the
problem and prevent it in the future.
(Which sometimes results in small fixes, sometimes in going up the organization
hierarchy to find more involved solutions)</p>
<figure>
<img src="/images/books/the-phoenix-project-andon-board.jpg" alt="An Andon board at a Toyota production plant (source)" />
<figcaption aria-hidden="true">An Andon board at a Toyota production plant (<a href="https://www.toyota-global.com/company/history_of_toyota/75years/text/entering_the_automotive_business/chapter1/section4/item4.html">source</a>)</figcaption>
</figure>
<p>Workers are typically incentivized to keep the throughput of their assembly line
maximized.
At first sight, this incentive contradicts the other incentive to keep the
quality high if this means stopping the assembly line from time to time.
This might be reminiscent of programmers who are encouraged to create as many
“green” (finished) JIRA tickets as possible per sprint in a hurry, which keeps
them from stopping from time to time what these are actually used for and how to
think better in terms of quality increase and defect avoidance.</p>
<p>When automatic production machines/robots are enhanced with automatic quality
checks, this is called
<a href="https://en.wikipedia.org/wiki/Autonomation">Jidōka (Autonomation)</a>.
The equivalent of this in software engineering is unit- and integration tests
that are executed after <em>every little change</em>.
These can (but in many companies don’t) act as quality gates that need before
some code change is merged into a release-relevant branch.</p>
<p>Creating fast feedback is critical to achieving quality, reliability, and safety
in the technology value stream.
The challenge is to balance the seemingly contradictive mix of incentives:
On the one hand, workers shall work quickly to produce high throughput.
On the other hand, they shall take some time to look out for defects and
interrupt production if they find any.
This combination is achieved by distributing responsibility for quality over
<em>all</em> the links in the production chain.</p>
<p>At Google, you can find this principle embodied by
<a href="https://www.oreilly.com/library/view/software-engineering-at/9781492082781/ch01.html">The Beyoncé Rule</a>,
which says <em>“If you liked it, you should have put a CI test on it”</em> -
or more formally:</p>
<blockquote>
<p>“If a product experiences outages or other problems as a result of
infrastructure changes, but the issue wasn’t surfaced by tests in our Continuous
Integration (CI) system, it is not the fault of the infrastructure change.”</p>
</blockquote>
<p>The comparison with production facilities lags a little bit because the
factory optimization theory does not directly map to how software engineering
works:
The output of a factory worker is assembled parts, while the output of a
software developer is changed software (which resembles blueprints).</p>
<dl>
<dt>3.) Continual Learning and Experimentation</dt>
<dd>
<p>The third way is about creating an organization with a culture of continual
learning and experimentation.
The book argues that high-performing operations require and actively promote
learning, with workers performing experiments in their daily work to generate
new improvements. (Remember the <a href="/2022/12/05/book-review-a-philosophy-of-software-design/">review of John Ousterhout’s book
“A Philosophy of Software Design”</a>
that says that code should be designed twice?)
The third way can only be achieved by companies that create an atmosphere
in which workers feel safe to experiment, speak up, and suggest changes.</p>
</dd>
</dl>
<p>In the Toyota Production System, continuous improvement culture throughout the
whole company from the CEO to the line worker is called
<a href="https://en.wikipedia.org/wiki/Kaizen">Kaizen</a>.
As an electrical engineer, this way of looking at an organization reminded me of
the <a href="https://en.wikipedia.org/wiki/Systems_theory">Systems Theory</a> lectures at
university and <a href="https://en.wikipedia.org/wiki/Cybernetics">Cybernetics</a>
literature.</p>
<h2 id="interpretation-and-opinion">Interpretation and Opinion</h2>
<p>During my career, I felt intrigued by ideas/schools like
<a href="https://en.wikipedia.org/wiki/Scrum_(software_development)">Scrum</a>,
<a href="https://en.wikipedia.org/wiki/Agile_software_development">agile</a>,
<a href="https://en.wikipedia.org/wiki/Kanban_(development)">Kanban</a>,
<a href="https://en.wikipedia.org/wiki/Extreme_programming">Extreme Programming</a>, etc.
because they promise a cure for many problems that IT companies have, but the
real-life adaptions that I have seen in most companies were rather
underwhelming.
It does not seem like you can get a few employees a Scrum certification, use
Kanban boards in JIRA and that’s it.
For that reason, I decided to delve deeper into this topic, and find the
original literature behind it, which ultimately led me to read the book about
<a href="https://amzn.to/3BxYPAt">The Toyota Production System</a>.
That was an eye-opening experience because reading the original literature
empowers one to assess the value behind all the structures and processes better
in relation to the big-picture outcome that they shall achieve.
Today I know that most companies simply do it wrong, mostly by
<a href="https://en.wikipedia.org/wiki/Cargo_cult_programming">cargo-culting</a>
what the latest books say without having understood completely what they are
aiming for.</p>
<p>Much later, I read the Phoenix project and got to know the Theory of Constraints
that it refers to.
Due to this order, I got the impression that the theory of constraints is kind
of a <em>rebranding</em> of the learnings from the TPS, as they are so strikingly
similar.</p>
<p>However, no one in IT companies ever reads TPS, but the Phoenix Project has been
a confirmed must-read by many in the IT industry.
Being an impactful book that motivated many professionals to change things for
the better, it proved its value in the book market to me.
Not to forget that it was so much fun to read because it’s a novel and not a
boring textbook (which is another reason for its popularity)!
The fact that it is a novel that explains it at a high level also brought many
managers and deciders to read it, and finally adopt the right mindset on how to
tackle IT.</p>
<p>Looking at the <strong>four kinds of work</strong>, I experienced that many employees confuse
<em>change control</em> with <em>unplanned work</em>.
In the last few years, I have seen Phoenix-Project-educated employees fighting
some <em>change</em> work because they were clogged with <em>unplanned work</em>, claiming
that the change project is unplanned work.
Many employees would jump on piles of unplanned work without reflecting that
<em>this</em> is the thing they need to fight (or believing that it’s hopeless to try)
- this is priority inversion at work.
It seems like it is not trivial for many to change perspectives just by reading
a novel.</p>
<p>This confusion was sometimes due to the fact that you can’t ask someone to
change something without reducing the burden of meeting deadlines from them at
some other point.
It happens a lot that managers try to keep the machine running and don’t dare to
relieve some resources from firefighting to make them available for strategic
change.
But even if they do,
<a href="https://lsaglobal.com/blog/how-top-leaders-combat-the-7-most-common-reasons-employees-fight-change/">organization structures often defend against change</a>.
Just handing out the Phoenix Project to people without further moderation is not
enough, with all the misunderstandings that I have seen.</p>
<p>Looking at the <strong>three ways</strong>, it’s clear that the first two are mostly
technical challenges.
Most companies do not keep up well, but fixing that is “just” a technological
challenge that a company can put their smartest employees and best managers on.
It is a matter of priority, resources, and the right technology.
Throw away your waterfall planning and stateful asynchronous testing pipelines.
Replace it with proper processes and synchronous quality barriers.
I am happy to help:
<a href="https://calendly.com/jacek-galowicz/60-minute-meeting">Let’s schedule a meeting and look at it together</a>.</p>
<p>The third way seems to be the hardest one because it is not technical but
purely about the social part of organizations:
Organizations consist of humans which sometimes hoard information, withhold it
for political reasons, and/or distort it to look better.
Some feel insecure (Especially among IT jobs, the impostor syndrome seems
widespread) and just try to keep their job.
The majority of employees are not bursting with motivation for performance and
desire for change, but are happy to find a defined list of tasks that they can
complete every week and in return feed their family and pay off their house
mortgage.
Many just want to secure their jobs and income by showing up in the morning,
following all the rules, signing off in the late afternoon, and making sure that
they did not do anything wrong.
There is nothing wrong with such an attitude - but it also does not model the
self-reflective and improvement-hungry worker that the Toyota Production System
and Theory of Constraints are asking for.</p>
<p>On the other side, organizations are often managed by strict processes and
rules, and trying to change them can result in punishment.
In many companies, the best way to make a career is to not do anything wrong.
So in between all the average employees, the ones who stand up for change might
be trimmed down by the org.</p>
<p>Both the TPS and the third way envision an organization that consists of a
majority of self-thinking and motivated workers who like to experiment -
but without experimentation being an explicit task and instead more an implicit
part of every workday.
(This sounds like the
<a href="/2022/12/05/book-review-a-philosophy-of-software-design"><em>strategic programmer</em> from John Ousterhout’s book</a>)</p>
<p>The majority of humans are not like this.
After looking at our school system, it’s not surprising:
People spend ten to twenty years of their early life in school and university
(For me it was 13 years of school, three for the Bachelor’s and two for
the Master’s degree - close to 20 years!).
At school, you learn that you are successful when you say what the teacher wants
to hear, and your answers must fit in the boxes of the exam paper (or worse, you
must check one of the four options given in a multiple-choice test).
If you had some teachers that were not like this - good for you - but it’s not
the experience of the majority.
It does not get much better at university, which is more difficult but success
still equals giving the right answers and results in exam papers.
Trying something new and doing it differently than others did before is not part
of the education system, which teaches conformity above everything else.
(In the next book review, we will see what this has to do with the
<a href="/2022/12/26/book-review-algorithms-to-live-by">Overfitting Problem</a>.)
To put it bluntly: Our school systems produce employees who expect their tasks
to be given in tickets that are presented like exam questions and which are not
to be questioned themselves regarding the task, the deliverable, or the method.
This way to look at it is especially interesting when looking at famous
college/university dropouts who were successful because they did <em>not</em> walk the
intended paths (Bill Gates, Mark Zuckerberg, Steve Jobs, etc.).</p>
<p>The remaining challenge for IT leaders of organizations is to find out how to
<em>empower the minority</em> of employees that show motivation for change,
experimentation and improvement before they are trimmed down by the rest of the
organization, or before they quit to launch their own business.
All the TPS/Theory of Constraints ideas need to be taken on by those.
The organization (and the part of the org which does not care about change or
even fights it) must give them enough room.</p>
<h2 id="summary">Summary</h2>
<p>Although I typically don’t read novels, I was not able to put this one down
until I finished it!
The characters and the development around them are very relatable.
It was exciting to see the protagonist go through a lot of trouble and
frustration but then finally pick up on a great success path.
The name <em>Brent</em> has become a synonym for human constraints in the company I
worked at in the last few years.</p>
<p>The idea to model a business with the three ways and divide the different
kinds of work into four categories makes sense.
It is just a model, and no model is ever a silver bullet that fits everywhere,
but I found this way to look at the structures and processes of an IT company
very convincing.
The underlying ideas are very old but they are rebranded in a way that helps IT
specialists apply them in their world.
The vocabulary also helps in convincing others.</p>
<p>This book is another must-read, but only for those who haven’t read the primary
TPS literature, or for those who simply prefer fun-to-read novels.</p>]]></summary>
</entry>
<entry>
    <title>Book Review: The Culture Map</title>
    <link href="https://blog.galowicz.de/2022/12/12/book-review-the-culture-map" />
    <id>https://blog.galowicz.de/2022/12/12/book-review-the-culture-map</id>
    <published>2022-12-12T00:00:00Z</published>
    <updated>2022-12-12T00:00:00Z</updated>
    <summary type="html"><![CDATA[<!-- cSpell:words INSEAD Ringi -->
<!-- cSpell:ignore erinmeyer -->
<p>Why do American coworkers always communicate so clearly and directly, but at the
same time criticize rather indirectly and cushioned?
When do Asian coworkers finally jump in and say something in a meeting?
How do German coworkers get done <em>anything</em> innovative in their fixated planning
madness?
After having experienced many strange situations in projects with people from
around the world, I found Erin Meyer’s book
<a href="https://amzn.to/3h7RAZ6">The Culture Map</a> to be so insightful that I would
declare it a must-read for any software professional who works with or in
international teams.</p>
<!--more-->
<div class="book-cover">
<figure>
<img src="/images/books/the-culture-map.jpg" alt="Book Cover of “The Culture Map”" />
<figcaption aria-hidden="true">Book Cover of “The Culture Map”</figcaption>
</figure>
</div>
<h2 id="book-author">Book &amp; Author</h2>
<p><a href="https://amzn.to/3h7RAZ6">Link to the Amazon Store Page</a></p>
<p>The book is originally from 2014 but got its new yellow cover in 2016, which
is still called the first edition.
It presents 8 handy chapters distributed over ~290 pages.</p>
<p><a href="https://erinmeyer.com">Erin Meyer</a> is a professor at
<a href="https://www.insead.edu/faculty-research/faculty/erin-meyer">INSEAD</a>, one of the
world’s leading business schools.
After having lived in many parts of the world and having helped global leaders
to manage the complexities of cultural differences, she presents this amazing
book that explains the cultural differences that she studied in striking
clarity.</p>
<h2 id="content-and-structure">Content and Structure</h2>
<p>There are plenty of articles out there that discuss the differences between
work cultures <em>per country</em> and also give handy tips about what to do and not to
do in different cultures at work to not commit a blunder.</p>
<p>What this book makes different is that it does not simply explain
“Colleagues from country A are like this and colleagues from country B are like
that”.
Instead, it categorizes certain <em>traits</em> which are common among all humans, and
puts each work culture in relation to each other on one scale for each trait.</p>
<h3 id="the-eight-scales">The Eight Scales</h3>
<p>Erin argues that there are eight different traits that can be projected onto
one scale each.
Every scale’s extremes on the left and right describe opposing behavior/rules
in certain scenarios:</p>
<figure>
<img src="/images/books/erin_meyer_multicultural_characteristics_chart.jpg" alt="Cultural Traits on the 8 scales. (source: erinmeyer.com)" />
<figcaption aria-hidden="true">Cultural Traits on the 8 scales. (<a href="https://erinmeyer.com/mapping-out-cultural-differences-on-teams/">source: erinmeyer.com</a>)</figcaption>
</figure>
<p>Each chapter of the book explains one of the scales.
The advantage of the approach to looking at these traits in isolation and then
putting them together for each culture is that each chapter is short,
comprehensible, and easy to memorize.
On top of that, it is relatively simple to assess cultures that are not treated
in this book on this map of scales.</p>
<p>Apart from per-country culture, many companies have developed their own work
culture which may be different from companies in the same country, but can be
explained quickly and efficiently to outsiders.</p>
<p>Let’s get to these traits.
I will briefly summarize them and maybe give some highlights, but apart from a
few highlights, won’t cover the long list of practicable advice from the full
book.
Every trait is also presented with a scale that positions each country according
to its culture.</p>
<dl>
<dt>1.) Communicating: Low-Context vs. High-Context</dt>
<dd>
<p>Different cultures have a different perspectives on what <em>good communication</em>
means:</p>
</dd>
</dl>
<ul>
<li><u>Low-Context</u>:
<ul>
<li>Precise, simple, clear.</li>
<li>Messages are expressed and understood at face value.</li>
<li>Repetition is appreciated if it helps clarify the communication.</li>
</ul></li>
<li><u>High-Context</u>:
<ul>
<li>Sophisticated, nuanced, and layered.</li>
<li>Messages are both spoken and read between the lines.</li>
<li>Messages are often implied but not plainly expressed.</li>
</ul></li>
</ul>
<p>An interesting perspective covering this scale is that the low-context
communication style could be taken as an affront to high-context people
because it can make them feel “This simple and redundant language gives me the
feeling that they think I am stupid.” – while the other way around might be
like “This layered language gives me the feeling that I am not trustworthy,
otherwise they would just speak straight.”, or “If the matter was important to
them, they would be more direct about it.”</p>
<p>A rule of thumb is that humans from countries with very old history and to
some extent isolation share a lot of context in their language, giving them
the possibility to say more with less. People with very different backgrounds
don’t have as much common context, hence the need for more precise
communication.</p>
<dl>
<dt>2.) Evaluating: Direct Negative Feedback vs. Indirect Negative Feedback</dt>
<dd>
<p>This scale is about different ways to give <em>negative</em> feedback:</p>
</dd>
</dl>
<ul>
<li><u>Direct</u>
<ul>
<li>Negative feedback to a colleague is provided frankly, bluntly, and
honestly.</li>
<li>Negative messages stand alone, not softened by positive ones.</li>
<li>Absolute descriptors are often used when criticizing.
(e.g. “totally inappropriate”, “completely unprofessional”)</li>
<li>Criticism may be given to an individual in front of a <em>group</em>.</li>
</ul></li>
<li><u>Indirect</u>
<ul>
<li>Negative feedback to a colleague is provided softly, subtly,
and diplomatically.</li>
<li>Positive messages are used to wrap negative ones.</li>
<li>Qualifying descriptors are often used when criticizing.
(e.g. “sort of inappropriate”, “slightly unprofessional”)</li>
<li>Criticism is given only in private</li>
</ul></li>
</ul>
<p>Especially cultures that are at the same time low-context and
direct-negative-feedback, like the US and UK, at first sight, seem like a
strange combination but are well explained in this chapter.</p>
<p>What is more important when giving <em>good criticism</em>? Making sure that it is
conveyed politely at the price of maybe being overlooked or not
understood to the full extent – or conveying it in its clearest form, which
also emphasizes a form of trust on the strength of the (work) relation?</p>
<dl>
<dt>3.) Persuading: Principles-First vs. Applications-First</dt>
<dd>
<p>Here, it is about how new ideas and approaches are explained to an audience:</p>
</dd>
</dl>
<ul>
<li><u>Principles First</u>
<ul>
<li>Individuals have been trained to first develop the theory or complex
concept before presenting a fact, statement, or opinion.</li>
<li>The preference is to begin a message or report by building up a
theoretical argument before moving on to a conclusion.</li>
<li>The conceptual principles underlying each situation are valued.</li>
</ul></li>
<li><u>Applications First</u>
<ul>
<li>Individuals are trained to begin with a fact, statement, or opinion and
later add concepts to back up or explain the conclusion as necessary.</li>
<li>The preference is to begin a message or report with an executive summary
or bullet points.</li>
<li>Discussions are approached in a practical, concrete manner.</li>
<li>Theoretical or philosophical discussions are avoided in a business
environment.</li>
</ul></li>
</ul>
<p>I found this distinction not only to be valid when talking to humans of
different cultures but also when talking to professionals from different
departments.</p>
<p>This part of the model does not fully apply to Asian cultures, to which the
chapter devotes another subsection.</p>
<dl>
<dt>4.) Leading: Egalitarian vs. Hierarchical</dt>
<dd>
<p>Different cultures prefer different leading styles:</p>
</dd>
</dl>
<ul>
<li><u>Egalitarian</u>
<ul>
<li>The ideal distance between a boss and a subordinate is low.</li>
<li>The best boss is a facilitator among equals.</li>
<li>Organizational structures are <em>flat</em>.</li>
<li>Communication often skips hierarchical lines.</li>
</ul></li>
<li><u>Hierarchical</u>
<ul>
<li>The ideal distance between a boss and a subordinate is high.</li>
<li>The best boss is a strong director who leads from the front.</li>
<li><em>Status</em> is important.</li>
<li>Organizational structures are <em>multilayered and fixed</em>.</li>
<li>Communication follows set hierarchical lines.</li>
</ul></li>
</ul>
<p>This chapter is especially interesting for the reason that it is easy to
accidentally undermine someone’s feeling of authority by e.g. skipping the
hierarchy when mailing questions to people etc.
Also, in hierarchical cultures, it may be pointless to ask about peoples’
opinions in the presence of their boss.
This chapter is full of examples and advice.</p>
<dl>
<dt>5.) Deciding: Consensual vs. Top-Down</dt>
<dd>
<p>How are decisions formed in organizations?</p>
</dd>
</dl>
<ul>
<li><u>Consensual</u>: Decisions are made in groups through unanimous agreement.</li>
<li><u>Top-Down</u>: Decisions are made by individuals (usually the boss).</li>
</ul>
<p>The principle is straightforward but the most interesting case are cultures
that combine strict hierarchy with consensual decision-making.
The strongest example of this is Japan with its <a href="https://en.wikipedia.org/wiki/Japanese_management_culture#Managerial_style">Ringi-Process</a>, but this is only the strongest
example of many (Germany also cultivates a bit of this mixture).</p>
<p>My highlight of this chapter is the <u>big D vs. little d</u> model that
helps explain to people from cultures that are at opposite ends of this scale,
what kind of decision is needed or made when.
When a <em>big D</em> decision is made, then it is to be seen as strict, while a
<em>little d</em> decision allows for flexibility even after the decision was made.
This distinction helps set expectations of groups that appear
chaotic or inflexible to each other.
The big D vs. little d model helped in the collaboration between US-American
and German teams in one of the book’s examples.
I really felt this one.</p>
<dl>
<dt>6.) Trusting: Task-Based vs. Relationship-Based</dt>
<dd>
<p>How do people build trust?</p>
</dd>
</dl>
<ul>
<li><u>Task-Based</u>
<ul>
<li>Trust is built through business-related activities.</li>
<li>Work relationships are built and dropped easily, based on the practicality
of the situation.</li>
<li>“You do good work consistently, you are reliable, I enjoy working with
you, I trust you.”</li>
</ul></li>
<li><u>Relationship-Based</u>
<ul>
<li>Trust is built through sharing meals, evening drinks, and visits to the
coffee machine.</li>
<li>Work relationships build up slowly over the long term.</li>
<li>“I’ve seen who you are at a deep level, I’ve shared personal time with
you, I know others well who trust you, I trust you.”</li>
</ul></li>
</ul>
<p>I felt this difference myself while working for US-American companies that
also have for example Indian offices.
Collaborating with my Indian colleagues got much better after getting to
know them in person at dinner, while I got to know my American colleagues only
at office times.
The book also explains that especially in Asian cultures it can be beneficial
to drink together:
Having seen each other with guards down increases trust on a personal level.</p>
<p>Another highlight of this chapter is the <u>Peach vs. Coconut</u> principle:
People from different cultures might misinterpret friendliness as too much of
an invitation for a personal connection or an offer of friendship, which might
look “fake” to them.
In that regard, <em>peach</em> cultures are friendly and soft with people they have
just met, but the real self stays protected professionally. <em>Coconut</em> cultures
in contrast present a hard protecting shell at first but an even softer core
inside (once you got through the shell).</p>
<dl>
<dt>7.) Disagreeing: Confrontational vs. Avoids Confrontation</dt>
<dd>
<p>How do colleagues disagree?</p>
</dd>
</dl>
<ul>
<li><u>Confrontational</u>
<ul>
<li>Disagreement and debate are positive for the team or organization.</li>
<li>Open confrontation is appropriate and will not negatively impact the
relationship.</li>
</ul></li>
<li><u>Avoids Confrontation</u>
<ul>
<li>Disagreement and debate are negative for the team or organization.</li>
<li>Open confrontation is inappropriate and will break group harmony or
negatively impact the relationship.</li>
</ul></li>
</ul>
<p>Confrontational cultures are also emotionally more expressive, which can be
overwhelming for people of different cultures – what looks like a normal
discussion to one can seem to another like a fight that is about to break out.</p>
<p>As the styles of disagreeing and decision-making can cause incompatibilities,
especially in big international companies, the chapter explains how to deal
with this on large scale, and how to present decisions/discussions to a mixed
audience.</p>
<dl>
<dt>8.) Scheduling: Linear-Time vs. Flexible-Time</dt>
<dd>
<p>How do cultures approach plans, projects, and appointments?</p>
</dd>
</dl>
<ul>
<li><u>Linear Time</u>
<ul>
<li>Project steps are approached sequentially, completing one task
before beginning the next.</li>
<li>One thing at a time. No interruptions.</li>
<li>The focus is on the deadline and sticking to the schedule.</li>
<li>Emphasis is on promptness and good organization over flexibility.</li>
</ul></li>
<li><u>Flexible Time</u>
<ul>
<li>Project steps are approached fluidly, changing tasks as opportunities
arise.</li>
<li>Many things are dealt with at once and interruptions are accepted.</li>
<li>The focus is on adaptability, and flexibility is valued over organization.</li>
</ul></li>
</ul>
<p>Living in Germany, I regularly see two different kinds of linear planning
extremes:</p>
<ul>
<li>Planning everything down to the smallest detail.</li>
<li>Sticking to a plan no matter what, even if the circumstances that led to
this plan, changed.</li>
</ul>
<p>Planning has advantages, but flexibility has advantages (hello lean and
scrum), too, and different cultures picked their favorite.
It seems that the political history of countries has affected the planning
mentality of their citizens.</p>
<p>This chapter puts some effort into giving tips on how to combine both in
multicultural companies.</p>
<h3 id="arent-stereotypes-racist">Aren’t Stereotypes Racist?</h3>
<p>For some, the book may trigger the question, of whether books like this may
propagate a school of thought that facilitates box-thinking or even racism.
Erin addresses these worries in the introductory chapter:</p>
<blockquote>
<p>After I published an online article on the differences among Asian cultures
[…], one reader commented, “Speaking of cultural differences leads us to
stereotype and therefore put individuals in boxes with ‘general traits’.
Instead of talking about culture, it is important to judge people as
individuals, not just products of their environment.”</p>
</blockquote>
<p>This is indeed a good point, but her answer puts it in a great perspective:</p>
<blockquote>
<p>If you go into every interaction assuming that culture doesn’t matter, your
default mechanism will be to view others through <em>your</em> cultural lens
and to judge or misjudge them accordingly.</p>
</blockquote>
<h2 id="summary">Summary</h2>
<p>As so often with books about culture and psychology, there are no big surprises
in it, or nothing <em>completely</em> new.
The value I took out of this book is that it puts everything in a structured
perspective that I would not have come up with myself.
After reading through it, I felt inspired and empowered on how to deal with
cultural differences in the future, and get more out of intercultural
collaboration!</p>
<p>Apart from cultural differences, many people within the same/your country
show different traits that can be found on the presented scales, too.
It helps make sense of behavior that could otherwise be considered
strange.
It is easier to show empathy when you understand where people are coming from.</p>
<p>I declare this book a must-read for every (not only) software engineer
professional who works internationally.</p>]]></summary>
</entry>
<entry>
    <title>Book Review: A Philosophy of Software Design</title>
    <link href="https://blog.galowicz.de/2022/12/05/book-review-a-philosophy-of-software-design" />
    <id>https://blog.galowicz.de/2022/12/05/book-review-a-philosophy-of-software-design</id>
    <published>2022-12-05T00:00:00Z</published>
    <updated>2022-12-05T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>What separates the truly great software developers from the average ones?
People have lots of opinions about this, but it’s often hard to describe what
makes the code of a great engineer so good - and what part of it novice
programmers should really try to learn from.
John Ousterhout’s book <a href="https://amzn.to/3B8ufgM">A philosophy of Software Design</a>
aims to answer this question and actually introduces some extraordinarily
appropriate vocabulary for <em>your</em> next discussion about software quality.</p>
<!--more-->
<div class="book-cover">
<figure>
<img src="/images/books/a-philosophy-of-software-design.png" alt="Book Cover of “A Philosophy of Software Design”" />
<figcaption aria-hidden="true">Book Cover of “A Philosophy of Software Design”</figcaption>
</figure>
</div>
<h2 id="book-author">Book &amp; Author</h2>
<p><a href="https://amzn.to/3B8ufgM">Link to the Amazon Store Page</a></p>
<p>The book is originally from 2018 and its second edition was released in 2021.
It it ~180 pages, so it should be a very quick read.</p>
<p><a href="https://en.wikipedia.org/wiki/John_Ousterhout">John Ousterhout</a> is a computer
science professor for computer science at <a href="https://web.stanford.edu/~ouster">Stanford</a>
who has worked on all kinds of software related topics, but also spent a lot of
time teaching how to <em>design</em> software in general - or more precisely, looking
at the design itself of software as the underlying challenge.</p>
<p>In his university lectures, as he also describes in this book, he lets his
students exercise the design of different kinds of software solutions for small
systems.
While doing that, he tried to find ways to explain the pitfalls that novice
programmers fall into, and what exactly experienced programmers do differently
so that the software they create not only works, but stays maintainable.</p>
<p>The experiences and insights he describes, were not new to me.
Most of the time I thought “Yeah I know this situation, and it’s also clear to
me how to do it differently - but the problem is always <em>explaining</em> it to
programmers who don’t think this way (yet)!”
But for all the principles and strategies that experienced developers follow
with their gut-feel that they developed over time, John found awesome vocabulary
and ways to explain them to programmers who don’t know them yet.</p>
<h2 id="content-and-structure">Content and Structure</h2>
<p>I experienced the book like a journey over two main topics, which shall get
their own subsection each.
As the book is already relatively short and every programmer should read it
anyway, I won’t go much into detail and instead just state the most important
points and principles.</p>
<h3 id="symptoms-and-causes-of-complexity-abstraction-modules-and-interfaces">Symptoms and Causes of Complexity, Abstraction, Modules, and Interfaces</h3>
<p>The preface of the book begins with an observation that is frustratingly apt:</p>
<blockquote>
<p>People have been writing programs for electronic computers for more than 80
years, but there has been surprisingly little conversation about how to design
those programs or what good programs should look like.</p>
</blockquote>
<p>Apart from all the computer languages with their different specialties,
agile or non-agile development methodologies, tooling as debuggers, linters,
version control systems, and techniques such as object-oriented/functional
programs, schools, universities and developers’ book shelves typically lack
material about how software should generally be <em>designed</em>.</p>
<p>The average developer just changes their code until the tooling stops emitting
errors/warnings and the code actually does what it is written for.
In some cases, the rules from <em>clean code</em> literature are over-applied,
resulting in short but entangled code.
While doing that, the program’s complexity increases.
Complexity is like a currency of programming: For more features, you give up
your program’s initial simplicity.
This typically ends in programs of huge complexity that cannot be maintained
easily any longer.</p>
<p>Complexity never comes with big leaps but always with a stepwise increase.
The three main <strong>symptoms</strong> of complexity are:</p>
<dl>
<dt>Change Amplification</dt>
<dd>
<p>Seemingly simple changes require modifications in many different places.</p>
</dd>
<dt>Cognitive Load</dt>
<dd>
<p>The amount of project-knowledge that a developer needs to know in order to
complete a task.</p>
</dd>
<dt>Unknown Unknowns</dt>
<dd>
<p>It is not obvious which pieces of code must be modified to complete a task.</p>
</dd>
</dl>
<p>The two main <strong>causes</strong> of complexity are:</p>
<dl>
<dt>Dependencies</dt>
<dd>
<p>A dependency exists when a given piece of code cannot be understood and
modified in isolation. They can’t be eliminated, but designed carefully.
(This is a definition for the discussions in the book.
Software dependencies like installable libraries mean something different.)</p>
</dd>
<dt>Obscurity</dt>
<dd>
<p>Obscurity occurs when important information is not obvious.</p>
</dd>
</dl>
<p>In order to keep complexity under control (i.e. to reach sustainable tradeoffs)
developers need the right mindset.
John distinguishes between two of them:</p>
<dl>
<dt>Tactical Programmers</dt>
<dd>
<p>The main focus of the tactical programmer is to <em>get something working</em>, such
as new features or bug fixes.</p>
</dd>
<dt>Strategic Programmers</dt>
<dd>
<p>The strategic programmer does not think of “working code” as their primary
goal (although accepting that delivered code must always be working).
Their primary goal is a <em>great design</em>.</p>
</dd>
</dl>
<p>Most real companies’ work culture and deadline pressure facilitates the rise of
tactical programmers (also called <em>tactical tornados</em>), which typically write
code with uncontrollable complexity.
Strategical programming requires an <strong>investment mindset</strong> for trading initial
upfront slowdown against long-term improvements.
The fight against continuously increasing complexity is fought with continuous
investments.
Discussions between strategic and tactical programmers are often frustrating.</p>
<p>The next topic is the organization of code into modules (as in classes,
subsystems, or services) that communicate via interfaces.
Modules encapsulate the complexity of systems into domain-specific units.
John coins terms for two different qualities of modules:</p>
<dl>
<dt>Deep Modules</dt>
<dd>
<p>The best modules provide powerful functionality over simple interfaces.
They provide good abstraction by providing complex functionality but only
exposing a small fraction of their internal complexity.
One example are the five basic system calls for I/O in UNIX operating systems.</p>
</dd>
<dt>Shallow Modules</dt>
<dd>
<p>In contrast to deep modules, shallow ones have relatively complex interfaces,
compared to the functionality they provide.
Many shallow interfaces do not even provide much keystroke saving when used,
compared to their reimplementation.</p>
</dd>
</dl>
<p>Shallow modules are often a result of <em>classitis</em>.
Students at university, or readers of <em>clean code</em> lecture are often advised to
break code into small units.
If this advise is followed without much strategy, it often results in shallow
modules which are not much more than leaking abstractions.</p>
<p>Another interesting word that John introduces in this context is <em>temporal
decomposition</em>:
Developers often structure code modules according to the order
in which operations occur.
This does often lead to code that shares knowledge, but at different places in
the code, leading to change amplification, higher cognitive load, and unknown
unknowns again.</p>
<p>A good rule of thumb in module design that John comes up with is:</p>
<blockquote>
<p>It is more important for a module to have a simple interface than a simple
implementation.</p>
</blockquote>
<p>If a module ends up being too complex, but its interface is very simple, then
this means that it can easily be substituted by a better one.
Also, changes on such a module do not increase change amplification.</p>
<p>Another great principle is to <em>define errors out of existence</em>:
John argues that error handling and exceptions make programs much more complex,
and a good way to reduce such complexity is representing data, interfaces, and
semantics in ways that make it impossible to encode erratic cases that need
special handling.</p>
<h3 id="how-to-write-code">How to Write Code</h3>
<p>The second half is about the act of designing and writing code.
John argues that interfaces and modules should be designed twice:
Implementing a design for the first time often exposes new insights that would
lead to a different design in a second approach.
I myself often designed things not only twice but tried three or four different
approaches, in order to come up with the one that leads to the best result.
It’s a game changer for complexity.</p>
<p>Working hard on the first design attempt, just to throw it away because it was
just a vehicle for learning how to do it right:
This is clearly not the mindset of the average programmer.
At first glance, it also looks like it would waste a lot of time, especially
from the point of view of a tactical programmer.</p>
<p>The next chapters are about comments, naming, modifying existing code,
consistency (of style/documentation/etc. across the project), performance,
and contain a lot of fine-grained advise that should not be unknown to the
working programmer.
Because it can’t be summarized by few principles, i will just drop some
interesting highlights:
(These read like rules, but they are not and should never be over-applied in a
dogmatic fashion)</p>
<ul>
<li>If an interface <em>comment</em> describes its implementation, it indicates that
the interface is shallow.</li>
<li>If a variable or type <em>name</em> is inherently <em>hard to pick</em>, it indicates a
bad abstraction.</li>
<li>When <em>modifying</em> code:
<ul>
<li>After each change, the system should have the structure that it would have
if you had designed it from the start with that change in mind.</li>
<li>If you’re not making the design better, you are probably making it worse.</li>
</ul></li>
<li>Software should be designed for <em>ease of reading</em>, not ease of writing.</li>
<li>Test-driven development focuses attention on getting specific features
working, rather than finding the best design.
In other words, it facilitates tactical work more than strategic work.</li>
</ul>
<p>John summarizes in his book’s conclusion:</p>
<blockquote>
<p>The reward for being a good designer is that you get to spend a larger
fraction of your time in the design phase, which is fun.
Poor designers spend most of their time chasing bugs in complicated and
brittle code.</p>
</blockquote>
<h2 id="summary">Summary</h2>
<p>I really enjoyed this book because it gave me an effective new vocabulary in the
epic fight against complexity, which is often more social than technical when
arguing with tactical programmers.</p>
<p>Read it - as early as possible - in your developer career.</p>]]></summary>
</entry>
<entry>
    <title>NixOS Community Oceansprint late 2022 Report</title>
    <link href="https://blog.galowicz.de/2022/11/26/nix-community-oceansprint3-report" />
    <id>https://blog.galowicz.de/2022/11/26/nix-community-oceansprint3-report</id>
    <published>2022-11-26T00:00:00Z</published>
    <updated>2022-11-26T00:00:00Z</updated>
    <summary type="html"><![CDATA[<!--
  cSpell:words Domen Kozar Neyts Zupan Raito Bezarius Niklas Sturm Noogle
  cSpell:words Florian Friesdorf Lanzarote Kirschbauer strandings kiosko
  cSpell:words Stecklina lanzaboote Hauer
-->
<p>This is my trip report from the late 2022 Oceansprint hackathon on Lanzarote.
For more information please also have a look on the website:
<a href="https://oceansprint.org">https://oceansprint.org</a></p>
<!--more-->
<h1 id="oceansprint">Oceansprint</h1>
<p><a href="https://oceansprint.org">Oceansprint</a> is a regular hackathon event in the
NixOS community (i wrote a bit about the community and event location
<a href="/2021/12/12/nix-community-oceansprint-report">in the last article</a>)
that is planned to happen twice a year.
From 21.11. to 25.11.2022 it happened for the third time.
This time we were roughly ~20 participants.</p>
<p>After i was at the first oceansprint, skipped the second, and participated in
the third one again, i have experienced quite an increment in the overall
event’s quality and the group activities.
Big kudos to Neyts Zupan, Domen Kozar and Florian Friesdorf for the great
organization!</p>
<figure>
<img src="/images/2021-12-oceansprint-location.jpg" alt="The Oceansprint location" />
<figcaption aria-hidden="true">The Oceansprint location</figcaption>
</figure>
<h1 id="socializing">Socializing</h1>
<p>During the hacking sessions over the day, we all typically sat in groups based
on who worked together on something.
At breakfast and lunch there was typically some random regrouping involved,
and over the day there were also many sporadic discussions at the coffee
machine.
This, combined with the fact that 99% of the time everyone was disciplined in
speaking english, it was very nice to catch some interesting insights in
random topics when walking by.</p>
<figure>
<img src="/images/2022-oceansprint3-buffet.jpg" alt="The catering Original Tweet" />
<figcaption aria-hidden="true">The catering <a href="https://twitter.com/nzupan/status/1594653961419644932">Original Tweet</a></figcaption>
</figure>
<p>Events that really lifted the interaction to a more fun level away from just
work-related discussions were the big grill party and the catamaran sailing
event (there was also wind surfing and hiking), as well as the regular
strandings at the cocktail bar “el kiosko”.
We even went into the sauna together that Domen has on his balcony.
This way it was possible to get to talk to <em>everyone</em> in more detail over the
week without any hassle and simply having a great time.</p>
<h1 id="sprint-projects">Sprint Projects</h1>
<p>I will only describe the topics where i either participated or that i followed
more closely out of personal interest.
There were many more projects, and i am pretty sure they are covered by other
blog articles.</p>
<h2 id="secure-boot-on-nixos">Secure Boot on NixOS</h2>
<p><a href="https://twitter.com/blitzclone">Julian Stecklina</a> (works at <a href="https://cyberus-technology.de/">Cyberus
Technology</a>), Niklas Sturm (works at
<a href="https://www.secunet.com/">Secunet</a>), and
<a href="https://twitter.com/Ra1t0_Bezar1us">Raito Bezarius</a> worked together to support
Secure Boot on NixOS.
They were very successful this week: We have seen laptops booting with activated
Secureboot and the Gnome Device Security dialogue displaying a green
Secureboot entry:</p>
<figure>
<img src="/images/2022-oceansprint3-secureboot.png" alt="Activated Secureboot on NixOS Original Tweet" />
<figcaption aria-hidden="true">Activated Secureboot on NixOS <a href="https://twitter.com/blitzclone/status/1596108176914493440">Original Tweet</a></figcaption>
</figure>
<p>There is already a pull request on github that introduces this work for at least
developer setups.
For secure and easy production use, some work on key management etc. is still
left to be done.
See the code and more details on github:
<a href="https://github.com/blitz/lanzaboote">github.com/blitz/lanzaboote</a></p>
<p>Multiple aspects of this make this project remarkable:</p>
<ul>
<li>Niklas and Julian did not know Raito before. The collaboration was a
spontaneous result of talking about their project plans on the first evening.</li>
<li>On every ocean sprint so far, we sent different Cyberus co-workers, and this
is the first sprint with Secunet co-workers. So far, every co-worker who
returned learned about new ways how to use nix and NixOS productively for both
personal profit and also company goals.</li>
<li>Both companies Cyberus Technology and Secunet are working with or evaluating
NixOS internally. Getting Secureboot working in NixOS is an important mile
stone.</li>
<li>This is a great example of how the sponsorship money of both companies, which
initially was a donation to support events of this kind, also turned out to be
an investment with immediate return.</li>
</ul>
<h2 id="dream2nix-enhancements">Dream2nix enhancements</h2>
<p><a href="https://github.com/hsjobeki">Johannes Kirschbauer</a> and
<a href="https://github.com/davhau">David Hauer</a> worked together on improving the
<a href="https://nix-community.github.io/dream2nix/">dream2nix</a> project.</p>
<p>From my perspective this is another remarkable collaboration between open source
maintainers and companies, as Johannes aims to push nix for frontend/UI projects
at secunet.
The Oceansprint has been the perfect chance to talk to David about how to get
the most value out of this for all parties over a beer in person.</p>
<h2 id="noogle">Noogle</h2>
<p>Another thing that Johannes and David came up with, is Noogle:
A search machine for nix and nixpkgs library functions.</p>
<figure>
<img src="/images/2022-oceansprint3-noogle.jpg" alt="Noogle Alpha Original Tweet" />
<figcaption aria-hidden="true">Noogle Alpha <a href="https://twitter.com/domenkozar/status/1596168388195545088">Original Tweet</a></figcaption>
</figure>
<p>All Haskellers immediately cheered for this as this is like
<a href="https://hoogle.haskell.org/">hoogle</a>, a great tool for finding the right
function in all available packages.</p>
<h1 id="sponsors">Sponsors</h1>
<p>The sponsors are a very important topic, as such events would not be possible
without them.</p>
<figure>
<img src="/images/2022-oceansprint3-shirt.jpg" alt="The sponsored Oceansprint Shirt Original Tweet" />
<figcaption aria-hidden="true">The sponsored Oceansprint Shirt <a href="https://twitter.com/domenkozar/status/1595004457653309440">Original Tweet</a></figcaption>
</figure>
<p>As one of the founders of Cyberus, i was interested in the company growing
into the Nix(OS) open source community, hence it was natural to become a sponsor
of this event.
In addition to that, i put some effort into convincing secunet to sponsor the
oceansprint, too, as we do work together with this awesome technology and they
are also interested having their colleagues to grow into the community.</p>
<p>After having spoken with the organizers <a href="https://twitter.com/nzupan">Neyts</a> and
<a href="https://twitter.com/domenkozar">Domen</a> at the
<a href="https://2022.nixcon.org/">NixCon 2022 in Paris</a> about the topic of sponsoring,
i learned that they found it easier to motivate small businesses to participate
in sponsoring than the very big ones.</p>
<p>It seems like the pressure to be able to explain the value proposition of a
sponsoring is bigger for large company management.
So far i personally see the following reasons why companies should
sponsor/invest in conferences and hackathons like this:</p>
<ul>
<li>Sponsoring is a good way to reserve seats for your employees at such events.
While attending, they will extend their network and as soon as some open
source library or tool has a problem that needs to be fixed, discussing how
to fix it quickly will be just a phone call with the maintainer away.
I experienced the profit of such short communication channels at work very
often already.</li>
<li>The exposure to long-year community members helps a lot understanding the
used technology better. Especially less experienced colleagues come back with
a lot of enlightenment, which is why we try to send different colleagues every
time.</li>
<li>Having your own employees hack together on things with people that are not on
your payroll is a very efficient way to get work done for all participating
sides.</li>
<li>Events like this in the sun with great social activities are a great way to
support the efforts of your most motivated co-workers. The overall motivation
at such events is infectious.</li>
<li>Most big companies use lots and lots of open source technology.
Often, employees do not get enough time due to project constraints, or are
legally not allowed to share code back.
Sponsoring such events is a great way to give back.</li>
</ul>]]></summary>
</entry>

</feed>
